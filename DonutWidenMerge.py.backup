"""
Enhanced WIDEN Merge with Advanced Quality Optimizations

This implementation includes several advanced features for high-quality model merging:

1. AUTO-TUNING: Skip threshold automatically adjusts based on Phase 1 compatibility scores
2. NEURON ALIGNMENT: Linear layer weights are optimally aligned using Hungarian algorithm
3. EMBEDDING TRANSPOSE: Automatic detection and correction of embedding orientation
4. ADAPTIVE THRESHOLDS: Per-layer skip thresholds (conv: 1e-3, linear: 1e-4, norm: 0.0)
5. FAILURE TRACKING: Comprehensive error handling with detailed diagnostics
6. SANITY METRICS: Over-merge detection with L2 change ratio analysis
7. NORM RECALIBRATION: BatchNorm/LayerNorm stats recalibration after merge
8. HYPERPARAMETER SEARCH: Grid search and Optuna optimization support
9. STRUCTURE PRESERVATION: Per-channel structure preservation using mean instead of flattening
10. CONDENSED DELTAS: Channel-preserving delta summaries for better metadata

Example usage with hyperparameter tuning:
```python
# Define evaluation function (optional - dummy evaluation if None)
def evaluate_model(model, validation_data):
    # Your evaluation logic here
    return score  # Higher = better

# Perform hyperparameter search
best_params = tune_merge_hyperparameters(
    merge_function=enhanced_widen_merging_with_dynamic_strength,
    base_model=base_model,
    models_to_merge=[model1, model2, model3],
    evaluate_function=evaluate_model,
    validation_data=val_loader,
    method="optuna",  # or "grid"
    n_trials=30
)

# Use best parameters for final merge
merged_model, diagnostics = enhanced_widen_merging_with_dynamic_strength(
    **best_params['params']
)

# Recalibrate normalization layers with real data
recalibrate_norm_stats(merged_model, calib_data=val_loader, num_batches=10)
```

Dependencies:
- scipy (optional): For neuron alignment via Hungarian algorithm
- optuna (optional): For Bayesian hyperparameter optimization
"""

import re
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import hashlib
import psutil
import gc
import os
import tempfile
import uuid
from contextlib import contextmanager
import time
import logging
import os
import numpy as np
import warnings
import itertools

# Import all shared modules
try:
    # Try relative imports first (when running as part of ComfyUI)
    from .shared.logging_config import (
        widen_logger, performance_logger, memory_logger, diagnostic_logger,
        print_progress_bar, ProgressBarContext, configure_widen_logging
    )
    from .shared.constants import *
    from .shared.exceptions import *
    from .shared.alignment import (
        align_linear_layer, transpose_embeddings_if_needed, align_and_stack,
        safe_stack, align_tensors, create_condensed_delta, safe_align_and_stack
    )
    from .shared.merge_strength import (
        get_adaptive_skip_threshold, _compatibility_to_merge_strength,
        _fast_sigmoid_strength, MergeFailureTracker, DEFAULT_SKIP_THRESHOLDS
    )
    from .shared.diagnostics import (
        compute_merge_sanity_metrics, print_merge_diagnostics,
        tune_merge_hyperparameters, sanitize_strength_distribution
    )
    from .shared.memory_management import (
        MemoryEfficientContext, get_reusable_tensor_buffer, clear_tensor_buffer_cache,
        clear_session_cache, monitor_memory, check_memory_safety, force_cleanup,
        gentle_cleanup, aggressive_memory_cleanup, monitor_memory_usage,
        MemoryProfiler, get_widen_memory_profiler, memory_cleanup_context,
        MemoryExhaustionError, ultra_memory_efficient_widen_merge
    )
    from .shared.tensor_operations import *
    from .shared.utility_functions import (
        _analyze_compatibility_patterns_and_recommend_threshold
    )
    from .shared.core_merge_functions import (
        enhanced_widen_merging_with_dynamic_strength,
        enhanced_widen_merging_with_post_refinement,
        create_enhanced_merge_with_refinement_config
    )
    from .shared.lora_processing import LoRAStackProcessor, LoRADelta
    from .shared.cache_management import (
        compute_merge_hash, check_cache_for_merge, store_merge_result, clear_merge_cache
    )
except ImportError:
    # Fallback to absolute imports (when running standalone)
    from shared.logging_config import (
        widen_logger, performance_logger, memory_logger, diagnostic_logger,
        print_progress_bar, ProgressBarContext, configure_widen_logging
    )
    from shared.constants import *
    from shared.exceptions import *
    from shared.alignment import (
        align_linear_layer, transpose_embeddings_if_needed, align_and_stack,
        safe_stack, align_tensors, create_condensed_delta, safe_align_and_stack
    )
    from shared.merge_strength import (
        get_adaptive_skip_threshold, _compatibility_to_merge_strength,
        _fast_sigmoid_strength, MergeFailureTracker, DEFAULT_SKIP_THRESHOLDS
    )
    from shared.diagnostics import (
        compute_merge_sanity_metrics, print_merge_diagnostics,
        tune_merge_hyperparameters, sanitize_strength_distribution
    )
    from shared.memory_management import (
        MemoryEfficientContext, get_reusable_tensor_buffer, clear_tensor_buffer_cache,
        clear_session_cache, monitor_memory, check_memory_safety, force_cleanup,
        gentle_cleanup, aggressive_memory_cleanup, monitor_memory_usage,
        MemoryProfiler, get_widen_memory_profiler, memory_cleanup_context,
        MemoryExhaustionError, ultra_memory_efficient_widen_merge
    )
    from shared.tensor_operations import *
    from shared.utility_functions import (
        _analyze_compatibility_patterns_and_recommend_threshold
    )
    from shared.core_merge_functions import (
        enhanced_widen_merging_with_dynamic_strength,
        enhanced_widen_merging_with_post_refinement,
        create_enhanced_merge_with_refinement_config
    )
    from shared.lora_processing import LoRAStackProcessor, LoRADelta
    from shared.cache_management import (
        compute_merge_hash, check_cache_for_merge, store_merge_result, clear_merge_cache
    )

# Scipy and Optuna availability are checked in the modules now
try:
    from scipy.optimize import linear_sum_assignment
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

# Progress bar and logging functions imported from logging_config module

# MergingMethod class - required by the ComfyUI node classes
class MergingMethod:
    """
    Permute the rows of B to best match A, based on cosine similarity.
    Uses Hungarian algorithm for optimal neuron matching.
    
    Args:
        A, B: (out_features, in_features) weight tensors
    Returns:
        Permuted copy of B that best aligns with A
    """
    if not SCIPY_AVAILABLE or A.shape != B.shape or A.ndim != 2:
        return B
    
    try:
        # Compute normalized rows for cosine similarity
        a_norm = A / (A.norm(dim=1, keepdim=True) + 1e-8)
        b_norm = B / (B.norm(dim=1, keepdim=True) + 1e-8)
        
        # Similarity matrix (out Ã— out)
        sim = torch.mm(a_norm, b_norm.T)  # cosine similarities
        
        # Convert to cost matrix (minimize negative similarity)
        cost = -sim.cpu().numpy()
        
        # Solve assignment problem
        row_idx, col_idx = linear_sum_assignment(cost)
        
        # Permute B according to optimal assignment
        B_permuted = B[col_idx]
        
        diagnostic_logger.debug(f"Aligned linear layer: similarity improved by {sim[row_idx, col_idx].mean().item():.4f}")
        return B_permuted
        
    except Exception as e:
        diagnostic_logger.warning(f"Linear layer alignment failed: {e}")
        return B

def transpose_embeddings_if_needed(param_name: str, param_tensor: torch.Tensor, hidden_size_hint: int = None) -> torch.Tensor:
    """
    Automatically detect and transpose embedding layers if dimensionality suggests it's needed.
    
    Args:
        param_name: Parameter name to check for embedding patterns
        param_tensor: The parameter tensor
        hidden_size_hint: Expected hidden dimension size for validation
    """
    if param_tensor.ndim != 2:
        return param_tensor
        
    # Check if this looks like an embedding layer
    embedding_patterns = ['embed', 'emb', 'token', 'pos', 'position']
    if not any(pattern in param_name.lower() for pattern in embedding_patterns):
        return param_tensor
        
    # Heuristic: if second dim looks like hidden_size, probably correct orientation
    if hidden_size_hint and param_tensor.shape[1] == hidden_size_hint:
        return param_tensor
        
    # If first dimension is much larger than second, might need transpose
    if param_tensor.shape[0] > param_tensor.shape[1] * 2:
        diagnostic_logger.info(f"Auto-transposing potential embedding {param_name}: {param_tensor.shape} -> {param_tensor.shape[::-1]}")
        return param_tensor.T
        
    return param_tensor

# Per-layer skip threshold configuration
DEFAULT_SKIP_THRESHOLDS = {
    "conv": 1e-3,
    "linear": 1e-4, 
    "norm": 0.0,  # Never skip normalization layers
    "attention": 1e-5,
    "embed": 0.0,  # Never skip embedding layers
    "bias": 1e-4,
    "weight": 1e-4
}

def get_adaptive_skip_threshold(param_name: str, default_threshold: float = 0.0) -> float:
    """
    Get adaptive skip threshold based on parameter name patterns.
    
    Args:
        param_name: Name of the parameter
        default_threshold: Fallback threshold
    """
    param_lower = param_name.lower()
    
    # Check each pattern and return first match
    for pattern, threshold in DEFAULT_SKIP_THRESHOLDS.items():
        if pattern in param_lower:
            return threshold
            
    return default_threshold

class MergeFailureTracker:
    """Track and report merge failures with detailed diagnostics"""
    
    def __init__(self):
        self.failed_params = []
        self.failure_reasons = {}
        self.fallback_count = 0
        
    def record_failure(self, param_name: str, exception: Exception, fallback_used: bool = True):
        """Record a parameter merge failure"""
        self.failed_params.append(param_name)
        self.failure_reasons[param_name] = str(exception)
        if fallback_used:
            self.fallback_count += 1
            
    def check_critical_failures(self, raise_on_critical: bool = True):
        """Check if failures are critical enough to abort"""
        if not self.failed_params:
            return
            
        critical_patterns = ['attention', 'embed', 'head', 'output']
        critical_failures = [p for p in self.failed_params 
                           if any(pattern in p.lower() for pattern in critical_patterns)]
        
        failure_rate = len(self.failed_params) / max(1, len(self.failed_params) + self.fallback_count)
        
        if critical_failures and raise_on_critical:
            raise RuntimeError(f"Critical merge failures in {len(critical_failures)} parameters: {critical_failures[:3]}...")
            
        if failure_rate > 0.1:  # More than 10% failure rate
            warnings.warn(f"High merge failure rate: {failure_rate:.1%} ({len(self.failed_params)} failed)")
            
    def get_summary(self) -> str:
        """Get a summary of merge failures"""
        if not self.failed_params:
            return "âœ“ All parameters merged successfully"
            
        return f"âš  {len(self.failed_params)} merge failures, {self.fallback_count} fallbacks used"

@torch.no_grad()
def recalibrate_norm_stats(model, calib_data=None, device="cuda", num_batches=5):
    """
    Re-calibrate BatchNorm/LayerNorm running statistics after model merging.
    This helps fix broken normalization stats that can cause merged model instability.
    
    Args:
        model: The merged model to recalibrate
        calib_data: Calibration data loader or tensor. If None, uses random data.
        device: Device to run calibration on
        num_batches: Number of calibration batches to use
    """
    if not hasattr(model, 'modules'):
        return
        
    # Find all BatchNorm-like layers
    bn_layers = []
    for module in model.modules():
        if hasattr(module, 'running_mean') and hasattr(module, 'running_var'):
            bn_layers.append(module)
    
    if not bn_layers:
        return
        
    diagnostic_logger.info(f"Recalibrating {len(bn_layers)} normalization layers")
    
    # Store original training state
    was_training = model.training
    model.train()
    
    # Reset running statistics and set high momentum for quick adaptation
    for layer in bn_layers:
        if hasattr(layer, 'reset_running_stats'):
            layer.reset_running_stats()
        if hasattr(layer, 'momentum'):
            layer.momentum = 1.0  # Full replacement each batch
    
    try:
        if calib_data is None:
            # Generate random calibration data based on model's expected input
            # This is a fallback - real calibration data is always better
            input_shape = getattr(model, 'input_shape', None)
            if input_shape is None:
                # Try to infer from first layer
                first_layer = next(model.modules(), None)
                if hasattr(first_layer, 'in_features'):
                    input_shape = (1, first_layer.in_features)
                elif hasattr(first_layer, 'in_channels'):
                    input_shape = (1, first_layer.in_channels, 32, 32)  # Assume image-like
                else:
                    diagnostic_logger.warning("Cannot determine input shape for norm recalibration")
                    return
                    
            for batch_idx in range(num_batches):
                fake_input = torch.randn(input_shape, device=device)
                try:
                    model(fake_input)
                except:
                    break  # Stop if model can't handle fake input
        else:
            # Use provided calibration data
            batch_count = 0
            for batch in calib_data:
                if batch_count >= num_batches:
                    break
                    
                if isinstance(batch, (list, tuple)):
                    x = batch[0]  # Assume first element is input
                else:
                    x = batch
                    
                if not isinstance(x, torch.Tensor):
                    continue
                    
                try:
                    model(x.to(device))
                    batch_count += 1
                except:
                    continue  # Skip problematic batches
                    
    except Exception as e:
        diagnostic_logger.warning(f"Norm recalibration failed: {e}")
    finally:
        # Restore original training state
        model.train(was_training)

def compute_merge_sanity_metrics(base_model, merged_model, param_names=None):
    """
    Compute sanity check metrics to detect over-merging or catastrophic changes.
    
    Returns:
        dict: Metrics including L2 change ratio, per-layer changes, etc.
    """
    metrics = {
        'total_norm_old': 0.0,
        'total_norm_delta': 0.0,
        'max_param_change': 0.0,
        'layer_changes': {},
        'suspicious_layers': []
    }
    
    base_state = base_model.state_dict() if hasattr(base_model, 'state_dict') else base_model
    merged_state = merged_model.state_dict() if hasattr(merged_model, 'state_dict') else merged_model
    
    if param_names is None:
        param_names = list(base_state.keys())
    
    for name in param_names:
        if name not in base_state or name not in merged_state:
            continue
            
        base_param = base_state[name]
        merged_param = merged_state[name]
        
        if not isinstance(base_param, torch.Tensor) or not isinstance(merged_param, torch.Tensor):
            continue
            
        # Compute norms
        old_norm = base_param.norm().item()
        delta_norm = (merged_param - base_param).norm().item()
        
        metrics['total_norm_old'] += old_norm
        metrics['total_norm_delta'] += delta_norm
        
        # Track per-layer changes
        if old_norm > 0:
            relative_change = delta_norm / old_norm
            metrics['layer_changes'][name] = relative_change
            metrics['max_param_change'] = max(metrics['max_param_change'], relative_change)
            
            # Flag suspicious changes (>50% change in a layer)
            if relative_change > 0.5:
                metrics['suspicious_layers'].append((name, relative_change))
    
    # Compute global change ratio
    if metrics['total_norm_old'] > 0:
        metrics['global_change_ratio'] = metrics['total_norm_delta'] / metrics['total_norm_old']
    else:
        metrics['global_change_ratio'] = 0.0
    
    return metrics

def print_merge_diagnostics(metrics):
    """Print human-readable merge diagnostics"""
    ratio = metrics['global_change_ratio']
    print(f"\nðŸ” MERGE DIAGNOSTICS:")
    print(f"  Global change ratio: Î”â€–Î¸â€–/â€–Î¸â€– = {ratio:.3%}")
    
    if ratio > 1.0:
        print(f"  âš ï¸  WARNING: Very high change ratio (>{100:.0%}) - possible over-merge")
    elif ratio > 0.5:
        print(f"  ðŸŸ¡ CAUTION: High change ratio (>{50:.0%}) - verify results")
    elif ratio > 0.1:
        print(f"  âœ… NORMAL: Moderate changes ({ratio:.1%})")
    else:
        print(f"  â„¹ï¸  LOW: Minimal changes ({ratio:.1%}) - conservative merge")
    
    if metrics['suspicious_layers']:
        print(f"  ðŸš¨ {len(metrics['suspicious_layers'])} layers with >50% change:")
        for name, change in metrics['suspicious_layers'][:3]:  # Show first 3
            print(f"    {name}: {change:.1%}")
        if len(metrics['suspicious_layers']) > 3:
            print(f"    ... and {len(metrics['suspicious_layers']) - 3} more")

def tune_merge_hyperparameters(
    merge_function, 
    base_model, 
    models_to_merge,
    evaluate_function=None,
    validation_data=None,
    method="grid",  # "grid" or "optuna"
    n_trials=20,
    **fixed_kwargs
):
    """
    Hyperparameter search for optimal merge settings.
    
    Args:
        merge_function: The merge function to optimize
        base_model: Base model for merging
        models_to_merge: List of models to merge
        evaluate_function: Function that takes (model, val_data) -> score (higher=better)
        validation_data: Validation dataset for evaluation
        method: "grid" for grid search, "optuna" for Bayesian optimization
        n_trials: Number of trials for Optuna
        **fixed_kwargs: Fixed parameters to pass to merge function
    """
    if evaluate_function is None:
        diagnostic_logger.warning("No evaluation function provided - using dummy evaluation")
        def dummy_eval(model, data):
            # Dummy evaluation: return negative of global change ratio (prefer smaller changes)
            metrics = compute_merge_sanity_metrics(base_model, model)
            return -metrics['global_change_ratio']
        evaluate_function = dummy_eval
    
    best_result = {"score": float('-inf'), "params": None}
    
    if method == "grid":
        # Grid search with reasonable parameter ranges
        param_grid = {
            "merge_strength": [0.5, 1.0, 1.5],
            "importance_threshold": [0.5, 1.0, 2.0],
            "importance_boost": [1.5, 2.5, 3.5],
            "skip_threshold": [0.0, 1e-5, 1e-4, 1e-3],
        }
        
        print(f"ðŸ” Starting grid search with {len(list(itertools.product(*param_grid.values())))} combinations...")
        
        for i, (ms, it, ib, st) in enumerate(itertools.product(*param_grid.values())):
            params = {
                "merge_strength": ms,
                "importance_threshold": it, 
                "importance_boost": ib,
                "skip_threshold": st,
                **fixed_kwargs
            }
            
            try:
                merged_model, _ = merge_function(
                    merged_model=base_model,
                    models_to_merge=models_to_merge,
                    **params
                )
                
                score = evaluate_function(merged_model, validation_data)
                
                if score > best_result["score"]:
                    best_result = {"score": score, "params": params}
                    
                print(f"  Trial {i+1}: score={score:.4f}, params={params}")
                
            except Exception as e:
                diagnostic_logger.warning(f"Grid search trial {i+1} failed: {e}")
                continue
                
    elif method == "optuna" and OPTUNA_AVAILABLE:
        print(f"ðŸ” Starting Optuna optimization with {n_trials} trials...")
        
        def objective(trial):
            params = {
                "merge_strength": trial.suggest_float("merge_strength", 0.1, 2.0),
                "importance_threshold": trial.suggest_float("importance_threshold", 0.1, 5.0),
                "importance_boost": trial.suggest_float("importance_boost", 1.0, 5.0),
                "skip_threshold": trial.suggest_loguniform("skip_threshold", 1e-6, 1e-2),
                **fixed_kwargs
            }
            
            try:
                merged_model, _ = merge_function(
                    merged_model=base_model,
                    models_to_merge=models_to_merge, 
                    **params
                )
                
                score = evaluate_function(merged_model, validation_data)
                return score
                
            except Exception as e:
                diagnostic_logger.warning(f"Optuna trial failed: {e}")
                return float('-inf')
        
        study = optuna.create_study(direction="maximize", sampler=optuna.samplers.TPESampler())
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        
        best_result = {
            "score": study.best_value,
            "params": {**study.best_params, **fixed_kwargs}
        }
        
    else:
        if method == "optuna":
            print("âš ï¸ Optuna not available, falling back to grid search")
        return tune_merge_hyperparameters(
            merge_function, base_model, models_to_merge, evaluate_function,
            validation_data, method="grid", **fixed_kwargs
        )
    
    print(f"\nðŸŽ¯ BEST HYPERPARAMETERS:")
    print(f"  Score: {best_result['score']:.4f}")
    for key, value in best_result['params'].items():
        print(f"  {key}: {value}")
    
    return best_result

def sanitize_strength_distribution(strength_dist):
    """
    Centralized helper to sanitize strength distribution for display.
    Returns a clean dict for formatting, with N/A handling for empty data.
    """
    if strength_dist['count'] == 0:
        return {
            'min_used': None,  # Will display as "N/A"
            'max_used': None,  # Will display as "N/A"
            'mean': None,      # Will display as "N/A"
            'count': 0,
            'display_text': "N/A (no dynamic adjustments)"
        }
    return {
        'min_used': strength_dist['min_used'],
        'max_used': strength_dist['max_used'],
        'mean': strength_dist['mean'],
        'count': strength_dist['count'],
        'display_text': f"{strength_dist['min_used']:.3f}-{strength_dist['max_used']:.3f} (avg {strength_dist['mean']:.3f})"
    }

# Memory-efficient context manager for large tensor operations
class MemoryEfficientContext:
    """Context manager for memory-intensive operations with automatic cleanup"""
    def __init__(self, operation_name="tensor_operation"):
        self.operation_name = operation_name
        self.initial_memory = None
        
    def __enter__(self):
        # Pre-emptive cleanup before large operations
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        # Cleanup after operations
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# Global tensor buffer cache for memory reuse
_TENSOR_BUFFER_CACHE = {}

def get_reusable_tensor_buffer(shape, dtype=torch.float32, device='cpu'):
    """Get a reusable tensor buffer to avoid repeated allocations"""
    key = (tuple(shape), dtype, device)
    if key not in _TENSOR_BUFFER_CACHE:
        _TENSOR_BUFFER_CACHE[key] = torch.empty(shape, dtype=dtype, device=device)
    else:
        # Reuse existing buffer, just zero it out if needed
        buffer = _TENSOR_BUFFER_CACHE[key]
        if buffer.shape != tuple(shape):
            # Shape mismatch, create new buffer
            _TENSOR_BUFFER_CACHE[key] = torch.empty(shape, dtype=dtype, device=device)
    return _TENSOR_BUFFER_CACHE[key]

def clear_tensor_buffer_cache():
    """Clear the tensor buffer cache to free memory"""
    global _TENSOR_BUFFER_CACHE
    _TENSOR_BUFFER_CACHE.clear()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def configure_widen_logging(level="INFO", enable_memory_logs=False, enable_diagnostic_debug=False):
    """
    Configure WIDEN merge logging levels.
    
    Args:
        level: Overall log level ("DEBUG", "INFO", "WARNING", "ERROR")
        enable_memory_logs: Show verbose memory debugging (default: False)  
        enable_diagnostic_debug: Show detailed diagnostic info (default: False)
    
    Examples:
        configure_widen_logging("DEBUG")  # Show everything
        configure_widen_logging("WARNING")  # Only warnings and errors
        configure_widen_logging("INFO", enable_diagnostic_debug=True)  # Info + diagnostics
    """
    level_map = {
        "DEBUG": logging.DEBUG,
        "INFO": logging.INFO, 
        "WARNING": logging.WARNING,
        "ERROR": logging.ERROR
    }
    
    log_level = level_map.get(level.upper(), logging.INFO)
    
    widen_logger.setLevel(log_level)
    performance_logger.setLevel(log_level)
    
    # Memory logs are verbose - only enable if requested
    memory_logger.setLevel(logging.DEBUG if enable_memory_logs else logging.WARNING)
    
    # Diagnostic logs default to warnings unless debug enabled
    diagnostic_logger.setLevel(logging.DEBUG if enable_diagnostic_debug else logging.WARNING)
    
    widen_logger.info(f"WIDEN logging configured: level={level}, memory_logs={enable_memory_logs}, diagnostics={enable_diagnostic_debug}")

# Usage examples for logging configuration:
# configure_widen_logging("DEBUG")  # Show all logs including detailed diagnostics
# configure_widen_logging("WARNING")  # Only show warnings and errors (quiet mode)
# configure_widen_logging("INFO", enable_diagnostic_debug=True)  # Show compatibility warnings
# configure_widen_logging("INFO", enable_memory_logs=True)  # Show memory debugging

# ComfyUI package imports with fallbacks
try:
    from .utils.sdxl_safetensors import ensure_same_device
except ImportError:
    # Fallback for standalone execution
    def ensure_same_device(x, y): return x, y

try:
    from .memory_utils import (MemoryProfiler, memory_profiler, optimize_tensor_operations,
                              smart_device_management, batch_process_parameters, 
                              memory_efficient_tensor_ops, get_optimized_cache)
except ImportError:
    # Fallback stubs for standalone execution
    class MemoryProfiler: pass
    memory_profiler = lambda x: x
    optimize_tensor_operations = lambda x: x
    smart_device_management = lambda x: x
    batch_process_parameters = lambda x: x
    memory_efficient_tensor_ops = lambda x: x
    get_optimized_cache = lambda: {}

try:
    from .merging_methods import MergingMethod
except ImportError:
    # Fallback for standalone execution
    class MergingMethod: pass

try:
    from .task_vector import TaskVector
except ImportError:
    # Will be defined later in this file
    pass

try:
    from .utils.utils import get_param_names_to_merge
except ImportError:
    # Fallback for standalone execution
    def get_param_names_to_merge(models): return []
import numpy as np

# Advanced tensor alignment functions for structure-preserving operations
def align_and_stack(t1, t2, spatial_dim=None):
    """
    Align two tensors using adaptive pooling instead of flattening.
    Preserves per-feature structure while handling dimension mismatches.
    """
    if t1.shape == t2.shape:
        return torch.stack([t1, t2], dim=0)
    
    # Handle different tensor types with appropriate pooling
    if t1.ndim == 4 and t2.ndim == 4:  # Conv weights (N, C, H, W)
        # Pool spatial dimensions to the smaller size
        target_h = min(t1.size(2), t2.size(2))
        target_w = min(t1.size(3), t2.size(3))
        t1_pooled = F.adaptive_avg_pool2d(t1, (target_h, target_w))
        t2_pooled = F.adaptive_avg_pool2d(t2, (target_h, target_w))
        return torch.stack([t1_pooled, t2_pooled], dim=0)
    
    elif t1.ndim == 2 and t2.ndim == 2:  # Linear weights (in, out)
        # Pool to the smaller dimension in each axis
        target_shape = (min(t1.size(0), t2.size(0)), min(t1.size(1), t2.size(1)))
        t1_pooled = F.adaptive_avg_pool1d(t1.unsqueeze(0), target_shape[1]).squeeze(0)
        t2_pooled = F.adaptive_avg_pool1d(t2.unsqueeze(0), target_shape[1]).squeeze(0)
        
        # Handle first dimension
        if t1_pooled.size(0) > target_shape[0]:
            t1_pooled = t1_pooled[:target_shape[0]]
        if t2_pooled.size(0) > target_shape[0]:
            t2_pooled = t2_pooled[:target_shape[0]]
            
        return torch.stack([t1_pooled, t2_pooled], dim=0)
    
    # Fallback: use broadcasting if possible
    try:
        bcast = torch.broadcast_tensors(t1, t2)
        return torch.stack(bcast, dim=0)
    except RuntimeError:
        # Last resort: pool to scalars but preserve semantic meaning
        return torch.stack([t1.mean(), t2.mean()], dim=0)

def safe_stack(tensors, dim=0):
    """
    Stack tensors safely using broadcast-aware operations.
    """
    if not tensors:
        return None
    
    if len(tensors) == 1:
        return tensors[0].unsqueeze(dim)
    
    # Check if all tensors have the same shape
    shapes = [t.shape for t in tensors]
    if all(s == shapes[0] for s in shapes):
        return torch.stack(tensors, dim)
    
    # Try broadcasting first
    try:
        bcast = torch.broadcast_tensors(*tensors)
        return torch.stack(bcast, dim)
    except RuntimeError:
        # Use adaptive alignment
        return align_tensors(tensors, dim)

def align_tensors(tensors, stack_dim=0):
    """
    Align tensors with different shapes using optimized einsum-style patterns.
    Uses efficient tensor operations and vectorized pooling for better performance.
    """
    if not tensors:
        return None
    
    # Find target shape by taking minimum in each dimension (conservative pooling)
    shapes = [t.shape for t in tensors]
    ndims = [len(s) for s in shapes]
    
    # Handle tensors with different number of dimensions
    if not all(nd == ndims[0] for nd in ndims):
        # Pad smaller tensors with singleton dimensions
        max_ndim = max(ndims)
        padded_tensors = []
        for t in tensors:
            while t.ndim < max_ndim:
                t = t.unsqueeze(-1)
            padded_tensors.append(t)
        tensors = padded_tensors
        shapes = [t.shape for t in tensors]
    
    # Compute target shape (minimum size in each dimension)
    target_shape = tuple(min(s[i] for s in shapes) for i in range(len(shapes[0])))
    
    aligned = []
    for tensor in tensors:
        aligned_tensor = tensor
        
        # Handle 4D tensors (conv weights) with adaptive pooling
        if tensor.ndim == 4 and (tensor.shape[2] > target_shape[2] or tensor.shape[3] > target_shape[3]):
            aligned_tensor = F.adaptive_avg_pool2d(aligned_tensor, target_shape[2:4])
        
        # Handle 2D tensors (linear weights) with optimized einsum-style pooling
        elif tensor.ndim == 2:
            h, w = aligned_tensor.shape
            target_h, target_w = target_shape[0], target_shape[1]
            
            # Optimized pooling using einsum patterns for better memory efficiency
            if h > target_h:
                # Pool along first dimension using vectorized operations
                pool_factor = h // target_h
                remainder = h % target_h
                
                if remainder == 0:
                    # Perfect division - use einsum for efficient reshape + mean
                    # einsum 'hpw->hw' where h=target_h, p=pool_factor, w=width
                    aligned_tensor = torch.einsum('hpw->hw', 
                        aligned_tensor[:target_h * pool_factor].view(target_h, pool_factor, w))
                else:
                    # Handle remainder with efficient concatenation
                    main_part = torch.einsum('hpw->hw',
                        aligned_tensor[:target_h * pool_factor].view(target_h, pool_factor, w))
                    remainder_part = aligned_tensor[target_h * pool_factor:].mean(dim=0, keepdim=True)
                    aligned_tensor = torch.cat([main_part, remainder_part], dim=0)[:target_h]
            
            if aligned_tensor.size(1) > target_w:
                # Pool along second dimension using vectorized operations
                h_curr, w_curr = aligned_tensor.shape
                pool_factor = w_curr // target_w
                remainder = w_curr % target_w
                
                if remainder == 0:
                    # Perfect division - use einsum for efficient reshape + mean
                    # einsum 'hwp->hw' where h=height, w=target_w, p=pool_factor
                    aligned_tensor = torch.einsum('hwp->hw',
                        aligned_tensor[:, :target_w * pool_factor].view(h_curr, target_w, pool_factor))
                else:
                    # Handle remainder with efficient concatenation
                    main_part = torch.einsum('hwp->hw',
                        aligned_tensor[:, :target_w * pool_factor].view(h_curr, target_w, pool_factor))
                    remainder_part = aligned_tensor[:, target_w * pool_factor:].mean(dim=1, keepdim=True)
                    aligned_tensor = torch.cat([main_part, remainder_part], dim=1)[:, :target_w]
        
        # Handle other dimensions with optimized slicing and padding
        else:
            # Efficient slicing using advanced indexing
            if aligned_tensor.shape != target_shape:
                # Create slice indices once and reuse
                slices = tuple(slice(0, min(aligned_tensor.size(i), target_shape[i])) 
                             for i in range(aligned_tensor.ndim))
                aligned_tensor = aligned_tensor[slices]
                
                # Vectorized padding calculation
                current_shape = aligned_tensor.shape
                pad_needed = any(current_shape[i] < target_shape[i] for i in range(len(current_shape)))
                
                if pad_needed:
                    # Compute padding in one pass using vectorized calculation
                    pad_widths = []
                    for i in range(aligned_tensor.ndim - 1, -1, -1):
                        pad_widths.extend([0, max(0, target_shape[i] - current_shape[i])])
                    
                    aligned_tensor = F.pad(aligned_tensor, pad_widths, mode='constant', value=0)
        
        aligned.append(aligned_tensor)
    
    # Use efficient stacking
    return torch.stack(aligned, dim=stack_dim)

# SDXL block grouping function for preserving WIDEN cross-parameter validation
def _group_parameters_by_blocks(param_names):
    """Group SDXL parameters by individual architectural blocks to preserve cross-parameter context"""
    import re
    from collections import defaultdict
    
    blocks = defaultdict(list)
    
    for param_name in param_names:
        name_lower = param_name.lower()
        
        # Time and class embeddings (critical for temporal/conditional consistency)
        if 'time_embed' in name_lower:
            blocks['time_embedding'].append(param_name)
        elif 'label_emb' in name_lower or 'class_emb' in name_lower:
            blocks['class_embedding'].append(param_name)
        
        # UNet block structure - Extract individual block numbers
        elif 'input_blocks' in name_lower or 'down_blocks' in name_lower:
            # Extract block number (e.g., input_blocks.0, input_blocks.1, etc.)
            block_match = re.search(r'(input_blocks|down_blocks)\.(\d+)', param_name)
            if block_match:
                block_num = block_match.group(2)
                blocks[f'input_block_{block_num}'].append(param_name)
            else:
                blocks['input_blocks_other'].append(param_name)
                
        elif 'middle_block' in name_lower:
            # Extract middle block sub-components if numbered
            block_match = re.search(r'middle_block\.(\d+)', param_name)
            if block_match:
                block_num = block_match.group(1)
                blocks[f'middle_block_{block_num}'].append(param_name)
            else:
                blocks['middle_block'].append(param_name)
                
        elif 'output_blocks' in name_lower or 'up_blocks' in name_lower:
            # Extract block number (e.g., output_blocks.0, output_blocks.1, etc.)
            block_match = re.search(r'(output_blocks|up_blocks)\.(\d+)', param_name)
            if block_match:
                block_num = block_match.group(2)
                blocks[f'output_block_{block_num}'].append(param_name)
            else:
                blocks['output_blocks_other'].append(param_name)
        
        # Attention layers (preserve cross-parameter relationships)
        elif 'cross' in name_lower and ('attn' in name_lower or 'attention' in name_lower):
            blocks['cross_attention'].append(param_name)
        elif 'attn' in name_lower or 'attention' in name_lower or any(x in name_lower for x in ['to_q', 'to_k', 'to_v', 'to_out']):
            blocks['self_attention'].append(param_name)
        
        # Convolution layers
        elif 'conv' in name_lower or 'convolution' in name_lower:
            blocks['convolutions'].append(param_name)
        
        # Normalization layers
        elif any(x in name_lower for x in ['norm', 'group_norm', 'layer_norm']):
            blocks['normalization'].append(param_name)
        
        # Everything else
        else:
            blocks['other'].append(param_name)
    
    # Convert to list of tuples and sort input/output blocks numerically
    result = []
    for block_name, params in blocks.items():
        if params:  # Only include non-empty blocks
            result.append((block_name, params))
    
    # Sort to ensure input_block_0, input_block_1, etc. are in order
    def sort_key(item):
        block_name = item[0]
        if 'input_block_' in block_name:
            try:
                num = int(block_name.split('_')[-1])
                return (0, num)  # Input blocks first, then by number
            except (ValueError, IndexError) as e:
                diagnostic_logger.warning(f"Could not parse input block number from '{block_name}': {e}")
                return (0, 999)
        elif 'middle_block' in block_name:
            if block_name == 'middle_block':
                return (1, 0)
            else:
                try:
                    num = int(block_name.split('_')[-1])
                    return (1, num)
                except (ValueError, IndexError) as e:
                    diagnostic_logger.warning(f"Could not parse middle block number from '{block_name}': {e}")
                    return (1, 999)
        elif 'output_block_' in block_name:
            try:
                num = int(block_name.split('_')[-1])
                return (2, num)  # Output blocks after middle, then by number
            except (ValueError, IndexError) as e:
                diagnostic_logger.warning(f"Could not parse output block number from '{block_name}': {e}")
                return (2, 999)
        else:
            return (3, block_name)  # Other blocks at end, alphabetically
    
    result.sort(key=sort_key)
    return result

# Enhanced WIDEN merging with dynamic compatibility-based strength
# Enhanced WIDEN merging now uses JIT-compiled hot functions for maximum performance

def enhanced_widen_merging_with_dynamic_strength(
    merger,
    merged_model,
    models_to_merge,
    exclude_param_names_regex,
    importance_threshold,
    importance_boost,
    merge_strength,
    min_strength,
    max_strength,
    rank_sensitivity,
    skip_threshold,
    normalization_mode,
    ultra_memory_mode=False
):
    """Enhanced WIDEN merging with dynamic compatibility-based merge strength"""
    
    # Determine devices for hybrid processing
    target_device = next(merged_model.parameters()).device
    
    # Check available VRAM and RAM for smart processing strategy
    vram_available_mb = 0
    ram_available_gb = 0
    
    if torch.cuda.is_available():
        try:
            free_memory, total_memory = torch.cuda.mem_get_info()
            vram_available_mb = free_memory / (1024 * 1024)
            allocated_mb = torch.cuda.memory_allocated() / (1024 * 1024)
        except (RuntimeError, AttributeError) as e:
            memory_logger.warning(f"Could not get CUDA memory info: {e}")
            vram_available_mb = 1000  # Conservative fallback
    
    # Check system RAM and auto-enable ultra memory mode if needed
    try:
        import psutil
        ram_info = psutil.virtual_memory()
        ram_available_gb = ram_info.available / (1024 * 1024 * 1024)
        ram_total_gb = ram_info.total / (1024 * 1024 * 1024)
        ram_used_percent = ram_info.percent
        
        # Log memory info for debugging
        
        if ram_available_gb < 2.0:
            print(f"[WARNING] Very low RAM detected ({ram_available_gb:.1f}GB available). Using ultra conservative processing.")
    except (ImportError, AttributeError) as e:
        memory_logger.warning(f"Could not get system RAM info: {e}")
        ram_available_gb = 4.0  # Conservative fallback
    
    # Simplified processing strategy: Always use CPU to avoid redundant GPU/CPU computation
    # Since we're doing all ranking on CPU anyway, using GPU for intermediate computations is wasteful
    computation_device = torch.device("cpu")
    storage_device = torch.device("cpu")
    ranking_device = torch.device("cpu")
    
    if ram_available_gb < 2.0:
        pass  # Low RAM mode: Conservative CPU processing
    elif ram_available_gb > 4.0:
        pass  # High RAM mode: Optimized CPU processing
    else:
        pass  # Standard mode: CPU processing
    
    # All processing on CPU for maximum efficiency and WIDEN validation
    
    # Memory debugging: Track where the 58GB spike occurs
    
    # Create task vectors efficiently (these contain the deltas we need)
    print("ðŸ”§ Creating TaskVectors...")
    monitor_memory_usage("PRE-TASKVECTOR")
    
    # MEMORY OPTIMIZATION: Adaptive batch size based on available memory and model count
    try:
        import psutil
        ram_gb = psutil.virtual_memory().available / (1024**3)
        if ram_gb < 4:
            batch_size = 1  # Ultra conservative for low memory
        elif ram_gb < 8:
            batch_size = min(2, len(models_to_merge))  # Conservative
        else:
            batch_size = min(3, len(models_to_merge))  # Slightly more aggressive when memory allows
    except ImportError:
        batch_size = min(2, len(models_to_merge))  # Fallback
    models_to_merge_task_vectors = []
    
    performance_logger.info(f"Creating TaskVectors in batches of {batch_size}")
    
    for batch_start in range(0, len(models_to_merge), batch_size):
        batch_end = min(batch_start + batch_size, len(models_to_merge))
        batch_models = models_to_merge[batch_start:batch_end]
        
        monitor_memory_usage(f"PRE-TASKVECTOR-BATCH-{batch_start//batch_size}")
        
        # Create TaskVectors in this batch with memory management
        batch_task_vectors = []
        for i, model_to_merge in enumerate(batch_models):
            global_i = batch_start + i
            memory_logger.debug(f"Creating TaskVector {global_i+1}/{len(models_to_merge)}")
            
            # Create TaskVector with memory-efficient context
            with MemoryEfficientContext(f"TaskVector-{global_i}"):
                tv = TaskVector(merged_model, model_to_merge, exclude_param_names_regex)
                batch_task_vectors.append(tv)
            
            # Debug the TaskVector memory usage (only for first few)
            if global_i < 3:
                # TaskVector memory analysis via monitor_memory_usage
                pass  # Analysis completed
        
        # Add batch to main list
        models_to_merge_task_vectors.extend(batch_task_vectors)
        
        # Cleanup batch references
        del batch_models, batch_task_vectors
        
        # Less frequent cleanup (only after each batch)
        gentle_cleanup()
        monitor_memory_usage(f"POST-TASKVECTOR-BATCH-{batch_start//batch_size}")
    
    performance_logger.info(f"Created {len(models_to_merge_task_vectors)} TaskVectors in {(len(models_to_merge) + batch_size - 1) // batch_size} batches")
    monitor_memory_usage("POST-TASKVECTOR")
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    monitor_memory_usage("POST-TASKVECTOR-CLEANUP")
    
    # MEMORY OPTIMIZATION: Don't create full parameter copy - access original model directly
    # Using direct model parameter access to minimize memory
    # pretrained_param_dict = {} - REMOVED to save 6.5GB
    # Instead, we'll access merged_model.named_parameters() directly when needed
    
    # Transpose token embeddings in TaskVectors only (we don't need separate copies)
    for task_vector in models_to_merge_task_vectors:
        _transpose_token_embeddings(task_vector.task_vector_param_dict)
    # Note: We'll handle transposition when accessing merged_model parameters directly
    
    # Create parameter cache for O(1) lookup instead of O(N) scan
    base_param_dict = dict(merged_model.named_parameters())
    
    # Helper function to get parameter directly from model (saves 6.5GB of memory + O(1) lookup)
    def get_base_param(param_name, device=storage_device):
        """Get parameter directly from base model without storing full copy"""
        if param_name not in base_param_dict:
            raise KeyError(f"Parameter {param_name} not found in base model")
        
        param = base_param_dict[param_name]
        result = param.detach().to(device).float()
        # Apply transpose if needed
        if param_name == "model.embed_tokens.weight":
            result = result.transpose(dim0=0, dim1=1)
        return result
    
    # Get list of parameter names for processing
    param_names_in_model = [name for name, _ in merged_model.named_parameters()]
    
    with torch.no_grad():
        widen_logger.info("Computing differences...")
        
        # Step 1: Use TaskVector deltas directly instead of recomputing everything
        memory_logger.debug("Computing magnitude and direction differences - monitoring for memory spikes")
        monitor_memory_usage("PRE-MAGNITUDE-DIRECTION")
        
        # Use batched computation for better performance and memory efficiency
        performance_logger.info("Using optimized batched tensor operations with mixed precision...")
        
        # Enable mixed precision for bandwidth-bound operations
        use_mixed_precision = (
            torch.cuda.is_available() and 
            torch.cuda.get_device_capability()[0] >= 7  # Only on GPUs that support FP16 efficiently
        )
        
        if use_mixed_precision:
            performance_logger.info("Mixed precision enabled - using FP16 for magnitude/direction computations")
        
        # Enhanced profiling for performance analysis
        # Enable profiling based on environment variable or logger level
        use_profiler = (
            performance_logger.isEnabledFor(logging.DEBUG) or 
            os.environ.get('WIDEN_ENABLE_PROFILER', '').lower() in ('1', 'true', 'yes')
        )
        
        if use_profiler:
            performance_logger.info("Profiler enabled - this will slow down execution but provide detailed performance analysis")
            
            with torch.profiler.profile(
                activities=[
                    torch.profiler.ProfilerActivity.CPU,
                    torch.profiler.ProfilerActivity.CUDA
                ] if torch.cuda.is_available() else [torch.profiler.ProfilerActivity.CPU],
                profile_memory=True,
                record_shapes=True,
                with_stack=True,
                # Only record a subset to avoid huge trace files
                schedule=torch.profiler.schedule(
                    wait=1,    # Skip first step
                    warmup=1,  # Warmup for 1 step  
                    active=3,  # Record 3 steps
                    repeat=1   # Only do this once
                )
            ) as prof:
                # Use mixed precision and memory-efficient context for magnitude/direction computations
                with MemoryEfficientContext("magnitude_direction_computation_profiled"):
                    # No autocast to avoid TorchScript vmap issues
                    models_to_merge_param_magnitude_direction_diff_tuples = _batch_compute_magnitude_direction_diffs(
                        models_to_merge_task_vectors, 
                        param_names_in_model,
                        skip_threshold
                    )
                prof.step()  # Required for scheduled profiling
            
            # Save profiling results with timestamp
            import time
            timestamp = int(time.time())
            trace_file = f"widen_profile_trace_{timestamp}.json"
            prof.export_chrome_trace(trace_file)
            performance_logger.info(f"Saved profiling trace to {trace_file}")
            
            # Show top operations
            key_averages = prof.key_averages(group_by_stack_n=5)
            performance_logger.info("Top 10 operations by time:")
            performance_logger.info(key_averages.table(
                sort_by="cuda_time_total" if torch.cuda.is_available() else "cpu_time_total", 
                row_limit=10
            ))
        else:
            # Use mixed precision and memory-efficient context for magnitude/direction computations
            with MemoryEfficientContext("magnitude_direction_computation"):
                # No autocast to avoid TorchScript vmap issues
                models_to_merge_param_magnitude_direction_diff_tuples = _batch_compute_magnitude_direction_diffs(
                    models_to_merge_task_vectors, 
                    param_names_in_model,
                    skip_threshold
                )
        
        print(f"ðŸ“Š Computed differences for {len(models_to_merge_task_vectors)} models")
        
        # Cleanup after magnitude/direction computation
        force_cleanup()
        
        # Step 2: Enhanced parameter merging with dynamic compatibility-based strength (no autocast for vmap compatibility)
        merged_params = _merge_param_magnitude_direction_with_dynamic_strength(
            models_to_merge_param_magnitude_direction_diff_tuples,
            get_base_param,  # Pass function instead of full parameter dict
            models_to_merge_task_vectors,
            exclude_param_names_regex,
            importance_threshold,
            importance_boost,
            merge_strength,
            min_strength,
            max_strength,
            rank_sensitivity,
            skip_threshold,
            normalization_mode,
            computation_device,
            target_device,
            storage_device,
            ranking_device,
            param_names_in_model
        )
        
        # Transpose back
        _transpose_token_embeddings(merged_params)
    
    return merged_params

def enhanced_widen_merging_with_post_refinement(
    merger,
    merged_model,
    models_to_merge,
    exclude_param_names_regex,
    importance_threshold,
    importance_boost,
    merge_strength,
    min_strength,
    max_strength,
    rank_sensitivity,
    skip_threshold,
    normalization_mode,
    ultra_memory_mode=False,
    enable_post_refinement=True,
    refinement_config=None,
    calibration_data=None
):
    """
    Enhanced WIDEN merging with optional post-merge refinement pipeline.
    
    This function applies the complete pipeline:
    1. Standard WIDEN merge with dynamic strength
    2. Post-merge refinement (optional):
       - Frobenius norm rescaling
       - Low-rank residual injection  
       - Normalization recalibration
       - Mini-finetuning (if calibration data provided)
       - Activation space sharpening
    3. Energy/contrast monitoring
    
    Args:
        (... same as enhanced_widen_merging_with_dynamic_strength ...)
        enable_post_refinement: Whether to apply post-merge refinement
        refinement_config: Dict of refinement settings (uses defaults if None)
        calibration_data: Optional data for calibration-based refinements
        
    Returns:
        Dict containing merged parameters and refinement statistics
    """
    diagnostic_logger.info("ðŸš€ Starting enhanced WIDEN merge with post-refinement pipeline")
    
    # Step 1: Perform standard WIDEN merge
    with MemoryEfficientContext("widen_merge"):
        merged_params = enhanced_widen_merging_with_dynamic_strength(
            merger=merger,
            merged_model=merged_model,
            models_to_merge=models_to_merge,
            exclude_param_names_regex=exclude_param_names_regex,
            importance_threshold=importance_threshold,
            importance_boost=importance_boost,
            merge_strength=merge_strength,
            min_strength=min_strength,
            max_strength=max_strength,
            rank_sensitivity=rank_sensitivity,
            skip_threshold=skip_threshold,
            normalization_mode=normalization_mode,
            ultra_memory_mode=ultra_memory_mode
        )
    
    result = {
        'merged_params': merged_params,
        'refinement_applied': False,
        'refinement_stats': {},
        'energy_contrast_analysis': {}
    }
    
    if not enable_post_refinement:
        diagnostic_logger.info("Post-refinement disabled, returning merged parameters")
        return result
    
    # Step 2: Apply the merged parameters to create a working model for refinement
    try:
        # Create a copy of the merged model for refinement
        import copy
        working_model = copy.deepcopy(merged_model)
        working_model.load_state_dict(merged_params)
        working_model.to(next(merged_model.parameters()).device)
        
        diagnostic_logger.info("ðŸ“Š Computing pre-refinement energy/contrast baseline")
        
        # Baseline energy/contrast measurement
        pre_refinement_stats = compute_layer_energy_stats(working_model)
        
        # Step 3: Apply post-merge refinement pipeline
        if refinement_config is None:
            refinement_config = create_default_refinement_config()
        
        pipeline = PostMergeRefinementPipeline(refinement_config)
        
        monitor_memory_usage("PRE-REFINEMENT")
        
        # Apply refinement pipeline
        refinement_stats = pipeline.apply_full_pipeline(
            merged_model=working_model,
            base_model=merged_model,
            other_models=models_to_merge,
            calibration_data=calibration_data,
            monitor_stats=True
        )
        
        monitor_memory_usage("POST-REFINEMENT")
        
        # Step 4: Compute post-refinement energy/contrast analysis
        diagnostic_logger.info("ðŸ“Š Computing post-refinement energy/contrast analysis")
        
        energy_contrast_comparison = compare_model_energy_contrast(
            merged_model, working_model, calibration_data
        )
        
        # Print summary
        print_energy_contrast_summary(energy_contrast_comparison)
        
        # Step 5: Extract refined parameters
        refined_params = working_model.state_dict()
        
        result.update({
            'merged_params': refined_params,
            'refinement_applied': True,
            'refinement_stats': refinement_stats,
            'energy_contrast_analysis': energy_contrast_comparison,
            'pre_refinement_stats': pre_refinement_stats
        })
        
        diagnostic_logger.info(f"âœ… Post-refinement pipeline complete: {len(refinement_stats['steps_completed'])} steps applied")
        
    except Exception as e:
        diagnostic_logger.error(f"Post-refinement pipeline failed: {e}")
        # Return original merged parameters if refinement fails
        result['refinement_error'] = str(e)
    
    return result

def create_enhanced_merge_with_refinement_config(enable_frobenius=True, enable_low_rank=True, 
                                                 enable_mini_finetune=False, enable_sharpening=False):
    """
    Create a configuration for enhanced WIDEN merge with post-refinement.
    
    Args:
        enable_frobenius: Enable Frobenius norm rescaling
        enable_low_rank: Enable low-rank residual injection  
        enable_mini_finetune: Enable mini-finetuning (requires calibration data)
        enable_sharpening: Enable activation space sharpening
        
    Returns:
        Dictionary of settings for enhanced merge
    """
    return {
        'frobenius_rescaling': enable_frobenius,
        'low_rank_injection': enable_low_rank,
        'svd_rank': 4,
        'injection_strength': 0.3,
        'norm_recalibration': True,
        'mini_finetune': enable_mini_finetune,
        'finetune_steps': 3,
        'finetune_lr': 1e-5,
        'activation_sharpening': enable_sharpening,
        'sharpening_lambda': 0.1
    }

# Helper functions for enhanced WIDEN merging
def _transpose_token_embeddings(param_dict):
    """Transpose token embeddings"""
    for param_name in param_dict:
        if param_name == "model.embed_tokens.weight":
            param_dict[param_name] = param_dict[param_name].transpose(dim0=0, dim1=1)

def _compute_param_magnitude_direction(param_dict, module_dict):
    """Compute magnitude vector and direction matrix for parameters"""
    param_magnitude_dict, param_direction_dict = {}, {}
    
    for param_name in param_dict:
        param_last_name = param_name.split(".")[-1]
        module_name = param_name[:-len(f".{param_last_name}")]
        
        if param_dict[param_name].dim() == 1:
            # Handle 1D parameters (bias, norm) - treat each element as a feature
            param_tensor = param_dict[param_name]
            magnitude_vector = torch.abs(param_tensor)  # For 1D, magnitude is just absolute value
            direction_matrix = torch.sign(param_tensor)  # For 1D, direction is just the sign
            
            param_magnitude_dict[param_name] = magnitude_vector
            param_direction_dict[param_name] = direction_matrix
            
        elif param_dict[param_name].dim() == 2:
            # Compute magnitude and direction for 2D parameters
            magnitude_vector = torch.norm(param_dict[param_name], p=2, dim=0)
            direction_matrix = param_dict[param_name] / (magnitude_vector + 1e-8)
            
            param_magnitude_dict[param_name] = magnitude_vector
            param_direction_dict[param_name] = direction_matrix
            
        elif param_dict[param_name].dim() > 2:
            # Handle higher dimensional parameters (conv layers, etc.)
            # Preserve per-channel structure by averaging only spatial dims
            tensor = param_dict[param_name]
            
            # Collapse only dims beyond the first two, preserving [out_features, in_features]
            if tensor.ndim > 2:
                # Mean over all spatial/extra dims, keep channel structure
                flat = tensor.mean(dim=tuple(range(2, tensor.ndim)))
            else:
                flat = tensor
            
            magnitude_vector = torch.norm(flat, p=2, dim=0)
            direction_matrix = flat / (magnitude_vector + 1e-8)
            
            param_magnitude_dict[param_name] = magnitude_vector
            param_direction_dict[param_name] = direction_matrix
    
    return param_magnitude_dict, param_direction_dict

def _compute_param_magnitude_direction_differences(pretrained_param_magnitude_dict, pretrained_param_direction_dict,
                                                 finetuned_param_magnitude_dict, finetuned_param_direction_dict, module_dict):
    """Compute magnitude and direction differences"""
    param_magnitude_diff_dict, param_direction_diff_dict = {}, {}
    
    for param_name in pretrained_param_magnitude_dict:
        # Ensure tensors are on the same device
        pretrained_mag = pretrained_param_magnitude_dict[param_name]
        finetuned_mag = finetuned_param_magnitude_dict[param_name]
        pretrained_dir = pretrained_param_direction_dict[param_name]
        finetuned_dir = finetuned_param_direction_dict[param_name]
        
        # Move to same device (prefer CUDA if available)
        target_device = pretrained_mag.device
        if finetuned_mag.device != target_device:
            finetuned_mag = finetuned_mag.to(target_device)
        if finetuned_dir.device != target_device:
            finetuned_dir = finetuned_dir.to(target_device)
        if pretrained_dir.device != target_device:
            pretrained_dir = pretrained_dir.to(target_device)
        
        # Compute magnitude difference
        param_magnitude_diff = torch.abs(finetuned_mag - pretrained_mag)
        
        # Compute direction difference
        param_direction_diff = 1.0 - torch.cosine_similarity(
            finetuned_dir,
            pretrained_dir,
            dim=0
        )
        
        param_magnitude_diff_dict[param_name] = param_magnitude_diff
        param_direction_diff_dict[param_name] = param_direction_diff
    
    return param_magnitude_diff_dict, param_direction_diff_dict

# Shared pooling kernels to reduce allocations
_POOLING_KERNELS = {}

def _get_pooling_kernel(target_size):
    """Get or create shared adaptive pooling kernel"""
    key = tuple(target_size) if isinstance(target_size, (list, tuple)) else target_size
    if key not in _POOLING_KERNELS:
        _POOLING_KERNELS[key] = torch.nn.AdaptiveAvgPool2d(target_size)
    return _POOLING_KERNELS[key]

def _batch_compute_magnitude_direction_diffs(task_vectors, param_names, skip_threshold=0.0):
    """
    Batch compute magnitude and direction differences for multiple task vectors.
    Replaces O(N*M) individual operations with batched O(M) operations.
    Includes early-exit optimization for low-magnitude parameters.
    Uses shared pooling kernels for efficiency.
    """
    result_tuples = []
    skipped_params = 0
    
    # Group parameters by name and stack deltas across models
    param_delta_stacks = {}
    
    # First pass: collect and stack deltas for each parameter with early exit
    for param_name in param_names:
        deltas = []
        max_magnitude = 0.0
        
        for task_vector in task_vectors:
            if param_name in task_vector.task_vector_param_dict:
                delta = task_vector.task_vector_param_dict[param_name]
                deltas.append(delta)
                # Quick magnitude check for early exit
                if skip_threshold > 0.0:
                    param_magnitude = delta.abs().max().item()
                    max_magnitude = max(max_magnitude, param_magnitude)
            else:
                # Create zero tensor with proper shape
                if deltas:
                    deltas.append(torch.zeros_like(deltas[0]))
                # If no deltas exist yet, skip this parameter
        
        # Early exit for low-magnitude parameters (saves 80%+ of processing)
        if skip_threshold > 0.0 and max_magnitude < skip_threshold:
            skipped_params += 1
            param_delta_stacks[param_name] = "SKIPPED"
            continue
            
        if deltas:
            try:
                # Use safe_stack for robust alignment
                param_delta_stacks[param_name] = safe_stack(deltas, dim=0)  # Shape: [num_models, ...]
            except Exception as e:
                print(f"[WARNING] Failed to stack deltas for {param_name}: {e}")
                # Fallback to individual processing for this parameter
                param_delta_stacks[param_name] = None
    
    # Second pass: batch compute magnitude and direction for each parameter
    for task_vector_idx in range(len(task_vectors)):
        magnitude_diffs = {}
        direction_diffs = {}
        
        for param_name, stacked_deltas in param_delta_stacks.items():
            if stacked_deltas == "SKIPPED":
                # Skip low-magnitude parameters entirely
                continue
            elif stacked_deltas is None:
                # Fallback to individual processing
                if param_name in task_vectors[task_vector_idx].task_vector_param_dict:
                    delta = task_vectors[task_vector_idx].task_vector_param_dict[param_name]
                    magnitude_diffs[param_name], direction_diffs[param_name] = _compute_single_param_diffs(delta, use_gpu_compute=True, use_mixed_precision=True)
                continue
                
            # Extract this model's delta from the stack
            delta = stacked_deltas[task_vector_idx]
            magnitude_diffs[param_name], direction_diffs[param_name] = _compute_single_param_diffs(delta, use_gpu_compute=True, use_mixed_precision=True)
        
        result_tuples.append((magnitude_diffs, direction_diffs))
    
    # Performance reporting
    if skipped_params > 0:
        total_params = len(param_names)
        performance_logger.info(f"Early-exit optimization: {skipped_params}/{total_params} parameters skipped ({100*skipped_params/total_params:.1f}%)")
    
    return result_tuples

def _analyze_compatibility_patterns_and_recommend_threshold(compatibility_scores, current_skip_threshold=0.0):
    """
    Analyze compatibility score distribution and recommend optimal skip threshold.
    Only suggests new thresholds when current setting is clearly suboptimal.
    """
    if not compatibility_scores:
        return current_skip_threshold, "No compatibility scores available for analysis"
    
    import numpy as np
    
    # Debug: Track function calls to detect duplicate calls
    if not hasattr(_analyze_compatibility_patterns_and_recommend_threshold, '_call_count'):
        _analyze_compatibility_patterns_and_recommend_threshold._call_count = 0
        _analyze_compatibility_patterns_and_recommend_threshold._last_scores = None
    
    _analyze_compatibility_patterns_and_recommend_threshold._call_count += 1
    call_num = _analyze_compatibility_patterns_and_recommend_threshold._call_count
    
    # Convert to numpy for statistical analysis
    scores = np.array(list(compatibility_scores.values()))
    
    # Analyze compatibility score patterns
    
    _analyze_compatibility_patterns_and_recommend_threshold._last_scores = scores.copy()
    
    # Remove extreme outliers (beyond 3 standard deviations)
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    filtered_scores = scores[np.abs(scores - mean_score) <= 3 * std_score]
    
    # Statistical analysis
    q10, q25, q50, q75, q90 = np.percentile(filtered_scores, [10, 25, 50, 75, 90])
    min_score, max_score = np.min(filtered_scores), np.max(filtered_scores)
    
    # Detect uniform score problem (like the 0.001463 issue)
    score_range = max_score - min_score
    relative_variation = score_range / mean_score if mean_score > 0 else 0
    
    # Don't recommend changes if user has set a reasonable threshold
    if current_skip_threshold > 0.000001:  # User has set a custom threshold
        current_effectiveness = np.sum(filtered_scores <= current_skip_threshold) / len(filtered_scores)
        if 0.05 <= current_effectiveness <= 0.8:  # Current threshold is working reasonably (5-80% filtered)
            return current_skip_threshold, f"Using current threshold: {current_skip_threshold:.6f} (filtering {current_effectiveness:.1%} of parameters)"
    
    # Recommendation logic based on distribution analysis
    recommendations = []
    
    if relative_variation < 0.1:  # Less than 10% variation
        # Uniform scores detected - recommend conservative threshold
        recommended_threshold = max(q75, current_skip_threshold)
        recommendations.append(f"âš ï¸  UNIFORM SCORES DETECTED (variation: {relative_variation:.1%})")
        recommendations.append(f"   Many parameters have nearly identical compatibility (~{mean_score:.6f})")
        recommendations.append(f"   Recommended skip_threshold: {recommended_threshold:.6f} (75th percentile)")
        recommendations.append(f"   This would skip {np.sum(filtered_scores <= recommended_threshold)/len(filtered_scores):.1%} of low-impact parameters")
        
    elif q25 > 0.01:  # Most scores are reasonably high
        # Normal distribution - recommend based on lower quartile
        recommended_threshold = max(q10, current_skip_threshold)
        recommendations.append(f"âœ… HEALTHY SCORE DISTRIBUTION (range: {min_score:.6f} - {max_score:.6f})")
        recommendations.append(f"   Recommended skip_threshold: {recommended_threshold:.6f} (10th percentile)")
        recommendations.append(f"   This would skip {np.sum(filtered_scores <= recommended_threshold)/len(filtered_scores):.1%} of lowest-impact parameters")
        
    else:  # Many very low scores
        # Heavy low-end distribution - recommend more aggressive filtering
        recommended_threshold = max(q50, current_skip_threshold * 2)
        recommendations.append(f"ðŸ“Š HEAVY LOW-END DISTRIBUTION detected")
        recommendations.append(f"   {np.sum(filtered_scores <= 0.01)/len(filtered_scores):.1%} of parameters have compatibility < 0.01")
        recommendations.append(f"   Recommended skip_threshold: {recommended_threshold:.6f} (median)")
        recommendations.append(f"   This would skip {np.sum(filtered_scores <= recommended_threshold)/len(filtered_scores):.1%} of low-impact parameters")
    
    # Performance impact estimate
    potential_savings = np.sum(filtered_scores <= recommended_threshold) / len(filtered_scores)
    if potential_savings > 0.5:
        recommendations.append(f"ðŸš€ PERFORMANCE BOOST: {potential_savings:.1%} parameter reduction possible")
    elif potential_savings > 0.2:
        recommendations.append(f"âš¡ MODERATE SPEEDUP: {potential_savings:.1%} parameter reduction possible")
    
    # Memory impact for very low scores (like the 0.001463 pattern)
    very_low_count = np.sum(filtered_scores <= 0.005)  # Threshold for "very low impact"
    if very_low_count > len(filtered_scores) * 0.3:  # More than 30% very low
        recommendations.append(f"ðŸ’¾ MEMORY OPTIMIZATION: {very_low_count}/{len(filtered_scores)} parameters have minimal impact")
        recommendations.append(f"   Consider skip_threshold â‰¥ 0.005 for significant memory savings")
    
    analysis_summary = "\n".join([
        f"[SKIP THRESHOLD ANALYSIS]",
        f"  Analyzed {len(filtered_scores)} compatibility scores",
        f"  Distribution: min={min_score:.6f}, q25={q25:.6f}, median={q50:.6f}, q75={q75:.6f}, max={max_score:.6f}",
        f"  Relative variation: {relative_variation:.1%} ({'LOW' if relative_variation < 0.1 else 'NORMAL' if relative_variation < 0.5 else 'HIGH'})",
        ""
    ] + recommendations)
    
    return recommended_threshold, analysis_summary

def _compute_single_param_diffs(delta: torch.Tensor, use_gpu_compute=True, use_mixed_precision=True):
    """Single parameter diff computation with smart device placement and mixed precision"""
    
    # Smart device placement: compute on GPU if available, then move small result to CPU
    original_device = delta.device
    compute_device = delta.device
    
    if use_gpu_compute and torch.cuda.is_available() and not delta.is_cuda:
        compute_device = torch.device('cuda')
        delta = delta.to(compute_device)
    
    # Mixed precision for memory efficiency (norms only need ~1e-4 precision)
    if use_mixed_precision and delta.dtype == torch.float32:
        delta = delta.half()
    
    if delta.dim() == 1:
        # 1D: Keep as-is, no need to pool
        magnitude_diff = torch.abs(delta)
        direction_diff = delta
    elif delta.dim() == 2:
        # 2D (Linear weights): Pool along one axis to preserve structure
        if delta.size(0) <= delta.size(1):
            magnitude_diff = torch.norm(delta, p=2, dim=0)  # Shape: [features]
            direction_diff = delta.mean(dim=0)              # Shape: [features]
        else:
            magnitude_diff = torch.norm(delta, p=2, dim=1)  # Shape: [features]
            direction_diff = delta.mean(dim=1)              # Shape: [features]
    elif delta.dim() == 4:
        # 4D (Conv weights): Pool spatial dims but preserve channel structure
        # Use shared pooling kernel for efficiency
        pooling_kernel = _get_pooling_kernel((1, 1))
        spatial_pooled = pooling_kernel(delta).squeeze(-1).squeeze(-1)
        magnitude_diff = torch.norm(spatial_pooled, p=2, dim=1)  # Shape: [C_out]
        direction_diff = spatial_pooled.mean(dim=1)               # Shape: [C_out]
    else:
        # Higher-D: Pool to preserve primary structure
        pooled_dims = tuple(range(2, delta.dim()))
        pooled_delta = delta.mean(dim=pooled_dims)
        
        # Add dimension check before norm calculation
        if pooled_delta.dim() > 1:
            magnitude_diff = torch.norm(pooled_delta, p=2, dim=1)  # Shape: [dim0]
            direction_diff = pooled_delta.mean(dim=1)               # Shape: [dim0]
        else:
            # 1D case: use absolute value instead of norm
            magnitude_diff = torch.abs(pooled_delta)               # Shape: [dim0]
            direction_diff = pooled_delta                          # Shape: [dim0]
    
    # Convert back to float32 and move small results to original device for compatibility
    if use_mixed_precision and magnitude_diff.dtype == torch.float16:
        magnitude_diff = magnitude_diff.float()
        direction_diff = direction_diff.float()
    
    if compute_device != original_device:
        magnitude_diff = magnitude_diff.to(original_device)
        direction_diff = direction_diff.to(original_device)
    
    return magnitude_diff, direction_diff

# Fast tensor ranking (JIT-compiled for performance)
@torch.jit.script
def _fast_tensor_ranking(tensor: torch.Tensor) -> torch.Tensor:
    """Fast ranking for tensors that fit in memory (JIT-compiled)"""
    num_models, num_features = tensor.shape
    device = tensor.device
    
    # Sort and create ranks
    sort_indices = torch.argsort(tensor, dim=1, descending=False, stable=True)
    ranks = torch.arange(num_features, device=device, dtype=torch.float32) / num_features
    ranks = ranks.unsqueeze(0).expand(num_models, -1)
    
    # Apply ranks
    result = torch.zeros_like(tensor)
    result.scatter_(1, sort_indices, ranks)
    return result

def _chunked_rank_computation(tensor: torch.Tensor, chunk_size: int = 32) -> torch.Tensor:
    """JIT-compiled chunked ranking to reduce memory spikes"""
    device = tensor.device
    num_models, num_features = tensor.shape
    
    if num_features <= chunk_size:
        # Small tensor - use JIT-compiled fast version
        return _fast_tensor_ranking(tensor)
    
    # Large tensor - process in chunks
    result = torch.zeros_like(tensor)
    for start_idx in range(0, num_features, chunk_size):
        end_idx = min(start_idx + chunk_size, num_features)
        chunk = tensor[:, start_idx:end_idx]
        chunk_size_actual = end_idx - start_idx
        
        sort_indices = torch.argsort(chunk, dim=1, descending=False, stable=True)
        ranks = torch.arange(chunk_size_actual, device=device, dtype=torch.float32) / chunk_size_actual
        ranks = ranks.unsqueeze(0).expand(num_models, -1)
        
        chunk_result = torch.zeros_like(chunk)
        chunk_result.scatter_(1, sort_indices, ranks)
        result[:, start_idx:end_idx] = chunk_result
        
    return result

def _rank_per_param_magnitude_or_direction_within_model(models_to_merge_param_diff):
    """Rank the magnitude or direction within model with memory optimization"""
    # Use chunked computation for large tensors to reduce memory spikes
    if models_to_merge_param_diff.numel() > 100_000:  # ~400KB threshold
        return _chunked_rank_computation(models_to_merge_param_diff)
    
    # Original implementation for smaller tensors
    device = models_to_merge_param_diff.device
    
    sort_indices = torch.argsort(models_to_merge_param_diff, dim=1, descending=False, stable=True)
    within_model_significance = (torch.arange(models_to_merge_param_diff.shape[1], device=device) / models_to_merge_param_diff.shape[1]).repeat(
        models_to_merge_param_diff.shape[0]
    ).reshape(models_to_merge_param_diff.shape)
    
    models_to_merge_param_within_model_significance = torch.zeros(within_model_significance.shape, device=device)
    models_to_merge_param_within_model_significance = torch.scatter(
        input=models_to_merge_param_within_model_significance,
        dim=1,
        index=sort_indices,
        src=within_model_significance
    )
    
    return models_to_merge_param_within_model_significance

# Core importance scoring (JIT-compiled for performance)
@torch.jit.script
def _compute_importance_scores_core(input_significance_tensor: torch.Tensor, 
                                       above_average_value_ratio: float = 1.0, 
                                       score_calibration_value: float = 1.0):
    """Core importance scoring logic - pure tensor operations for vmap compatibility"""
    # Handle scalar or 1D tensors (e.g. clip.logit_scale) as uniform scores
    if input_significance_tensor.ndim <= 1:
        return torch.ones_like(input_significance_tensor), torch.tensor(True, device=input_significance_tensor.device)
    
    # Check if input is uniform (all values nearly identical) - use mask instead of if/else
    tensor_variance = torch.var(input_significance_tensor)
    is_uniform = tensor_variance < 1e-8
    
    # Uniform scores fallback (always computed, selected by mask)
    uniform_scores = torch.full_like(input_significance_tensor, 1.0 / input_significance_tensor.numel())
    
    # Non-uniform scores path - compute softmax scores
    # Handle dimension cases using tensor operations instead of if/else
    dim_0_softmax = torch.softmax(input_significance_tensor, dim=0)  # Multi-model case
    dim_1_softmax = torch.softmax(input_significance_tensor, dim=1)  # Single model case
    
    # Select appropriate softmax based on tensor dimensions (pure tensor operation)
    is_single_model = (input_significance_tensor.dim() == 2) and (input_significance_tensor.size(0) == 1)
    importance_scores = torch.where(
        torch.tensor(is_single_model, device=input_significance_tensor.device),
        dim_1_softmax,
        dim_0_softmax
    )
    
    # Apply above-average boost (pure tensor operations)
    if above_average_value_ratio != 1.0 and above_average_value_ratio > 0.0:
        mean_score = importance_scores.mean()
        above_avg_mask = importance_scores > mean_score
        importance_scores = torch.where(
            above_avg_mask,
            importance_scores * above_average_value_ratio,
            importance_scores
        )
        
        # Renormalize to maintain probability distribution
        total = importance_scores.sum()
        importance_scores = torch.where(
            total > 1e-8,
            importance_scores / total,
            importance_scores
        )
    
    # Apply score calibration (pure tensor operations)
    if score_calibration_value != 1.0 and score_calibration_value > 0.0:
        importance_scores = importance_scores * score_calibration_value
        
        # Renormalize
        total = importance_scores.sum()
        importance_scores = torch.where(
            total > 1e-8,
            importance_scores / total,
            importance_scores
        )
    
    # Select between uniform and non-uniform scores based on variance mask
    final_scores = torch.where(is_uniform, uniform_scores, importance_scores)
    
    return final_scores, is_uniform  # Return scores and uniform flag tensor

def _compute_importance_scores(input_significance_tensor, above_average_value_ratio=1.0, score_calibration_value=1.0):
    """
    Compute importance scores (fixed WIDEN algorithm) - JIT-optimized
    Returns: (importance_scores, is_uniform_flag)
    """
    # Use JIT-compiled core for maximum performance
    return _compute_importance_scores_core(input_significance_tensor, above_average_value_ratio, score_calibration_value)

def _compatibility_to_merge_strength(compatibility_score, merge_strength, min_strength, max_strength, sensitivity):
    """
    Convert compatibility score to merge strength using merge_strength as base multiplier.
    
    CRITICAL BEHAVIOR NOTES FOR FUTURE DEBUGGING:
    
    1. MERGE STRENGTH SCALING:
       final_strength = merge_strength * (min_strength + range * sigmoid_factor)
       
       This means with default min_strength=0.5, max_strength=1.5:
       - merge_strength=0.01 â†’ actual range: 0.005 to 0.015 (0.5% to 1.5% change)
       - merge_strength=1.0  â†’ actual range: 0.5 to 1.5 (50% to 150% change)
    
    2. FOR NEAR-BASE RESULTS:
       To get merge_strength=0.01 behaving like 1% max change:
       - Set min_strength=0.0, max_strength=1.0
       - This gives actual range: 0.0 to 0.01 (0% to 1% change)
    
    3. WHY DEFAULT VALUES ARE HIGH:
       min_strength=0.5 is designed for normal merging (merge_strength â‰¥ 0.5)
       where you want meaningful parameter changes, not micro-adjustments.
       
    4. IMPORTANCE_BOOST AMPLIFICATION:
       ComfyUI default importance_boost=2.5 further amplifies the effect!
       merge_strength=0.01 Ã— importance_boost=2.5 â†’ 0.025 (2.5% effective change)
       
    This is correct behavior by design, but users need to adjust ALL parameters
    (min/max_strength AND importance_boost) for very low merge_strength values.
    """
    
    # Special case: merge_strength=0 should return 0 (base model unchanged)
    if merge_strength == 0.0:
        return 0.0
    
    # If sensitivity is 0, disable dynamic strength (use merge_strength directly)
    if sensitivity == 0.0:
        return float(merge_strength)
    
    # Normalize compatibility scores to a reasonable range for sigmoid
    # Most compatibility scores are very small (0.0001-0.01), so we need to scale them
    # Use the compatibility score directly without centering on 0.5, which is too high
    
    # Scale the raw compatibility score for better sigmoid behavior
    # For typical compatibility scores (0.0001-0.01), this provides better distribution
    scaled_compatibility = compatibility_score * 100.0  # Scale up small values
    sigmoid_input = (scaled_compatibility - 0.5) * sensitivity
    
    # Apply sigmoid with numerical stability
    if sigmoid_input > 20:
        sigmoid_output = 1.0
    elif sigmoid_input < -20:
        sigmoid_output = 0.0
    else:
        sigmoid_output = 1.0 / (1.0 + np.exp(-sigmoid_input))
    
    # Map sigmoid to strength multiplier range: min_strength to max_strength  
    strength_multiplier = min_strength + (max_strength - min_strength) * sigmoid_output
    
    # Apply merge_strength as the final scaling factor
    # This ensures merge_strength=1.0 gives you the full range, merge_strength=0.5 gives half effect
    final_strength = merge_strength * strength_multiplier
    
    return float(final_strength)

# Core tensor multiplication (JIT-compiled for performance)
@torch.jit.script
def _safe_tensor_multiply_core(delta_param: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
    """
    Multiply delta_param by weights, handling either perâ€‘batch or perâ€‘feature vectors.

    delta_param: [B, F, â€¦]  (B=models or batch, F=features, possibly spatial dims after)
    weights: either
      â€¢ scalar (0â€‘D) â†’ scale everything, or
      â€¢ lengthâ€‘F vector (1â€‘D) â†’ broadcast along dim=1, or
      â€¢ lengthâ€‘B vector (1â€‘D) â†’ broadcast along dim=0
    """
    # Promote to float32
    d = delta_param.to(torch.float32)
    w = weights.to(torch.float32)

    # scalar case
    if w.dim() == 0:
        return d * w

    # perâ€‘feature: if weight size matches F (second dim)
    if w.dim() == 1 and d.dim() >= 2 and w.size(0) == d.size(1):
        # broadcast [1, F, 1, 1â€¦]
        shape = [1, w.size(0)] + [1] * (d.dim() - 2)
        return d * w.reshape(shape)

    # perâ€‘batch: if weight size matches B (first dim)
    if w.dim() == 1 and d.dim() >= 1 and w.size(0) == d.size(0):
        shape = [w.size(0)] + [1] * (d.dim() - 1)
        return d * w.reshape(shape)

    # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    # tailâ€‘match case: if weights.shape matches the last k dims of delta_param
    # e.g. weights=[F,H,W] vs d.shape=[B,F,H,W], or [H,W] vs [B,F,H,W].
    if w.dim() >= 1 and w.dim() <= d.dim():
        # compare weights shape to the last w.dim() dims of d
        match = True
        for i in range(w.dim()):
            if d.size(d.dim() - w.dim() + i) != w.size(i):
                match = False
                break
        
        if match:
            # build broadcast shape: [1Ã—(d.dim()-w.dim())] + list(w.shape)
            shape = [1] * (d.dim() - w.dim()) + list(w.shape)
            return d * w.reshape(shape)

    # lastâ€‘resort: safe fallback for incompatible shapes
    # If dimensions are completely incompatible, use scalar fallback
    if w.dim() == 1 and d.dim() >= 1:
        # For cases where dimensions don't match any pattern, use mean as scalar
        return d * w.mean()
    
    # Final fallback: always use scalar multiplication to avoid dimension crashes
    return d * w.mean()

def _safe_tensor_multiply(models_to_merge_delta_param, weight_scores, param_name, failure_tracker=None):
    """
    Safely multiply delta tensors with weight scores, handling dimension mismatches.
    Returns weighted_deltas or None if operation fails.
    """
    try:
        # Apply neuron alignment for linear layers if appropriate
        if SCIPY_AVAILABLE and models_to_merge_delta_param.ndim == 2 and "weight" in param_name.lower():
            # Check if this looks like a linear layer (2D weight matrix)
            if models_to_merge_delta_param.shape[0] > 1 and models_to_merge_delta_param.shape[1] > 1:
                # For multi-model case, align each model to the first one
                if models_to_merge_delta_param.shape[0] > 1:  # Multiple models
                    aligned_deltas = models_to_merge_delta_param.clone()
                    base_delta = aligned_deltas[0]  # Use first model as reference
                    
                    for i in range(1, aligned_deltas.shape[0]):
                        aligned_deltas[i] = align_linear_layer(base_delta, aligned_deltas[i])
                    
                    models_to_merge_delta_param = aligned_deltas
        
        # Apply automatic embedding transpose if needed
        if "embed" in param_name.lower() and models_to_merge_delta_param.ndim >= 2:
            # Get hint for hidden size from tensor shape
            hidden_size_hint = models_to_merge_delta_param.shape[-1]
            models_to_merge_delta_param = transpose_embeddings_if_needed(
                param_name, models_to_merge_delta_param, hidden_size_hint
            )
        
        # First try the JIT-compiled core for maximum performance
        return _safe_tensor_multiply_core(models_to_merge_delta_param, weight_scores)
            
    except Exception as e:
        # Fallback: try to find matching dimensions for complex cases
        try:
            delta = models_to_merge_delta_param.to(torch.float32)
            weights = weight_scores.to(torch.float32)
            
            # Handle edge cases that JIT core might miss
            if weights.dim() == 1 and delta.dim() >= 2:
                # Look for a dimension that matches weights.size(0)
                for axis in range(delta.dim()):
                    if delta.size(axis) == weights.size(0):
                        # Create shape that broadcasts weights along this axis
                        shape = [1] * delta.dim()
                        shape[axis] = weights.size(0)
                        weights_shaped = weights.reshape(shape)
                        return delta * weights_shaped
                        
            # If no specific match found, try general broadcasting
            return delta * weights
                        
        except Exception as e2:
            # Final fallback to complex alignment
            try:
                return _align_and_multiply_tensors(models_to_merge_delta_param, weight_scores, param_name)
            except Exception as e3:
                # Record the failure if tracker is provided
                if failure_tracker:
                    failure_tracker.record_failure(param_name, e3, fallback_used=False)
                diagnostic_logger.warning(f"All tensor multiplication methods failed for {param_name}: {e}, {e2}, {e3}")
                return None

def _align_and_multiply_tensors(models_to_merge_delta_param, weight_scores, param_name):
    """Helper function for complex tensor alignment cases using structure-preserving operations"""
    
    # First, try to preserve structure by using mean to collapse extra dimensions
    delta = models_to_merge_delta_param
    weights = weight_scores
    
    try:
        # If delta has more than 2 dims, preserve first 2 dims and average the rest
        if delta.dim() > 2:
            # Preserve [out_features, in_features] structure, average spatial/extra dims
            condensed_delta = delta.mean(dim=tuple(range(2, delta.ndim)))
        else:
            condensed_delta = delta
        
        # Similarly condense weights if needed
        if weights.dim() > 2:
            condensed_weights = weights.mean(dim=tuple(range(2, weights.ndim)))
        else:
            condensed_weights = weights
        
        # Now try multiplication on the condensed tensors
        if condensed_delta.dim() == 2 and condensed_weights.dim() == 1:
            # Standard case: 2D delta with 1D weights
            if condensed_weights.size(0) == condensed_delta.size(0):
                # Per-output-channel weighting
                result = condensed_delta * condensed_weights.unsqueeze(1)
            elif condensed_weights.size(0) == condensed_delta.size(1):
                # Per-input-channel weighting
                result = condensed_delta * condensed_weights.unsqueeze(0)
            else:
                # Fallback: broadcast or use mean
                result = condensed_delta * condensed_weights.mean()
        else:
            # Try broadcasting the condensed tensors
            result = condensed_delta * condensed_weights
            
        # If original delta had spatial dims, expand result back with proper broadcasting
        if delta.dim() > 2 and result.dim() == 2:
            # Create the proper broadcast shape
            target_shape = list(delta.shape)
            # Result should broadcast to delta's shape
            result = result.view(target_shape[0], target_shape[1], *([1] * (delta.dim() - 2)))
            result = result.expand(delta.shape)
                
        return result
        
    except Exception as e:
        diagnostic_logger.debug(f"Structure-preserving alignment failed for {param_name}: {e}, using fallback")
        
        # Fallback to original logic for compatibility
        if models_to_merge_delta_param.dim() == 4:  # Conv weights
            target_shape = models_to_merge_delta_param.shape[2:]
            aligned_weights = weight_scores.unsqueeze(-1).unsqueeze(-1)
            aligned_weights = F.adaptive_avg_pool2d(aligned_weights, target_shape)
            return models_to_merge_delta_param * aligned_weights
        else:
            # Robust axis-matching approach: find where weight_scores shape matches delta tensor
            S = tuple(models_to_merge_delta_param.shape)
            R = tuple(weight_scores.shape)
            
            # Find the axis where R matches a slice of S
            for axis in range(len(S) - len(R) + 1):
                if S[axis:axis+len(R)] == R:
                    # Insert singleton dimensions to align properly
                    aligned_weights = weight_scores.view(
                        *([1] * axis), *R, *([1] * (len(S) - axis - len(R)))
                    )
                    return models_to_merge_delta_param * aligned_weights
            
            # Final fallback: broadcast weight_scores along the last dims
            extra = models_to_merge_delta_param.dim() - weight_scores.dim()
            aligned_weights = weight_scores.view(*([1] * extra), *weight_scores.shape)
            return models_to_merge_delta_param * aligned_weights

def create_condensed_delta(delta: torch.Tensor, param_name: str) -> torch.Tensor:
    """
    Create a condensed version of delta that preserves per-channel structure
    while averaging out spatial dimensions for more informative metadata.
    
    Args:
        delta: The parameter delta tensor
        param_name: Name of the parameter for debugging
    
    Returns:
        Condensed delta preserving channel structure
    """
    if delta.ndim <= 2:
        # Already condensed enough
        return delta
    
    # For conv layers and higher-D tensors: preserve [out_ch, in_ch], average spatial
    condensed = delta.mean(dim=tuple(range(2, delta.ndim)))
    
    diagnostic_logger.debug(f"Condensed {param_name}: {delta.shape} -> {condensed.shape}")
    return condensed

def safe_align_and_stack(t1: torch.Tensor, t2: torch.Tensor, keep_dims: int = 2) -> torch.Tensor:
    """
    Safely align and stack two tensors, preserving important dimensions while
    condensing others using mean instead of aggressive flattening.
    
    Args:
        t1, t2: Tensors to align and stack
        keep_dims: Number of leading dimensions to preserve
    
    Returns:
        Stacked tensor with aligned shapes
    """
    # Collapse dims beyond `keep_dims` so shapes match more naturally
    if t1.dim() > keep_dims:
        t1 = t1.mean(dim=tuple(range(keep_dims, t1.dim())))
    if t2.dim() > keep_dims:
        t2 = t2.mean(dim=tuple(range(keep_dims, t2.dim())))
    
    try:
        # Try broadcasting first
        a, b = torch.broadcast_tensors(t1, t2)
        return torch.stack([a, b], dim=0)
    except Exception as e:
        diagnostic_logger.debug(f"Broadcasting failed, using fallback: {e}")
        # Fallback: ensure same shape by padding or truncating
        if t1.shape != t2.shape:
            # Use the smaller shape
            min_shape = [min(s1, s2) for s1, s2 in zip(t1.shape, t2.shape)]
            slices = tuple(slice(0, s) for s in min_shape)
            t1 = t1[slices]
            t2 = t2[slices]
        return torch.stack([t1, t2], dim=0)

# JIT-compiled version for tensor operations
@torch.jit.script
def _fast_sigmoid_strength(compatibility_tensor: torch.Tensor, merge_strength: float, 
                          min_strength: float, max_strength: float, sensitivity: float) -> torch.Tensor:
    """
    JIT-compiled version for batch sigmoid strength computation.
    
    IMPORTANT: This must behave identically to _compatibility_to_merge_strength!
    See that function for detailed behavior documentation.
    """
    if sensitivity == 0.0:
        midpoint = (min_strength + max_strength) / 2.0
        return torch.full_like(compatibility_tensor, merge_strength * midpoint)
    
    # Vectorized sigmoid computation
    sigmoid_input = (compatibility_tensor - 0.5) * sensitivity
    sigmoid_output = torch.sigmoid(sigmoid_input)  # More numerically stable than manual exp
    
    # Map to strength range
    strength_multiplier = min_strength + (max_strength - min_strength) * sigmoid_output
    final_strength = merge_strength * strength_multiplier
    
    return final_strength

def _vectorized_parameter_batch_merge(param_tensors_batch: list, task_vector_deltas_batch: list, 
                                     weight_scores_batch: list, device: torch.device) -> list:
    """
    Vectorized batch processing of multiple parameters using torch.vmap for elimination of sequential overhead
    
    Args:
        param_tensors_batch: List of base parameter tensors  
        task_vector_deltas_batch: List of corresponding task vector deltas
        weight_scores_batch: List of corresponding weight scores
        device: Target computation device
        
    Returns:
        List of merged parameter tensors
    """
    if not param_tensors_batch:
        return []
    
    try:
        # Group parameters by shape for efficient batch processing
        shape_groups = {}
        for i, param_tensor in enumerate(param_tensors_batch):
            shape_key = tuple(param_tensor.shape)
            if shape_key not in shape_groups:
                shape_groups[shape_key] = []
            shape_groups[shape_key].append(i)
        
        merged_results = [None] * len(param_tensors_batch)
        
        # Process each shape group with vectorized operations
        for shape_key, indices in shape_groups.items():
            if len(indices) == 1:
                # Single parameter - no vectorization benefit
                idx = indices[0]
                base_param = param_tensors_batch[idx].to(device, non_blocking=True)
                delta = task_vector_deltas_batch[idx].to(device, non_blocking=True) 
                weights = weight_scores_batch[idx].to(device, non_blocking=True)
                
                merged_results[idx] = base_param + _safe_tensor_multiply_core(delta, weights)
            else:
                # Multiple parameters with same shape - use vectorized batch processing
                batch_base_params = torch.stack([param_tensors_batch[i].to(device, non_blocking=True) for i in indices])
                batch_deltas = torch.stack([task_vector_deltas_batch[i].to(device, non_blocking=True) for i in indices])
                batch_weights = torch.stack([weight_scores_batch[i].to(device, non_blocking=True) for i in indices])
                
                # Vectorized multiplication using vmap for parallel processing
                def single_param_merge(base_param, delta, weights):
                    return base_param + _safe_tensor_multiply_core(delta, weights)
                
                # Use vmap to process entire batch in parallel
                # Force all inputs to float32 to avoid autocast issues  
                batch_base_float32 = batch_base_params.to(torch.float32)
                batch_deltas_float32 = batch_deltas.to(torch.float32)
                batch_weights_float32 = batch_weights.to(torch.float32)
                
                # Explicitly disable autocast around vmap to prevent TorchScript issues
                with torch.cuda.amp.autocast(enabled=False):
                    batch_merged = torch.vmap(single_param_merge)(batch_base_float32, batch_deltas_float32, batch_weights_float32)
                
                # Store results back in original order
                for i, idx in enumerate(indices):
                    merged_results[idx] = batch_merged[i]
        
        return merged_results
        
    except Exception as e:
        print(f"[WARNING] Vectorized batch merge failed: {e}")
        # Fallback to sequential processing
        merged_results = []
        for i in range(len(param_tensors_batch)):
            base_param = param_tensors_batch[i].to(device, non_blocking=True)
            delta = task_vector_deltas_batch[i].to(device, non_blocking=True)
            weights = weight_scores_batch[i].to(device, non_blocking=True)
            merged_results.append(base_param + _safe_tensor_multiply_core(delta, weights))
        return merged_results

def _batch_importance_score_computation(magnitude_diffs_batch: list, direction_diffs_batch: list,
                                       above_average_value_ratio: float, score_calibration_value: float,
                                       device: torch.device) -> tuple:
    """
    Vectorized batch computation of importance scores for multiple parameters
    
    Returns:
        (batch_importance_scores, batch_variances) - Lists of importance scores and variance info
    """
    if not magnitude_diffs_batch:
        return [], []
    
    try:
        # Process parameters in vectorized batches
        batch_importance_scores = []
        batch_variances = []
        
        # Group by compatible tensor shapes for batch processing
        compatible_groups = []
        for i, (mag_diff, dir_diff) in enumerate(zip(magnitude_diffs_batch, direction_diffs_batch)):
            found_group = False
            for group in compatible_groups:
                sample_mag, sample_dir = magnitude_diffs_batch[group[0]], direction_diffs_batch[group[0]]
                if (mag_diff.shape == sample_mag.shape and dir_diff.shape == sample_dir.shape):
                    group.append(i)
                    found_group = True
                    break
            if not found_group:
                compatible_groups.append([i])
        
        # Process each compatible group with vectorization
        results = [None] * len(magnitude_diffs_batch)
        for group_indices in compatible_groups:
            if len(group_indices) == 1:
                # Single parameter
                idx = group_indices[0]
                mag_tensor = magnitude_diffs_batch[idx].to(device, non_blocking=True)
                dir_tensor = direction_diffs_batch[idx].to(device, non_blocking=True)
                
                mag_scores, mag_uniform = _compute_importance_scores_core(mag_tensor, above_average_value_ratio, score_calibration_value)
                dir_scores, dir_uniform = _compute_importance_scores_core(dir_tensor, above_average_value_ratio, score_calibration_value) 
                
                results[idx] = ((mag_scores, dir_scores), {'magnitude_uniform': mag_uniform.item() if hasattr(mag_uniform, 'item') else mag_uniform, 'direction_uniform': dir_uniform.item() if hasattr(dir_uniform, 'item') else dir_uniform})
            else:
                # Batch process compatible parameters
                batch_mag_tensors = torch.stack([magnitude_diffs_batch[i].to(device, non_blocking=True) for i in group_indices])
                batch_dir_tensors = torch.stack([direction_diffs_batch[i].to(device, non_blocking=True) for i in group_indices])
                
                # Vectorized importance score computation
                def compute_single_importance(mag_tensor, dir_tensor):
                    mag_scores, mag_uniform = _compute_importance_scores_core(mag_tensor, above_average_value_ratio, score_calibration_value)
                    dir_scores, dir_uniform = _compute_importance_scores_core(dir_tensor, above_average_value_ratio, score_calibration_value)
                    return mag_scores, dir_scores, mag_uniform, dir_uniform
                
                # Use vmap for parallel batch processing 
                # Force all inputs to float32 to avoid autocast issues
                batch_mag_float32 = batch_mag_tensors.to(torch.float32)
                batch_dir_float32 = batch_dir_tensors.to(torch.float32)
                
                # Explicitly disable autocast around vmap to prevent TorchScript issues
                with torch.cuda.amp.autocast(enabled=False):
                    batch_mag_scores, batch_dir_scores, batch_mag_uniform, batch_dir_uniform = torch.vmap(compute_single_importance)(batch_mag_float32, batch_dir_float32)
                
                # Store results
                for i, idx in enumerate(group_indices):
                    results[idx] = ((batch_mag_scores[i], batch_dir_scores[i]), 
                                   {'magnitude_uniform': batch_mag_uniform[i].item() if hasattr(batch_mag_uniform[i], 'item') else batch_mag_uniform[i], 'direction_uniform': batch_dir_uniform[i].item() if hasattr(batch_dir_uniform[i], 'item') else batch_dir_uniform[i]})
        
        # Extract final results
        for result in results:
            batch_importance_scores.append(result[0])
            batch_variances.append(result[1])
            
        return batch_importance_scores, batch_variances
        
    except Exception as e:
        print(f"[WARNING] Batch importance computation failed: {e}")
        # Fallback to sequential processing
        batch_importance_scores = []
        batch_variances = []
        
        for mag_diff, dir_diff in zip(magnitude_diffs_batch, direction_diffs_batch):
            mag_tensor = mag_diff.to(device, non_blocking=True)
            dir_tensor = dir_diff.to(device, non_blocking=True)
            
            mag_scores, mag_uniform = _compute_importance_scores_core(mag_tensor, above_average_value_ratio, score_calibration_value)
            dir_scores, dir_uniform = _compute_importance_scores_core(dir_tensor, above_average_value_ratio, score_calibration_value)
            
            batch_importance_scores.append((mag_scores, dir_scores))
            batch_variances.append({'magnitude_uniform': mag_uniform.item() if hasattr(mag_uniform, 'item') else mag_uniform, 'direction_uniform': dir_uniform.item() if hasattr(dir_uniform, 'item') else dir_uniform})
        
        return batch_importance_scores, batch_variances

def _process_block_parameters_together(
    block_params,
    param_names_merged_by_magnitude_direction,
    magnitude_rankings,
    direction_rankings,
    models_to_merge_task_vectors,
    get_base_param_func,
    above_average_value_ratio,
    score_calibration_value,
    merge_strength,
    min_strength,
    max_strength,
    rank_sensitivity,
    skip_threshold,
    normalization_mode,
    computation_device,
    target_device,
    widen_diagnostics
):
    """Process all parameters in a block together to preserve WIDEN cross-parameter evaluation"""
    import torch
    # Note: smart_device_management already imported at module level
    # Note: calibrate_renormalize and _compute_importance_scores are defined in this file
    
    block_merged_params = {}
    
    # Collect all delta tensors for all parameters in the block at once
    block_delta_tensors = {}
    block_rankings = {}
    
    # Phase 1: Collect all data for block parameters
    with torch.no_grad():
        for param_name in block_params:
            try:
                # Get delta tensors for this parameter
                if param_name in param_names_merged_by_magnitude_direction and param_name in magnitude_rankings:
                    delta_tensors = []
                    for models_to_merge_task_vector in models_to_merge_task_vectors:
                        if param_name in models_to_merge_task_vector.task_vector_param_dict:
                            delta = models_to_merge_task_vector.task_vector_param_dict[param_name]
                            delta = smart_device_management(delta, computation_device)
                            delta_tensors.append(delta)
                    
                    if delta_tensors:
                        block_delta_tensors[param_name] = torch.stack(delta_tensors, dim=0)
                        block_rankings[param_name] = {
                            'magnitude': magnitude_rankings[param_name].to(computation_device),
                            'direction': direction_rankings[param_name].to(computation_device)
                        }
                        del delta_tensors
                    else:
                        # No deltas - use base parameter
                        widen_diagnostics['parameters_skipped_no_rankings'] += 1
                        if len(widen_diagnostics['compatibility_scores']) < 3:  # Debug first few
                            # No deltas - using base parameter unchanged
                            pass
                        block_merged_params[param_name] = get_base_param_func(param_name, target_device)
                        
            except Exception as e:
                widen_logger.error(f"Failed to collect data for {param_name}: {e}")
                # Fallback to base parameter
                block_merged_params[param_name] = get_base_param_func(param_name, target_device)
    
    # Phase 2: Vectorized batch processing of all collected parameters
    if block_delta_tensors:
        # Prepare batch data for vectorized processing
        param_names_batch = list(block_delta_tensors.keys())
        magnitude_ranks_batch = [block_rankings[name]['magnitude'] for name in param_names_batch]
        direction_ranks_batch = [block_rankings[name]['direction'] for name in param_names_batch] 
        
        # Vectorized importance score computation for the entire batch
        batch_importance_scores, batch_variances = _batch_importance_score_computation(
            magnitude_ranks_batch, direction_ranks_batch, 
            above_average_value_ratio, score_calibration_value, computation_device
        )
        
        # Prepare batch data for parameter merging
        param_tensors_batch = []
        task_vector_deltas_batch = []
        weight_scores_batch = []
        valid_param_indices = []
        
        for idx, param_name in enumerate(param_names_batch):
            try:
                magnitude_scores, direction_scores = batch_importance_scores[idx]
                variance_info = batch_variances[idx]
                
                # WIDEN diagnostics
                widen_diagnostics['parameters_with_rankings'] += 1
                
                # Count uniform scores
                if variance_info['magnitude_uniform']:
                    widen_diagnostics['uniform_score_count'] += 1
                    # Uniform magnitude scores detected
                    if len(widen_diagnostics['compatibility_scores']) < 5:
                        # Uniform magnitude scores detected
                        pass
                else:
                    widen_diagnostics['varied_score_count'] += 1
                    
                if variance_info['direction_uniform']:
                    widen_diagnostics['uniform_score_count'] += 1
                    # Uniform direction scores detected
                    if len(widen_diagnostics['compatibility_scores']) < 5:
                        # Uniform direction scores detected
                        pass
                else:
                    widen_diagnostics['varied_score_count'] += 1
                
                widen_diagnostics['importance_score_variances'].append({
                    'parameter': param_name,
                    'magnitude_variance': torch.var(magnitude_scores).item(),
                    'direction_variance': torch.var(direction_scores).item()
                })
                
                # Check for empty scores
                if magnitude_scores.numel() == 0 or direction_scores.numel() == 0:
                    print(f"[WARNING] Empty scores for {param_name}, using fallback")
                    block_merged_params[param_name] = get_base_param_func(param_name, target_device)
                    continue
                
                # Compute compatibility and dynamic strength
                combined_scores = 0.5 * (magnitude_scores + direction_scores)
                compatibility_score = torch.mean(combined_scores).item()
                
                # Raw compatibility score analysis
                if len(widen_diagnostics['compatibility_scores']) < 10:  # Log first 10 parameters
                    # Parameter compatibility analysis completed
                    pass
                
                widen_diagnostics['compatibility_scores'].append({
                    'parameter': param_name,
                    'compatibility': compatibility_score
                })
                
                # Calculate dynamic strength BEFORE skip check to see what we would get
                parameter_strength = _compatibility_to_merge_strength(
                    compatibility_score, merge_strength, min_strength, max_strength, rank_sensitivity
                )
                
                # Strength calculation analysis
                if len(widen_diagnostics['compatibility_scores']) <= 5:
                    # Parameter strength calculated
                    pass
                
                # Use adaptive skip threshold based on parameter type
                adaptive_threshold = get_adaptive_skip_threshold(param_name, skip_threshold)
                
                # Skip if incompatible
                if abs(compatibility_score) < adaptive_threshold:
                    diagnostic_logger.debug(f"Skipped {param_name}: {compatibility_score:.6f} < {adaptive_threshold:.6f} (adaptive)")
                    widen_diagnostics['parameters_skipped_threshold'] += 1
                    block_merged_params[param_name] = get_base_param_func(param_name, target_device)
                    continue
                
                # Track applied strength for diagnostics
                widen_diagnostics['applied_strengths'].append({
                    'parameter': param_name,
                    'strength': parameter_strength,
                    'compatibility': compatibility_score
                })
                
                # Update strength distribution stats
                dist = widen_diagnostics['strength_distribution']
                if dist['count'] == 0:
                    # First strength value
                    dist['min_used'] = parameter_strength
                    dist['max_used'] = parameter_strength
                    dist['mean'] = parameter_strength
                else:
                    # Subsequent values
                    dist['min_used'] = min(dist['min_used'], parameter_strength)
                    dist['max_used'] = max(dist['max_used'], parameter_strength)
                    dist['mean'] = (dist['mean'] * dist['count'] + parameter_strength) / (dist['count'] + 1)
                dist['count'] += 1
                
                # Check for scalar/1D tensors that need simple blend fallback
                base_param = get_base_param_func(param_name, computation_device)
                delta_param = block_delta_tensors[param_name]
                
                if base_param.dim() <= 1:
                    # Simple blend for scalars/vectors like logit_scale
                    merged_param = base_param + delta_param * parameter_strength
                    block_merged_params[param_name] = merged_param.to(target_device)
                    continue
                
                # Prepare data for vectorized batch merge (multi-dimensional tensors)
                weight_scores = torch.full_like(combined_scores, parameter_strength)
                
                param_tensors_batch.append(base_param)
                task_vector_deltas_batch.append(delta_param)  
                weight_scores_batch.append(weight_scores)
                valid_param_indices.append(idx)
                
            except Exception as e:
                widen_logger.error(f"Failed to process scores for {param_name}: {e}")
                block_merged_params[param_name] = get_base_param_func(param_name, target_device)
        
        # Vectorized batch parameter merging
        if param_tensors_batch:
            try:
                # Vectorized batch merge (no autocast to avoid TorchScript issues)
                merged_params_batch = _vectorized_parameter_batch_merge(
                    param_tensors_batch, task_vector_deltas_batch, weight_scores_batch, computation_device
                )
                
                # Apply results and renormalization
                for i, param_idx in enumerate(valid_param_indices):
                    param_name = param_names_batch[param_idx]
                    merged_param = merged_params_batch[i]
                    
                    # Apply renormalization if enabled
                    if hasattr(merged_param, 'shape') and normalization_mode != "none":
                        try:
                            base_param_for_renorm = get_base_param_func(param_name, computation_device)
                            merged_delta = merged_param - base_param_for_renorm
                            
                            # Skip renormalization if delta is effectively zero
                            if torch.any(torch.abs(merged_delta) > 1e-10):
                                if normalization_mode == "calibrate":
                                    merged_param = calibrate_renormalize(merged_param, base_param_for_renorm, normalization_mode, 0.3, 1.1)
                                else:  # magnitude
                                    merged_param = calibrate_renormalize(merged_param, base_param_for_renorm, normalization_mode, 1.0, 1.0)
                        except Exception as e:
                            widen_logger.warning(f"Renormalization failed for {param_name}: {e}")
                    
                    # Store result
                    block_merged_params[param_name] = merged_param.to(target_device)
                    
            except Exception as e:
                widen_logger.error(f"Vectorized batch merge failed: {e}")
                # Fallback to sequential processing for this batch
                for i, param_idx in enumerate(valid_param_indices):
                    param_name = param_names_batch[param_idx]
                    try:
                        base_param = param_tensors_batch[i]
                        delta_param = task_vector_deltas_batch[i]
                        weight_scores = weight_scores_batch[i]
                        
                        weighted_deltas = _safe_tensor_multiply(delta_param, weight_scores, param_name)
                        if weighted_deltas is not None:
                            merged_delta_param = weighted_deltas.sum(dim=0)
                        else:
                            merged_delta_param = delta_param.mean(dim=0) * weight_scores.mean().item()
                        
                        merged_param = base_param + merged_delta_param
                        block_merged_params[param_name] = merged_param.to(target_device)
                        
                    except Exception as inner_e:
                        widen_logger.error(f"Fallback processing failed for {param_name}: {inner_e}")
                        block_merged_params[param_name] = get_base_param_func(param_name, target_device)
    
    # Clean up block data
    for param_name in list(block_delta_tensors.keys()):
        del block_delta_tensors[param_name]
    for param_name in list(block_rankings.keys()):
        del block_rankings[param_name]['magnitude']
        del block_rankings[param_name]['direction']
    del block_delta_tensors, block_rankings
    
    return block_merged_params

def _merge_param_magnitude_direction_with_dynamic_strength(
    models_to_merge_param_magnitude_direction_diff_tuples,
    get_base_param_func,  # Function to get parameters instead of full dict
    models_to_merge_task_vectors,
    exclude_param_names_regex,
    importance_threshold,
    importance_boost,
    merge_strength,
    min_strength,
    max_strength,
    rank_sensitivity,
    skip_threshold,
    normalization_mode,
    computation_device,
    target_device,
    storage_device,
    ranking_device,
    param_names_in_model
):
    """Enhanced parameter merging with dynamic strength based on compatibility"""
    
    # Map new parameter names to original WIDEN algorithm variable names
    above_average_value_ratio = importance_threshold
    score_calibration_value = importance_boost
    
    # Check available RAM for memory management
    ram_available_gb = 4.0  # Conservative fallback
    try:
        import psutil
        ram_info = psutil.virtual_memory()
        ram_available_gb = ram_info.available / (1024 * 1024 * 1024)
    except:
        pass
    
    # Get parameters to merge
    param_names_to_merge = get_param_names_to_merge(
        input_param_names=param_names_in_model,  # Use passed parameter names
        exclude_param_names_regex=exclude_param_names_regex
    )
    
    # Initialize WIDEN diagnostics collection
    widen_diagnostics = {
        'compatibility_scores': [],
        'importance_score_variances': [],
        'magnitude_score_ranges': [],
        'direction_score_ranges': [],
        'applied_strengths': [],  # Track actual strength values applied
        'strength_distribution': {'min_used': 0.0, 'max_used': 0.0, 'mean': 0.0, 'count': 0},
        'parameters_with_rankings': 0,
        'parameters_skipped_threshold': 0,
        'parameters_skipped_no_rankings': 0,
        'uniform_score_count': 0,
        'varied_score_count': 0
    }
    
    # Unpack magnitude and direction differences
    
    if not models_to_merge_param_magnitude_direction_diff_tuples:
        print(f"[CRITICAL] No magnitude/direction differences computed!")
        # Fallback: create empty rankings and skip to Phase 2
        magnitude_rankings = {}
        direction_rankings = {}
        param_names_merged_by_magnitude_direction = []
    else:
        # Extract magnitude and direction differences from the new format
        models_to_merge_param_magnitude_diff_tuple = [model_diffs[0] for model_diffs in models_to_merge_param_magnitude_direction_diff_tuples]
        models_to_merge_param_direction_diff_tuple = [model_diffs[1] for model_diffs in models_to_merge_param_magnitude_direction_diff_tuples]
        
        param_names_merged_by_magnitude_direction = list(models_to_merge_param_magnitude_diff_tuple[0].keys())
        if not param_names_merged_by_magnitude_direction:
            print(f"[CRITICAL] No parameters eligible for ranking - all filtered out!")
    
    # PHASE 1: Compute ALL rankings on CPU to preserve WIDEN cross-parameter validation
    if param_names_merged_by_magnitude_direction:
        print(f"Phase 1: Computing rankings for {len(param_names_merged_by_magnitude_direction)} parameters...")
        if 'magnitude_rankings' not in locals():
            magnitude_rankings = {}
        if 'direction_rankings' not in locals():  
            direction_rankings = {}
            
        # Organize parameters by SDXL blocks to preserve WIDEN cross-parameter context
        block_groups = _group_parameters_by_blocks(param_names_merged_by_magnitude_direction)
        total_blocks = len(block_groups)
        total_params = len(param_names_merged_by_magnitude_direction)
        
        # Block-wise processing setup complete
        
        import time
        start_time = time.time()
        
        # Process parameters block by block to preserve WIDEN cross-parameter validation
        # Starting block processing
        monitor_memory_usage("PRE-BLOCK-PROCESSING")
        
        profiler = get_widen_memory_profiler()
        profiler.start()
        profiler.checkpoint(f"Block processing started - {total_blocks} blocks, {total_params} params")
        
        for block_idx, (block_name, block_params) in enumerate(block_groups):
            block_start_time = time.time()
            
            # Update progress bar with clean output (suppress profiler logs during update)
            with ProgressBarContext():
                profiler.checkpoint(f"Block {block_idx+1}/{total_blocks} start: {block_name}")
                print_progress_bar(block_idx + 1, total_blocks, prefix=f'Block Merge', suffix=f'{block_name} ({len(block_params)} params)')
            
            # MEMORY OPTIMIZATION: Process all parameters in block together (preserves WIDEN cross-parameter evaluation)
            # But use memory-efficient tensor operations and aggressive cleanup
            block_magnitude_diffs = {}
            block_direction_diffs = {}
            valid_params_in_block = []
            
            # Processing parameters together for cross-parameter evaluation
            
            # Collect all magnitude/direction diffs for this block with memory optimization
            for param_name in block_params:
                try:
                    magnitude_diffs = []
                    direction_diffs = []
                    
                    for model_idx in range(len(models_to_merge_param_magnitude_diff_tuple)):
                        if param_name in models_to_merge_param_magnitude_diff_tuple[model_idx]:
                            mag_diff = models_to_merge_param_magnitude_diff_tuple[model_idx][param_name]
                            dir_diff = models_to_merge_param_direction_diff_tuple[model_idx][param_name]
                            
                            # Handle both scalar and tensor values
                            if isinstance(mag_diff, (int, float)):
                                # Scalar values - no device management or size checking needed
                                magnitude_diffs.append(mag_diff)
                                direction_diffs.append(dir_diff)
                            else:
                                # Tensor values - apply original logic
                                mag_diff = smart_device_management(mag_diff, torch.device('cpu'))
                                dir_diff = smart_device_management(dir_diff, torch.device('cpu'))
                                
                                # Skip extremely large tensors to prevent memory issues
                                if mag_diff.numel() > 50000000:  # 50M elements
                                    print(f"[ENHANCED WIDEN] Skipping large tensor {param_name} ({mag_diff.numel()} elements)")
                                    break
                                    
                                magnitude_diffs.append(mag_diff.to(ranking_device))
                                direction_diffs.append(dir_diff.to(ranking_device))
                    
                    if magnitude_diffs and direction_diffs:
                        block_magnitude_diffs[param_name] = magnitude_diffs
                        block_direction_diffs[param_name] = direction_diffs
                        valid_params_in_block.append(param_name)
                        
                except Exception as e:
                    print(f"[WARNING] Failed to collect data for {param_name}: {e}")
                
                # Immediate cleanup of intermediate tensors to prevent accumulation
                del magnitude_diffs, direction_diffs
                
                # Gentle cleanup every 20 parameters to manage memory without breaking WIDEN evaluation
                if len(valid_params_in_block) % 20 == 0:
                    gentle_cleanup()
            
            # Now compute rankings for all valid parameters in this block
            # Computing rankings for block parameters
            
            # MEMORY OPTIMIZATION: Process rankings one at a time but maintain cross-parameter visibility
            for param_idx, param_name in enumerate(valid_params_in_block):
                # Initialize variables to avoid scoping issues
                mag_tensor = None
                dir_tensor = None
                
                try:
                    # Use memory-efficient tensor operations for stacking - FORCE CPU TO AVOID VRAM OOM
                    with torch.no_grad():
                        # Block-wise logging - log first parameter of each block
                        if param_idx == 0:
                            # Starting rankings computation
                            pass
                        
                        # Handle scalar values from dimension-agnostic computation
                        mag_values = block_magnitude_diffs[param_name]
                        dir_values = block_direction_diffs[param_name]
                        
                        # Use sophisticated tensor alignment instead of naive stacking
                        if isinstance(mag_values[0], (int, float)):
                            # We have scalar values, create 1D tensor
                            mag_tensor = torch.tensor(mag_values, dtype=torch.float32, device='cpu').unsqueeze(1)
                            dir_tensor = torch.tensor(dir_values, dtype=torch.float32, device='cpu').unsqueeze(1)
                        else:
                            # We have tensor values, use structure-preserving alignment
                            cpu_mag_tensors = [t.cpu() if t.device.type != 'cpu' else t for t in mag_values]
                            cpu_dir_tensors = [t.cpu() if t.device.type != 'cpu' else t for t in dir_values]
                            
                            # Use safe_stack for robust tensor alignment
                            mag_tensor = safe_stack(cpu_mag_tensors, dim=0)
                            dir_tensor = safe_stack(cpu_dir_tensors, dim=0)
                        
                        # Keep on CPU for memory efficiency
                        mag_tensor = mag_tensor.cpu()
                        dir_tensor = dir_tensor.cpu()
                    
                    # Verify ranking shapes for WIDEN correctness (first parameter only) - BEFORE deletion
                    if block_idx == 0 and param_name == valid_params_in_block[0]:
                        print(f"[WIDEN VERIFICATION] Parameter '{param_name}': magnitude shape {mag_tensor.shape} -> ranking shape will be computed")
                        print(f"[WIDEN VERIFICATION] This ranks {mag_tensor.shape[1]} features across {mag_tensor.shape[0]} models - CORRECT WIDEN behavior")
                    
                    # Compute rankings with FULL parameter visibility (critical for WIDEN)
                    # Handle scalar/1D tensors like logit_scale that don't need ranking
                    if mag_tensor.dim() <= 1 or mag_tensor.shape[1] <= 1:
                        # For scalar or single-element tensors, create simple uniform rankings
                        magnitude_rankings[param_name] = torch.ones_like(mag_tensor, dtype=torch.float32) * 0.5
                        direction_rankings[param_name] = torch.ones_like(dir_tensor, dtype=torch.float32) * 0.5
                    else:
                        magnitude_rankings[param_name] = _rank_per_param_magnitude_or_direction_within_model(mag_tensor)
                        direction_rankings[param_name] = _rank_per_param_magnitude_or_direction_within_model(dir_tensor)
                    
                    # Block-wise processing - no cleanup within block to maintain WIDEN integrity
                    
                except Exception as e:
                    print(f"[WARNING] Failed to compute rankings for {param_name}: {e}")
                
                finally:
                    # IMMEDIATE cleanup of large tensors after each parameter - even on failure
                    if mag_tensor is not None:
                        del mag_tensor
                    if dir_tensor is not None:
                        del dir_tensor
            
            # Clean up block data
            del block_magnitude_diffs, block_direction_diffs
            
            # Block timing and cleanup
            block_time = time.time() - block_start_time
            profiler.checkpoint(f"Block {block_idx+1}/{total_blocks} complete: {block_name} ({block_time:.1f}s)")
            # Block completed
            
            # Clean up after each block's ranking computation - this is when memory is allocated
            gentle_cleanup()  # Clean up intermediate ranking tensors and computations
    
    print(f"âœ… Phase 1 complete: Rankings computed for {len(magnitude_rankings)} parameters")
    
    # AUTO-TUNE SKIP THRESHOLD based on Phase 1 compatibility scores
    if widen_diagnostics['compatibility_scores']:
        compat_dict = {d['parameter']: d['compatibility'] for d in widen_diagnostics['compatibility_scores']}
        new_thresh, analysis = _analyze_compatibility_patterns_and_recommend_threshold(
            compat_dict, skip_threshold
        )
        if abs(new_thresh - skip_threshold) > 1e-7:  # Only adjust if meaningful difference
            performance_logger.info(f"ðŸŽ¯ Auto-adjusting skip_threshold {skip_threshold:.6f} â†’ {new_thresh:.6f}")
            performance_logger.info(f"Analysis: {analysis}")
            skip_threshold = new_thresh
            print(f"[AUTO-TUNE] Skip threshold adjusted: {skip_threshold:.6f}")
        else:
            print(f"[AUTO-TUNE] Skip threshold optimal: {skip_threshold:.6f}")
    
    # CRITICAL DIAGNOSTIC: Check if Phase 1 failed completely
    if len(magnitude_rankings) == 0:
        print(f"[CRITICAL] Phase 1 FAILED: No rankings computed!")
        print(f"[CRITICAL] Input parameters available: {len(param_names_merged_by_magnitude_direction)}")
        print(f"[CRITICAL] Task vectors: {len(models_to_merge_task_vectors)}")
        print(f"[CRITICAL] Magnitude diff tuples: {len(models_to_merge_param_magnitude_diff_tuple) if models_to_merge_param_magnitude_diff_tuple else 'NONE'}")
        if param_names_merged_by_magnitude_direction:
            print(f"[CRITICAL] Sample parameters: {param_names_merged_by_magnitude_direction[:3]}")
    else:
        print(f"[ENHANCED WIDEN] Phase 1 successful: {len(magnitude_rankings)} parameters have rankings")
        print(f"[ENHANCED WIDEN] Sample ranking parameters: {list(magnitude_rankings.keys())[:3]}")
    
    # PHASE 2: Process individual parameters using precomputed rankings
    print("[ENHANCED WIDEN] Phase 2: Processing individual parameters...")
    print("[ENHANCED WIDEN] Note: All parameters (1D, 2D, >2D) will use WIDEN rankings for consistent merging")
    merged_params = {}
    skipped_count = 0
    failed_count = 0
    no_rankings_count = 0
    widen_merged_count = 0  # For parameters using WIDEN rankings
    processed_count = 0  # Track total processed parameters for memory cleanup
    
    # Memory monitoring function (for logging only - never stop processing)
    def check_memory_status():
        try:
            import psutil
            ram_info = psutil.virtual_memory()
            ram_available_gb = ram_info.available / (1024 * 1024 * 1024)
            if ram_available_gb < 1.0:
                print(f"[WARNING] Critical RAM: {ram_available_gb:.1f}GB available - continuing but may be slow")
                # Force aggressive cleanup but don't stop
                import gc
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            return ram_available_gb
        except:
            return 4.0  # Assume sufficient if can't check
    
    # Process parameters block-wise for Phase 2 as well for consistency
    phase2_block_groups = _group_parameters_by_blocks(param_names_to_merge)
    total_phase2_blocks = len(phase2_block_groups)
    processed_block_count = 0
    
    for block_idx, (block_name, block_params) in enumerate(phase2_block_groups):
        processed_block_count += 1
        ram_gb = check_memory_status()
        
        # Update progress bar with clean output (suppress profiler logs during update)
        with ProgressBarContext():
            profiler.checkpoint(f"Phase2 Block {processed_block_count}/{total_phase2_blocks} start: {block_name}")
            print_progress_bar(processed_block_count, total_phase2_blocks, prefix=f'Phase 2 Merge', suffix=f'{block_name} ({len(block_params)} params, {ram_gb:.1f}GB RAM)')
        # Processing parameters together for cross-parameter evaluation
        
        # Process all parameters in this block together (WIDEN cross-parameter evaluation)
        block_merged_params = _process_block_parameters_together(
            block_params,
            param_names_merged_by_magnitude_direction,
            magnitude_rankings,
            direction_rankings,
            models_to_merge_task_vectors,
            get_base_param_func,
            above_average_value_ratio,
            score_calibration_value,
            merge_strength,
            min_strength,
            max_strength,
            rank_sensitivity,
            skip_threshold,
            normalization_mode,
            computation_device,
            target_device,
            widen_diagnostics
        )
        
        # Update merged_params with block results
        merged_params.update(block_merged_params)
        
        # Update counters based on block processing results
        # Parameters were already processed by _process_block_parameters_together function above
        # Just update the counters to reflect what was processed in this block
        block_params_count = len(block_params)
        widen_merged_count += block_params_count
        processed_count += block_params_count
        
        # Block completion checkpoint and cleanup
        profiler.checkpoint(f"Phase2 Block {processed_block_count}/{total_phase2_blocks} complete: {block_name}")
        
        # Clean up after each Phase 2 block - this is when parameter merging allocations happen
        gentle_cleanup()  # Clean up intermediate merge computations
    
    # Finalize memory profiling
    profiler.checkpoint("WIDEN merge processing complete")
    memory_summary = profiler.finish()
    
    # Calculate actual merge statistics
    total_merged_count = len(merged_params)
    
    # Generate enhanced WIDEN diagnostics
    compatibility_scores = widen_diagnostics['compatibility_scores']
    score_variances = widen_diagnostics['importance_score_variances']
    
    print(f"[ENHANCED WIDEN] MERGE RESULTS:")
    print(f"  Total parameters: {len(param_names_to_merge)}")
    print(f"  WIDEN merged (all dimensions): {widen_merged_count}")
    print(f"  Total successfully merged: {total_merged_count}")
    print(f"  Skipped (no rankings): {no_rankings_count}")
    print(f"  Skipped (low compatibility): {skipped_count}")
    print(f"  Failed: {failed_count}")
    print(f"  Overall merge success rate: {total_merged_count/len(param_names_to_merge)*100:.1f}%")
    print(f"  WIDEN success rate: {widen_merged_count/len(param_names_to_merge)*100:.1f}%")
    
    # ENHANCED: WIDEN Algorithm Health Diagnostics
    print(f"\n[WIDEN ALGORITHM HEALTH]:")
    if compatibility_scores:
        compat_values = [score['compatibility'] for score in compatibility_scores]
        compat_min, compat_max = min(compat_values), max(compat_values)
        compat_mean = sum(compat_values) / len(compat_values)
        compat_variance = sum((x - compat_mean)**2 for x in compat_values) / len(compat_values)
        compat_range = compat_max - compat_min
        # Use relative variance threshold based on score range
        relative_variance_threshold = max(1e-6, (compat_range * 0.01) ** 2)
        print(f"  Compatibility Scores: {compat_min:.4f} - {compat_max:.4f} (mean: {compat_mean:.4f}, var: {compat_variance:.6f})")
        print(f"  Score Distribution: {'âœ“ VARIED' if compat_variance > relative_variance_threshold else 'âœ— UNIFORM (BUG!)'}")
    else:
        print(f"  Compatibility Scores: NONE COMPUTED")
    
    if score_variances:
        # Extract numeric values from variance dictionaries
        mag_variances = [v['magnitude_variance'] if isinstance(v, dict) else v for v in score_variances]
        dir_variances = [v['direction_variance'] if isinstance(v, dict) else v for v in score_variances]
        avg_mag_variance = sum(mag_variances) / len(mag_variances) if mag_variances else 0
        avg_dir_variance = sum(dir_variances) / len(dir_variances) if dir_variances else 0
        overall_avg = (avg_mag_variance + avg_dir_variance) / 2
        print(f"  Importance Score Variance: mag={avg_mag_variance:.6f}, dir={avg_dir_variance:.6f}, avg={overall_avg:.6f} ({'âœ“ VARIED' if overall_avg > 1e-6 else 'âœ— UNIFORM (BUG!)'})")
    
    varied_count = widen_diagnostics['varied_score_count']
    uniform_count = widen_diagnostics['uniform_score_count']
    total_scored = varied_count + uniform_count
    if total_scored > 0:
        print(f"  Parameter Ranking: {varied_count}/{total_scored} varied ({100*varied_count/total_scored:.1f}%)")
        print(f"  Ranking Algorithm: {'âœ“ HEALTHY' if varied_count > uniform_count else 'âœ— FAILING (UNIFORM SCORES!)'}")
    
    if skip_threshold > 0.0:
        skip_effectiveness = widen_diagnostics['parameters_skipped_threshold']
        print(f"  Skip Threshold: {skip_threshold} (percentile) -> {skip_effectiveness} parameters skipped")
        print(f"  Threshold Status: {'âœ“ ACTIVE' if skip_effectiveness > 0 else 'â“˜ NO EFFECT'}")
    
    # Critical diagnostic: If no parameters merged, investigate Phase 1
    if total_merged_count == 0:
        print(f"[CRITICAL] Zero parameters merged! Phase 1 rankings: {len(magnitude_rankings)} magnitude, {len(direction_rankings)} direction")
        print(f"[CRITICAL] Parameter list sample: {param_names_to_merge[:5] if param_names_to_merge else 'EMPTY'}")
        print(f"[CRITICAL] Magnitude rankings sample: {list(magnitude_rankings.keys())[:5] if magnitude_rankings else 'EMPTY'}")
    
    # Verify we processed all parameters (critical for WIDEN integrity)
    processed_count = len(merged_params)
    if processed_count != len(param_names_to_merge):
        print(f"[WARNING] Parameter count mismatch: processed {processed_count}, expected {len(param_names_to_merge)}")
    else:
        print(f"[ENHANCED WIDEN] âœ“ All parameters processed - WIDEN merge integrity maintained")
    
    # COMPREHENSIVE MEMORY CLEANUP - Essential for large models
    # Clean up all intermediate data structures
    cleanup_items = [
        'models_to_merge_param_magnitude_direction_diff_tuples',
        'models_to_merge_task_vectors', 
        'magnitude_rankings',
        'direction_rankings',
        'models_to_merge_param_magnitude_diff_tuple',
        'models_to_merge_param_direction_diff_tuple',
        'param_names_merged_by_magnitude_direction'
    ]
    
    for item_name in cleanup_items:
        try:
            if item_name in locals():
                del locals()[item_name]
        except: pass
    
    # Clear TaskVector parameters explicitly
    try:
        if 'models_to_merge_task_vectors' in locals():
            for tv in models_to_merge_task_vectors:
                if hasattr(tv, 'task_vector_param_dict'):
                    tv.task_vector_param_dict.clear()
            del models_to_merge_task_vectors
    except: pass
    
    # Force comprehensive garbage collection
    import gc
    gc.collect()  # Single collection is sufficient
    
    # CUDA memory cleanup
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()  # Wait for GPU operations to complete
        torch.cuda.empty_cache()  # Second clear after sync
    
    # Final memory check
    try:
        import psutil
        final_ram = psutil.virtual_memory().used / (1024**3)
        print(f"[ENHANCED WIDEN] Cleanup complete - Final RAM: {final_ram:.2f}GB")
    except: pass
    
    # Add strength debug info to diagnostics
    strength_values = [s['strength'] for s in widen_diagnostics['applied_strengths']]
    if strength_values:
        actual_min = min(strength_values)
        actual_max = max(strength_values)
        widen_diagnostics['strength_debug'] = {
            'actual_strength_range': f"{actual_min:.3f}-{actual_max:.3f}",
            'theoretical_range': f"{merge_strength * min_strength:.3f}-{merge_strength * max_strength:.3f}",
            'parameter_range': f"{min_strength}-{max_strength}",
            'avg_strength': sum(strength_values) / len(strength_values),
            'merge_strength_used': merge_strength,
            'sample_calculations': [(s['parameter'], s['compatibility'], s['strength']) for s in widen_diagnostics['applied_strengths'][:5]]
        }
    
    return merged_params, widen_diagnostics

# LoRA Delta-Only Processing Classes
class LoRADelta:
    """Memory-efficient LoRA delta storage for WIDEN merging"""
    def __init__(self, base_model, lora_model, lora_name="unknown"):
        self.base_model = base_model  # Reference to base model (no copy)
        self.lora_name = lora_name
        self.deltas = {}  # Only store the differences
        self.param_metadata = {}
        
        print(f"[LoRADelta] Computing deltas for {lora_name}...")
        self._compute_deltas(lora_model)
        
    def _compute_deltas(self, lora_model):
        """Compute only the parameter differences between base and LoRA-enhanced model"""
        try:
            base_params = dict(self.base_model.named_parameters())
            lora_params = dict(lora_model.named_parameters())
            
            delta_count = 0
            total_delta_size = 0
            
            for name, lora_param in lora_params.items():
                if name in base_params:
                    base_param = base_params[name]
                    if base_param.shape == lora_param.shape:
                        # Compute delta
                        delta = lora_param.detach().cpu().float() - base_param.detach().cpu().float()
                        
                        # Only store if there's a meaningful difference
                        delta_magnitude = torch.norm(delta).item()
                        if delta_magnitude > 1e-8:
                            self.deltas[name] = delta
                            self.param_metadata[name] = {
                                'delta_magnitude': delta_magnitude,
                                'base_magnitude': torch.norm(base_param).item(),
                                'change_ratio': delta_magnitude / (torch.norm(base_param).item() + 1e-8)
                            }
                            delta_count += 1
                            total_delta_size += delta.numel() * 4  # 4 bytes per float32
            
            # Memory usage summary
            size_mb = total_delta_size / (1024 * 1024)
            print(f"[LoRADelta] {self.lora_name}: {delta_count} changed parameters, {size_mb:.1f}MB delta storage")
            
        except Exception as e:
            print(f"[LoRADelta] Error computing deltas for {self.lora_name}: {e}")
            self.deltas = {}
    
    def get_parameter(self, name):
        """Get parameter value (base + delta if exists)"""
        if name in self.deltas:
            base_param = dict(self.base_model.named_parameters())[name]
            return base_param + self.deltas[name].to(base_param.device)
        else:
            return dict(self.base_model.named_parameters())[name]
    
    def named_parameters(self):
        """Generator that yields (name, parameter) tuples with deltas applied"""
        base_params = dict(self.base_model.named_parameters())
        for name, base_param in base_params.items():
            if name in self.deltas:
                # Apply delta
                enhanced_param = base_param + self.deltas[name].to(base_param.device)
                yield name, enhanced_param
            else:
                # Use base parameter unchanged
                yield name, base_param
    
    def get_delta_info(self):
        """Get information about stored deltas"""
        return {
            'lora_name': self.lora_name,
            'delta_count': len(self.deltas),
            'changed_parameters': list(self.deltas.keys()),
            'metadata': self.param_metadata
        }

class LoRAStackProcessor:
    """Process LoRA stacks efficiently for WIDEN merging"""
    def __init__(self, base_model, base_clip=None):
        self.base_model = base_model
        self.base_clip = base_clip
        self.lora_deltas = []
        
    def add_lora_from_stack(self, lora_stack):
        """Add LoRAs from a LoRA stack, creating deltas for each"""
        if lora_stack is None:
            return
            
        try:
            # Handle LoRA stack format from DonutLoRAStack: list of (name, model_weight, clip_weight, block_vector)
            if hasattr(lora_stack, '__iter__'):
                for idx, lora_item in enumerate(lora_stack):
                    lora_name = f"LoRA_{idx+1}"
                    if isinstance(lora_item, tuple) and len(lora_item) >= 4:
                        name, model_weight, clip_weight, block_vector = lora_item
                        lora_name = f"{name}_{idx+1}"
                    self._process_single_lora(lora_item, lora_name)
            else:
                self._process_single_lora(lora_stack, "LoRA_1")
                
        except Exception as e:
            print(f"[LoRAStackProcessor] Error processing LoRA stack: {e}")
    
    def _process_single_lora_for_unet(self, lora_item, lora_name):
        """Process UNet part of LoRA (Step 1 of DonutApplyLoRAStack)"""
        try:
            # Extract LoRA details from DonutLoRAStack format: (name, model_weight, clip_weight, block_vector)
            if isinstance(lora_item, tuple) and len(lora_item) >= 4:
                name, model_weight, clip_weight, block_vector = lora_item
            else:
                print(f"[LoRAStackProcessor] Invalid LoRA format for {lora_name}: {lora_item}")
                return
                
            print(f"[LoRADelta] Processing UNet LoRA {lora_name}: {name} (model_weight: {model_weight})")
            
            try:
                # Use ComfyUI's LoRA loading system (same as DonutApplyLoRAStack Step 1)
                import comfy.utils
                import folder_paths
                try:
                    from .lora_block_weight import LoraLoaderBlockWeight
                except ImportError:
                    # Fallback for standalone execution
                    class LoraLoaderBlockWeight:
                        def __init__(self): pass
                        def load_lora(self, *args, **kwargs): return None, None
                
                # Get the full path to the LoRA file
                path = folder_paths.get_full_path("loras", name)
                if path is None:
                    raise FileNotFoundError(f"LoRA file not found: {name}")
                
                print(f"[LoRADelta] Loading UNet LoRA from: {path}")
                
                # Load the LoRA file
                lora = comfy.utils.load_torch_file(path, safe_load=True)
                
                # Create a temporary copy of the base model to apply LoRA
                if hasattr(self.base_model, 'clone'):
                    temp_model = self.base_model.clone()
                else:
                    import copy
                    temp_model = copy.copy(self.base_model)
                    if hasattr(self.base_model, 'model'):
                        temp_model.model = copy.deepcopy(self.base_model.model)
                
                # Apply UNet LoRA using block weights (DonutApplyLoRAStack Step 1)
                loader = LoraLoaderBlockWeight()
                vector = block_vector if block_vector else ",".join(["1"] * 12)
                
                # Step 1: block-weighted UNet merge (clip_strength=0)
                enhanced_model, _, _ = loader.load_lora_for_models(
                    temp_model, None, lora,  # UNet only, no CLIP
                    strength_model=model_weight,
                    strength_clip=0.0,  # No CLIP changes in UNet step
                    inverse=False,
                    seed=0,
                    A=1.0,
                    B=1.0,
                    block_vector=vector
                )
                
                # Create delta object comparing base vs LoRA-enhanced UNet
                lora_delta = LoRADelta(self.base_model, enhanced_model, lora_name)
                if len(lora_delta.deltas) > 0:
                    self.lora_deltas.append(lora_delta)
                    print(f"[LoRADelta] Successfully created UNet delta for {lora_name} with {len(lora_delta.deltas)} changed parameters")
                else:
                    print(f"[LoRADelta] No UNet deltas found for {lora_name}, skipping")
                
                # Clean up temporary model
                del temp_model, enhanced_model
                gc.collect()
                
            except Exception as e:
                print(f"[LoRADelta] Error processing UNet LoRA {name}: {e}")
                import traceback
                traceback.print_exc()
            
        except Exception as e:
            print(f"[LoRAStackProcessor] Error processing UNet LoRA {lora_name}: {e}")

    def _process_single_lora_for_clip(self, lora_item, lora_name):
        """Process CLIP part of LoRA (Step 2 of DonutApplyLoRAStack)"""
        try:
            # Extract LoRA details from DonutLoRAStack format: (name, model_weight, clip_weight, block_vector)
            if isinstance(lora_item, tuple) and len(lora_item) >= 4:
                name, model_weight, clip_weight, block_vector = lora_item
            else:
                print(f"[LoRAStackProcessor] Invalid LoRA format for {lora_name}: {lora_item}")
                return
                
            print(f"[LoRADelta] Processing CLIP LoRA {lora_name}: {name} (clip_weight: {clip_weight})")
            
            try:
                # Use ComfyUI's LoRA loading system (same as DonutApplyLoRAStack Step 2)
                import comfy.utils
                import comfy.sd
                import folder_paths
                
                # Get the full path to the LoRA file
                path = folder_paths.get_full_path("loras", name)
                if path is None:
                    raise FileNotFoundError(f"LoRA file not found: {name}")
                
                print(f"[LoRADelta] Loading CLIP LoRA from: {path}")
                
                # Load the LoRA file
                lora = comfy.utils.load_torch_file(path, safe_load=True)
                
                # Create a temporary copy of the base CLIP to apply LoRA
                if hasattr(self.base_model, 'clone'):
                    temp_clip = self.base_model.clone()
                else:
                    import copy
                    temp_clip = copy.copy(self.base_model)
                    # For CLIP, copy the actual encoder
                    if hasattr(self.base_model, 'cond_stage_model'):
                        temp_clip.cond_stage_model = copy.deepcopy(self.base_model.cond_stage_model)
                    elif hasattr(self.base_model, 'clip'):
                        temp_clip.clip = copy.deepcopy(self.base_model.clip)
                
                # Step 2: uniform CLIP merge (no block control)
                _, enhanced_clip = comfy.sd.load_lora_for_models(
                    None, temp_clip, lora,
                    0.0,         # No UNet change in CLIP step
                    clip_weight  # CLIP strength
                )
                
                # Create delta object comparing base vs LoRA-enhanced CLIP
                lora_delta = LoRADelta(self.base_model, enhanced_clip, lora_name)
                if len(lora_delta.deltas) > 0:
                    self.lora_deltas.append(lora_delta)
                    print(f"[LoRADelta] Successfully created CLIP delta for {lora_name} with {len(lora_delta.deltas)} changed parameters")
                else:
                    print(f"[LoRADelta] No CLIP deltas found for {lora_name}, skipping")
                
                # Clean up temporary model
                del temp_clip, enhanced_clip
                gc.collect()
                
            except Exception as e:
                print(f"[LoRADelta] Error processing CLIP LoRA {name}: {e}")
                import traceback
                traceback.print_exc()
            
        except Exception as e:
            print(f"[LoRAStackProcessor] Error processing CLIP LoRA {lora_name}: {e}")

    def _process_single_lora(self, lora_item, lora_name):
        """Process LoRA for the specific model type (UNet or CLIP)"""
        # Determine if we're processing UNet or CLIP and call the appropriate method
        if hasattr(self.base_model, 'model') and hasattr(self.base_model.model, 'diffusion_model'):
            # This is a UNet MODEL - process UNet LoRA
            self._process_single_lora_for_unet(lora_item, lora_name)
        elif hasattr(self.base_model, 'cond_stage_model') or hasattr(self.base_model, 'clip'):
            # This is a CLIP object - process CLIP LoRA  
            self._process_single_lora_for_clip(lora_item, lora_name)
        else:
            print(f"[LoRADelta] Unknown model type for {lora_name}, skipping")
    
    
    def get_virtual_models(self):
        """Return list of virtual models (base + each delta)"""
        models = [self.base_model]  # Include base model
        models.extend(self.lora_deltas)  # Add delta models
        return models
    
    def get_summary(self):
        """Get summary of processed LoRAs"""
        total_deltas = sum(len(delta.deltas) for delta in self.lora_deltas)
        return {
            'base_model': 'included',
            'lora_count': len(self.lora_deltas),
            'total_delta_parameters': total_deltas,
            'lora_names': [delta.lora_name for delta in self.lora_deltas]
        }

# Global cache for preventing redundant processing
_MERGE_CACHE = {}
_CACHE_MAX_SIZE = 3  # Reduced from 10 to prevent memory bloat

def clear_session_cache():
    """Clear global merge cache for fresh session start"""
    global _MERGE_CACHE
    if _MERGE_CACHE:
        print(f"[Session] Clearing {len(_MERGE_CACHE)} cached merge results")
        _MERGE_CACHE.clear()
        # Light cleanup after cache clear
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def monitor_memory(label=""):
    """Print current memory usage including VRAM"""
    try:
        process = psutil.Process()
        ram_mb = process.memory_info().rss / 1024 / 1024

        # Always try to get VRAM info
        vram_info = ""
        if torch.cuda.is_available():
            try:
                vram_mb = torch.cuda.memory_allocated() / 1024 / 1024
                vram_reserved_mb = torch.cuda.memory_reserved() / 1024 / 1024
                vram_info = f", VRAM: {vram_mb:.1f}MB (reserved: {vram_reserved_mb:.1f}MB)"
            except Exception:
                vram_info = ", VRAM: unavailable"

        memory_logger.debug(f"[{label}] RAM: {ram_mb:.1f}MB{vram_info}")

    except Exception as e:
        memory_logger.warning(f"[{label}] Memory monitoring error: {e}")

def check_memory_safety():
    """Check if memory is safe to continue"""
    try:
        process = psutil.Process()
        current_ram_gb = process.memory_info().rss / 1024 / 1024 / 1024
        total_ram_gb = psutil.virtual_memory().total / 1024 / 1024 / 1024
        available_ram_gb = total_ram_gb - current_ram_gb
        ram_usage_percent = current_ram_gb / total_ram_gb

        if ram_usage_percent > 0.95 or available_ram_gb < 1.5:
            return False, ram_usage_percent, available_ram_gb

        return True, ram_usage_percent, available_ram_gb
    except Exception:
        return True, 0.0, 999.0

def compute_merge_hash(models, merge_strength, min_strength, max_strength, importance_threshold, importance_boost, rank_sensitivity, skip_threshold, normalization_mode, lora_stack=None):
    """Compute hash of merge parameters to detect changes - FIXED: More robust hashing"""
    hasher = hashlib.sha256()  # Changed from md5 to sha256 for better collision resistance

    # Hash model inputs - FIXED: Use model state checksum instead of object ID
    for i, model in enumerate(models):
        if model is not None and not getattr(model, "_is_filler", False):
            try:
                # Create a more stable hash based on model parameters
                model_params = list(model.named_parameters()) if hasattr(model, 'named_parameters') else []
                if model_params:
                    # Use first and last parameter shapes and a few sample values for hash
                    first_param = model_params[0][1] if model_params else None
                    last_param = model_params[-1][1] if len(model_params) > 1 else first_param

                    if first_param is not None:
                        hasher.update(str(first_param.shape).encode())
                        hasher.update(str(first_param.flatten()[:10].tolist()).encode())
                    if last_param is not None and last_param is not first_param:
                        hasher.update(str(last_param.shape).encode())
                        hasher.update(str(last_param.flatten()[:10].tolist()).encode())
                else:
                    # Deterministic fallback - use model class and id
                    model_class = type(model).__name__
                    hasher.update(f"{model_class}_{id(model)}".encode())
            except Exception:
                # Deterministic ultimate fallback
                model_class = type(model).__name__ if hasattr(model, '__class__') else 'unknown'
                hasher.update(f"{model_class}_{id(model)}".encode())

    # Hash merge parameters - FIXED: Include ALL parameters that affect merge
    hasher.update(f"{merge_strength}_{min_strength}_{max_strength}_{importance_threshold}_{importance_boost}_{rank_sensitivity}_{skip_threshold}_{normalization_mode}".encode())
    
    # Hash LoRA stack if present
    if lora_stack is not None:
        try:
            if hasattr(lora_stack, '__iter__'):
                for idx, lora_item in enumerate(lora_stack):
                    if isinstance(lora_item, tuple) and len(lora_item) >= 4:
                        # Hash lora name, model_weight, clip_weight, block_vector
                        hasher.update(f"lora_{idx}_{lora_item[0]}_{lora_item[1]}_{lora_item[2]}_{lora_item[3]}".encode())
                    else:
                        hasher.update(f"lora_{idx}_{str(lora_item)}".encode())
            else:
                hasher.update(f"lora_single_{str(lora_stack)}".encode())
        except Exception:
            # Fallback for unparseable lora_stack
            hasher.update(f"lora_fallback_{id(lora_stack)}".encode())

    return hasher.hexdigest()

def check_cache_for_merge(cache_key):
    """Check if we have a cached result for this merge"""
    if cache_key in _MERGE_CACHE:
        print("[Cache] Found cached merge result - skipping processing")
        cached_result = _MERGE_CACHE[cache_key]
        # Clone the cached model to prevent mutation of cached result
        cached_model, results_text, parameter_info = cached_result
        fresh_model = cached_model.clone()
        return (fresh_model, results_text, parameter_info)
    return None

def store_merge_result(cache_key, result):
    """Store merge result in cache with memory monitoring"""
    global _MERGE_CACHE

    # Clear old entries if cache is full
    if len(_MERGE_CACHE) >= _CACHE_MAX_SIZE:
        oldest_key = next(iter(_MERGE_CACHE))
        del _MERGE_CACHE[oldest_key]
        # Light cleanup when removing old cache entries
        gc.collect()
        print(f"[Cache] Removed oldest entry, cache size: {len(_MERGE_CACHE)}")

    _MERGE_CACHE[cache_key] = result
    print(f"[Cache] Stored merge result, cache size: {len(_MERGE_CACHE)}")

def force_cleanup():
    """Conservative memory cleanup to prevent CUDA allocator conflicts"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # Removed torch.cuda.synchronize() to prevent allocator conflicts

def gentle_cleanup():
    """Very light cleanup for frequent use during processing"""
    gc.collect()
    # No CUDA operations to avoid allocator stress

def aggressive_memory_cleanup():
    """Aggressive memory cleanup for critical memory optimization"""
    # Clear optimized cache
    cache = get_optimized_cache()
    cache.clear()
    
    # Force garbage collection
    gc.collect()
    
    # Clear CUDA cache if available
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()  # Ensure all operations complete

def monitor_memory_usage(label=""):
    """Monitor memory usage for debugging"""
    try:
        import psutil
        process = psutil.Process()
        ram_mb = process.memory_info().rss / 1024 / 1024
        ram_gb = ram_mb / 1024
        
        # Also get system memory info
        system_memory = psutil.virtual_memory()
        system_ram_gb = system_memory.used / (1024**3)
        
        if torch.cuda.is_available():
            gpu_allocated = torch.cuda.memory_allocated() / 1024 / 1024
            gpu_reserved = torch.cuda.memory_reserved() / 1024 / 1024
            gpu_allocated_gb = gpu_allocated / 1024
            print(f"[MEMORY-{label}] Process RAM: {ram_gb:.2f}GB, System RAM: {system_ram_gb:.2f}GB, VRAM: {gpu_allocated_gb:.2f}GB allocated, {gpu_reserved/1024:.2f}GB reserved")
        else:
            print(f"[MEMORY-{label}] Process RAM: {ram_gb:.2f}GB, System RAM: {system_ram_gb:.2f}GB")
        
        # Track memory spikes
        if hasattr(monitor_memory_usage, 'last_system_ram'):
            delta = system_ram_gb - monitor_memory_usage.last_system_ram
            if abs(delta) > 2.0:  # Alert on 2GB+ changes
                print(f"[MEMORY SPIKE-{label}] System RAM changed by {delta:+.2f}GB")
        monitor_memory_usage.last_system_ram = system_ram_gb
        
    except Exception as e:
        print(f"[MEMORY-{label}] Monitor failed: {e}")

# Debug tensor memory function removed - using monitor_memory_usage instead

# Global memory profiler for WIDEN merge operations
_WIDEN_MEMORY_PROFILER = None

def get_widen_memory_profiler():
    """Get or create the global WIDEN memory profiler"""
    global _WIDEN_MEMORY_PROFILER
    if _WIDEN_MEMORY_PROFILER is None:
        _WIDEN_MEMORY_PROFILER = MemoryProfiler("WIDEN_MERGE")
    return _WIDEN_MEMORY_PROFILER

def ultra_memory_efficient_widen_merge(
    merged_model, models_to_merge, exclude_param_names_regex,
    importance_threshold, importance_boost, base_merge_strength,
    rank_sensitivity, skip_threshold, normalization_mode,
    computation_device, target_device, storage_device
):
    """Ultra memory-efficient WIDEN merge - processes parameters one at a time"""
    
    print("[ULTRA MEMORY] Starting ultra memory-efficient WIDEN merge")
    initial_memory = psutil.virtual_memory().used / (1024**3)
    print(f"[ULTRA MEMORY] Initial RAM: {initial_memory:.2f}GB")
    
    # Get parameter names to process
    param_names_to_merge = []
    for name, _ in merged_model.named_parameters():
        if not exclude_param_names_regex or not exclude_param_names_regex.search(name):
            param_names_to_merge.append(name)
    
    print(f"[ULTRA MEMORY] Processing {len(param_names_to_merge)} parameters one at a time")
    
    merged_params = {}
    processed_count = 0
    peak_memory = initial_memory
    
    # Process each parameter individually to minimize memory usage
    for param_idx, param_name in enumerate(param_names_to_merge):
        try:
            # Get base parameter
            base_param = None
            for name, param in merged_model.named_parameters():
                if name == param_name:
                    base_param = param.detach().to(storage_device).float()
                    if param_name == "model.embed_tokens.weight":
                        base_param = base_param.transpose(dim0=0, dim1=1)
                    break
            
            if base_param is None:
                continue
            
            # Create deltas one at a time
            delta_tensors = []
            for model_to_merge in models_to_merge:
                other_param = None
                for name, param in model_to_merge.named_parameters():
                    if name == param_name:
                        other_param = param.detach().to(storage_device).float()
                        if param_name == "model.embed_tokens.weight":
                            other_param = other_param.transpose(dim0=0, dim1=1)
                        break
                
                if other_param is not None:
                    delta = other_param - base_param
                    delta_tensors.append(delta.to(computation_device))
                    del other_param
            
            if not delta_tensors:
                merged_params[param_name] = base_param.to(target_device)
                processed_count += 1
                continue
            
            # Simple weighted merge (simplified WIDEN)
            with torch.no_grad():
                deltas_tensor = torch.stack(delta_tensors, dim=0)
                
                # Simplified importance scoring
                if deltas_tensor.dim() > 1:
                    magnitudes = torch.norm(deltas_tensor, p=2, dim=tuple(range(1, deltas_tensor.dim())))
                    importance_scores = magnitudes / (magnitudes.max() + 1e-8)
                    
                    # Apply importance boost to top features
                    top_k = max(1, int(len(importance_scores) * importance_threshold / 100.0))
                    top_indices = torch.argsort(importance_scores, descending=True)[:top_k]
                    
                    weights = torch.ones_like(importance_scores) * 0.1
                    weights[top_indices] = importance_boost
                    
                    # Weighted merge
                    weighted_deltas = deltas_tensor * weights.view(-1, *([1] * (deltas_tensor.dim() - 1)))
                    merged_delta = weighted_deltas.sum(dim=0)
                else:
                    merged_delta = deltas_tensor.mean(dim=0)
                
                merged_param = base_param.to(computation_device) + merged_delta * base_merge_strength
                merged_params[param_name] = merged_param.to(target_device)
                
                del deltas_tensor, delta_tensors, merged_delta, merged_param
            
            processed_count += 1
            
            # Monitor memory and cleanup periodically
            if param_idx % 20 == 0:
                current_memory = psutil.virtual_memory().used / (1024**3)
                peak_memory = max(peak_memory, current_memory)
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                print(f"[ULTRA MEMORY] Progress: {param_idx}/{len(param_names_to_merge)}, RAM: {current_memory:.2f}GB")
            
        except Exception as e:
            print(f"[ERROR] Failed to process {param_name}: {e}")
            processed_count += 1
    
    final_memory = psutil.virtual_memory().used / (1024**3)
    total_memory_used = peak_memory - initial_memory
    
    print(f"[ULTRA MEMORY] Complete! Processed: {processed_count}/{len(param_names_to_merge)}")
    print(f"[ULTRA MEMORY] Peak RAM: {peak_memory:.2f}GB (Î”{total_memory_used:+.2f}GB)")
    
    # Transpose back
    if "model.embed_tokens.weight" in merged_params:
        merged_params["model.embed_tokens.weight"] = merged_params["model.embed_tokens.weight"].transpose(dim0=0, dim1=1)
    
    return merged_params

class MemoryExhaustionError(Exception):
    pass

@contextmanager
def memory_cleanup_context(label=""):
    """Context manager for automatic memory cleanup"""
    monitor_memory(f"{label}-START")
    try:
        yield
    finally:
        force_cleanup()
        monitor_memory(f"{label}-END")

def calibrate_renormalize(merged_param, base_param, mode="calibrate", t=1.0, s=1.5):
    """Renormalize using calibration algorithm or simple methods - ENHANCED with conservative options"""
    if mode == "none":
        return merged_param

    elif mode == "magnitude":
        # Simple magnitude preservation (original method)
        base_norm = torch.norm(base_param)
        merged_norm = torch.norm(merged_param)
        if merged_norm > 1e-8:  # Avoid division by zero
            return merged_param * (base_norm / merged_norm)
        return merged_param

    elif mode == "calibrate":
        # More conservative calibration-style renormalization with adjustable parameters
        import torch.nn.functional as F

        # Get the difference from base parameter (the "delta")
        param_delta = merged_param - base_param

        # Only calibrate if there's a significant change
        delta_magnitude = torch.norm(param_delta).item()
        if delta_magnitude < 1e-6:
            return merged_param

        # Work with absolute values for calibration
        param_abs = torch.abs(param_delta)
        param_sign = torch.sign(param_delta)

        # Apply softmax normalization to absolute delta values
        if param_abs.numel() > 1:
            # Flatten for softmax, then reshape back
            original_shape = param_abs.shape
            param_flat = param_abs.flatten()

            if param_flat.sum() > 1e-8:  # Avoid division by zero
                # ENHANCED: Adjustable softmax temperature for more/less smoothing
                # Lower t = more conservative (sharper), higher t = more smoothing
                temperature = max(0.1, min(2.0, t))  # Clamp between 0.1-2.0
                sm_m = F.softmax(param_flat * temperature, dim=0)

                # ENHANCED: Adjustable calibration thresholding
                # Lower t = higher threshold (more selective), higher t = lower threshold
                K = param_flat.numel()
                threshold_factor = max(0.1, min(1.0, t))  # Clamp between 0.1-1.0
                thr_m = (threshold_factor / K) * sm_m.sum() * 0.5

                # ENHANCED: Adjustable scaling intensity
                # Lower s = less aggressive scaling, higher s = more aggressive
                scaling_factor = max(1.0, min(3.0, s))  # Clamp between 1.0-3.0
                conservative_intensity = max(0.1, min(1.0, (scaling_factor - 1.0) * 0.5))  # More conservative
                cal_m = torch.where(sm_m > thr_m, 1.0 + conservative_intensity * sm_m, sm_m)

                # Renormalize to preserve relative magnitudes
                if cal_m.sum() > 1e-8:
                    cal_m = cal_m * (param_flat.sum() / cal_m.sum())

                # Reshape back and restore signs
                cal_m = cal_m.reshape(original_shape)
                calibrated_delta = cal_m * param_sign

                # Apply calibrated delta back to base parameter
                calibrated_param = base_param + calibrated_delta

                return calibrated_param
            else:
                return merged_param
        else:
            return merged_param

    else:
        raise ValueError(f"Unknown renormalization mode: {mode}. Use 'none', 'magnitude', or 'calibrate'")

class TaskVector:
    """Extract task vector (delta) between two models with SDXL awareness"""
    def __init__(self, base_model, finetuned_model, exclude_param_names_regex=None):
        self.task_vector_param_dict = {}
        self.param_metadata = {}  # Store SDXL-specific metadata

        # FIXED: Proper memory management with explicit cleanup
        try:
            base_params = {n: p.detach().cpu().float()  # FIXED: Removed redundant .clone()
                          for n, p in base_model.named_parameters()}
            finetuned_params = {n: p.detach().cpu().float()  # FIXED: Removed redundant .clone()
                               for n, p in finetuned_model.named_parameters()}

            # Extract deltas with SDXL layer classification
            for name in base_params:
                if name in finetuned_params:
                    if exclude_param_names_regex:
                        # Compile regex patterns once for performance
                        if not hasattr(exclude_param_names_regex, '_compiled'):
                            exclude_param_names_regex._compiled = [
                                re.compile(pattern) if isinstance(pattern, str) else pattern 
                                for pattern in exclude_param_names_regex
                            ]
                        skip = any(pattern.search(name) for pattern in exclude_param_names_regex._compiled)
                        if skip:
                            continue

                    delta = finetuned_params[name] - base_params[name]
                    self.task_vector_param_dict[name] = delta

                    # Classify SDXL layer type and store metadata
                    base_magnitude = torch.norm(base_params[name]).item()
                    delta_magnitude = torch.norm(delta).item()
                    self.param_metadata[name] = {
                        'layer_type': self._classify_sdxl_layer(name),
                        'base_magnitude': base_magnitude,
                        'delta_magnitude': delta_magnitude,
                        'change_ratio': delta_magnitude / (base_magnitude + 1e-8)  # FIXED: Avoid division by zero
                    }

        finally:
            # FIXED: Explicit cleanup to prevent memory leaks
            if 'base_params' in locals():
                del base_params
            if 'finetuned_params' in locals():
                del finetuned_params
            gc.collect()

    def _classify_sdxl_layer(self, param_name):
        """Classify SDXL layer types for specialized handling - ENHANCED"""
        name_lower = param_name.lower()

        # UNet structure classification - more comprehensive
        if 'time_embed' in name_lower:
            return 'time_embedding'
        elif 'label_emb' in name_lower:
            return 'class_embedding'
        elif any(x in name_lower for x in ['attn', 'attention']):
            if 'cross' in name_lower:
                return 'cross_attention'  # Text conditioning
            else:
                return 'self_attention'   # Spatial attention
        elif any(x in name_lower for x in ['conv', 'convolution']):
            if 'in_layers' in name_lower or 'input' in name_lower:
                return 'input_conv'
            elif 'out_layers' in name_lower or 'output' in name_lower:
                return 'output_conv'
            elif 'skip' in name_lower or 'residual' in name_lower:
                return 'skip_conv'
            else:
                return 'feature_conv'
        elif any(x in name_lower for x in ['norm', 'group_norm', 'layer_norm']):
            return 'normalization'
        elif 'bias' in name_lower:
            return 'bias'
        elif any(x in name_lower for x in ['down', 'upsample']):
            return 'resolution_change'
        # Enhanced classification for previously 'other' parameters
        elif any(x in name_lower for x in ['proj', 'projection']):
            return 'self_attention'  # Projections are usually part of attention
        elif any(x in name_lower for x in ['to_q', 'to_k', 'to_v', 'to_out']):
            return 'self_attention'  # Attention components
        elif any(x in name_lower for x in ['ff', 'feedforward', 'mlp']):
            return 'self_attention'  # Feed-forward networks in transformers
        elif 'weight' in name_lower and 'emb' in name_lower:
            return 'time_embedding'  # Embedding weights
        else:
            return 'other'

class MergingMethod:
    def __init__(self, merging_method_name: str):
        self.method = merging_method_name

        # SDXL-specific thresholds based on WIDEN paper principles - FIXED: Much higher base thresholds
        self.sdxl_thresholds = {
            'time_embedding': 0.1,        # MUCH higher threshold for critical layers
            'class_embedding': 0.1,       # MUCH higher threshold for critical layers
            'cross_attention': 0.05,      # Higher threshold for attention
            'self_attention': 0.05,       # Higher threshold for attention
            'input_conv': 0.05,           # Higher threshold for convolutions
            'output_conv': 0.1,           # High threshold for output layers
            'feature_conv': 0.05,         # Higher threshold for convolutions
            'skip_conv': 0.03,            # Moderate threshold for skip connections
            'resolution_change': 0.05,    # Higher threshold
            'normalization': 0.02,        # Moderate threshold for normalization
            'bias': 0.01,                 # Higher threshold for bias
            'other': 0.05                 # Higher threshold for unclassified layers
        }

        # Layer importance weights for SDXL
        self.sdxl_importance_weights = {
            'time_embedding': 1.5,        # Very important for temporal consistency
            'class_embedding': 1.3,       # Important for conditioning
            'cross_attention': 1.4,       # Critical for text alignment
            'self_attention': 1.2,        # Important for spatial coherence
            'input_conv': 1.1,           # Feature extraction
            'output_conv': 1.3,          # Final output quality
            'feature_conv': 1.0,         # Standard processing
            'skip_conv': 1.1,            # Residual connections
            'resolution_change': 1.2,    # Scale handling
            'normalization': 0.8,        # Less critical for merging
            'bias': 0.6,                 # Least critical
            'other': 1.0                 # Default
        }

    def should_merge_parameter(self, param_name, delta_magnitude, metadata, widen_threshold=0.5):
        """Determine if parameter should be merged based on SDXL-specific criteria - FIXED: More aggressive threshold logic"""
        layer_type = metadata.get('layer_type', 'other')
        base_threshold = self.sdxl_thresholds.get(layer_type, 0.0001)

        # FIXED: Much more aggressive exponential scaling
        # 0.0 -> 0.001x (extremely permissive)
        # 0.5 -> 1.0x (standard)
        # 1.0 -> 1000x (extremely selective)
        if widen_threshold <= 0.5:
            # Permissive range: 0.0-0.5 maps to 0.001x-1.0x
            threshold_multiplier = 0.001 + (widen_threshold * 2) ** 3 * 0.999
        else:
            # Selective range: 0.5-1.0 maps to 1.0x-1000x
            threshold_multiplier = 1.0 + ((widen_threshold - 0.5) * 2) ** 4 * 999

        threshold = base_threshold * threshold_multiplier

        # Additional criteria for SDXL
        change_ratio = metadata.get('change_ratio', 0)
        scaled_change_threshold = 0.0001 * threshold_multiplier

        # FIXED: More strict checking - both conditions must pass
        magnitude_check = delta_magnitude >= threshold
        ratio_check = change_ratio >= scaled_change_threshold

        # Threshold analysis completed

        if not magnitude_check or not ratio_check:
            return False

        # Special handling for critical layers - even more selective at high thresholds
        if layer_type in ['time_embedding', 'class_embedding']:
            critical_threshold = threshold * (0.5 + widen_threshold * 0.5)  # 0.5-1.0x multiplier
            return delta_magnitude > critical_threshold

        # Special handling for 'other' category - less lenient at high thresholds
        if layer_type == 'other':
            other_threshold = threshold * (0.3 + widen_threshold * 0.7)  # 0.3-1.0x multiplier
            return delta_magnitude > other_threshold

        return True

    def compute_magnitude_direction_sdxl(self, param_dict, desc="processing"):
        """Compute magnitude and direction optimized for SDXL parameters"""
        mags, dirs = {}, {}

        for name, tensor in param_dict.items():
            try:
                # SDXL-optimized magnitude/direction computation
                if tensor.dim() == 4:  # Conv layers (out_ch, in_ch, h, w)
                    o, c, h, w = tensor.shape
                    # For SDXL conv: preserve spatial structure in direction
                    if h * w <= 9:  # Small kernels (1x1, 3x3): flatten spatial
                        flat = tensor.view(o, -1)
                    else:  # Large kernels: treat each spatial position separately
                        flat = tensor.view(o, c, -1).mean(dim=2)  # Average spatial

                elif tensor.dim() == 3:  # Attention weights (heads, seq, dim)
                    flat = tensor.view(tensor.shape[0], -1)

                elif tensor.dim() == 2:  # Linear layers
                    flat = tensor

                elif tensor.dim() == 1:  # Bias, normalization parameters
                    # For 1D: each element is its own "feature"
                    flat = tensor.unsqueeze(0)

                else:
                    continue

                # Compute magnitude per output feature/channel
                if flat.dim() > 1:
                    mag = flat.norm(dim=-1)
                    # Stable direction computation
                    dir = flat / (mag.unsqueeze(-1) + 1e-8)
                else:
                    mag = flat.abs()
                    dir = torch.sign(flat)

                # Reshape back to match original tensor structure
                if tensor.dim() == 4 and h * w > 9:
                    # Expand back for large kernels
                    dirs[name] = dir.unsqueeze(-1).expand(-1, -1, h*w).view(tensor.shape)
                elif tensor.dim() == 1:
                    dirs[name] = dir.squeeze(0)
                    mag = mag.squeeze(0)
                else:
                    dirs[name] = dir

                mags[name] = mag

            except Exception as e:
                print(f"Warning: Failed to process {name}: {e}")
                continue

        return mags, dirs

    def rank_significance_adaptive(self, diff_tensor, layer_type='other'):
        """Enhanced ranking with SDXL layer-specific adaptations - BULLETPROOF - FIXED: Infinite loop prevention"""
        # Handle edge cases first
        if diff_tensor.numel() == 0:
            return diff_tensor

        if diff_tensor.numel() == 1:
            # Scalar tensor - return as-is
            return diff_tensor

        # Ensure minimum dimensionality
        if diff_tensor.ndim == 0:
            # 0D tensor - return as-is
            return diff_tensor

        original_shape = diff_tensor.shape

        try:
            # FIXED: Handle 1D tensors specially to prevent infinite recursion
            if diff_tensor.ndim == 1:
                # For 1D tensors, create simple ranking
                if diff_tensor.numel() <= 1:
                    return diff_tensor

                indices = torch.argsort(diff_tensor, dim=0)
                L = diff_tensor.shape[0]

                if layer_type in ['time_embedding', 'class_embedding']:
                    sig = torch.pow(torch.arange(L, device=diff_tensor.device, dtype=diff_tensor.dtype) / max(L-1, 1), 0.7)
                else:
                    sig = torch.arange(L, device=diff_tensor.device, dtype=diff_tensor.dtype) / max(L-1, 1)

                ranked = torch.zeros_like(diff_tensor)
                ranked.scatter_(0, indices, sig)
                return ranked

            # For multi-dimensional tensors, be more careful with flattening
            flat = None

            # FIXED: Safe flattening strategy to prevent infinite recursion
            if diff_tensor.ndim == 2:
                # 2D tensor - use as-is or flatten to 1D if one dimension is 1
                if diff_tensor.shape[0] == 1:
                    flat = diff_tensor.view(-1)  # Use view instead of flatten to prevent recursion
                elif diff_tensor.shape[1] == 1:
                    flat = diff_tensor.view(-1)  # Use view instead of flatten to prevent recursion
                else:
                    flat = diff_tensor
            else:
                # Higher dimensional tensors - flatten carefully
                try:
                    if layer_type in ['cross_attention', 'self_attention']:
                        # For attention: try to preserve structure
                        if diff_tensor.ndim > 2:
                            flat = diff_tensor.view(diff_tensor.shape[0], -1)  # Use view instead of flatten
                        else:
                            flat = diff_tensor
                    else:
                        # For other types: safe flattening
                        if diff_tensor.ndim > 2:
                            flat = diff_tensor.view(diff_tensor.shape[0], -1)  # Use view instead of flatten
                        else:
                            flat = diff_tensor
                except Exception:
                    # Fallback: complete flattening using view
                    flat = diff_tensor.view(-1)

            # Ensure we have a valid tensor for ranking
            if flat is None:
                flat = diff_tensor.view(-1)  # Use view instead of flatten

            # Handle the flattened tensor
            if flat.ndim == 1:
                # 1D case after flattening
                if flat.numel() <= 1:
                    return diff_tensor

                indices = torch.argsort(flat, dim=0)
                L = flat.shape[0]

                if layer_type in ['time_embedding', 'class_embedding']:
                    sig = torch.pow(torch.arange(L, device=flat.device, dtype=flat.dtype) / max(L-1, 1), 0.7)
                else:
                    sig = torch.arange(L, device=flat.device, dtype=flat.dtype) / max(L-1, 1)

                ranked_flat = torch.zeros_like(flat)
                ranked_flat.scatter_(0, indices, sig)

                # Reshape back to original if possible
                if ranked_flat.numel() == diff_tensor.numel():
                    return ranked_flat.view(original_shape)
                else:
                    return ranked_flat

            elif flat.ndim == 2:
                # 2D case - apply ranking along last dimension
                if flat.shape[-1] <= 1:
                    return diff_tensor

                indices = torch.argsort(flat, dim=-1)
                L = flat.shape[-1]

                if layer_type in ['time_embedding', 'class_embedding']:
                    sig = torch.pow(torch.arange(L, device=flat.device, dtype=flat.dtype) / max(L-1, 1), 0.7)
                else:
                    sig = torch.arange(L, device=flat.device, dtype=flat.dtype) / max(L-1, 1)

                # Create ranking matrix safely
                base = sig.unsqueeze(0).expand(flat.shape[0], -1)
                ranked = torch.zeros_like(flat)
                ranked.scatter_(-1, indices, base)

                # Reshape back to original if possible
                if ranked.numel() == diff_tensor.numel():
                    return ranked.view(original_shape)
                else:
                    return ranked
            else:
                # Higher dimensional - return original to avoid errors
                return diff_tensor

        except Exception as e:
            print(f"Warning: Ranking failed for tensor shape {diff_tensor.shape}, layer {layer_type}: {e}")
            # Ultimate fallback: return normalized tensor
            try:
                norm = torch.norm(diff_tensor)
                if norm > 1e-8:
                    return diff_tensor / norm
                else:
                    return diff_tensor
            except:
                return diff_tensor

    def compute_importance_sdxl(self, sig_tensor, layer_type='other', widen_threshold=0.5, calibration_value=0.0):
        """SDXL-optimized importance computation following WIDEN principles - FIXED: 0-1 calibration range"""

        try:
            # Handle edge cases
            if sig_tensor.numel() == 0:
                return torch.tensor(1.0, dtype=torch.float32, device=sig_tensor.device)

            # Layer-specific importance weighting
            layer_weight = self.sdxl_importance_weights.get(layer_type, 1.0)

            # Handle scalar tensors
            if sig_tensor.numel() == 1:
                # FIXED: Map 0-1 calibration to meaningful range (0.1-2.0)
                calibration_mapped = 0.1 + calibration_value * 1.9
                return torch.tensor(calibration_mapped * layer_weight, dtype=sig_tensor.dtype, device=sig_tensor.device)

            # Handle very small tensors
            if sig_tensor.numel() <= 2:
                calibration_mapped = 0.1 + calibration_value * 1.9
                return torch.full_like(sig_tensor, calibration_mapped * layer_weight)

            # Base softmax scoring with error handling
            try:
                if sig_tensor.ndim == 0:
                    calibration_mapped = 0.1 + calibration_value * 1.9
                    return torch.tensor(calibration_mapped * layer_weight, dtype=sig_tensor.dtype, device=sig_tensor.device)
                elif sig_tensor.ndim == 1:
                    softmax_dim = 0
                else: # ndim > 1
                    softmax_dim = -1 # FIXED: Apply softmax along the last dimension

                # Apply softmax with numerical stability
                sig_scaled = sig_tensor * layer_weight
                # Clamp to prevent overflow
                sig_scaled = torch.clamp(sig_scaled, min=-50, max=50)
                sc = torch.softmax(sig_scaled, dim=softmax_dim)

            except Exception as e:
                print(f"Warning: Softmax failed for tensor shape {sig_tensor.shape}: {e}")
                calibration_mapped = 0.1 + calibration_value * 1.9
                return torch.full_like(sig_tensor, calibration_mapped * layer_weight)

            # FIXED: Much more aggressive adaptive thresholding
            try:
                if sig_tensor.ndim > 1:
                    avg = sig_tensor.mean(0, keepdim=True)
                else:
                    avg = sig_tensor.mean()

                # FIXED: Use same aggressive scaling as should_merge_parameter
                if widen_threshold <= 0.5:
                    # Permissive range: 0.0-0.5 maps to 0.1x-1.0x
                    threshold_multiplier = 0.1 + (widen_threshold * 2) ** 2 * 0.9
                else:
                    # Selective range: 0.5-1.0 maps to 1.0x-100x
                    threshold_multiplier = 1.0 + ((widen_threshold - 0.5) * 2) ** 3 * 99

                # Layer-specific threshold adjustment
                if layer_type in ['time_embedding', 'class_embedding', 'cross_attention']:
                    # More selective for critical layers
                    adjusted_multiplier = threshold_multiplier * 1.2
                elif layer_type in ['normalization', 'bias']:
                    # Less selective for less critical layers
                    adjusted_multiplier = threshold_multiplier * 0.8
                else:
                    adjusted_multiplier = threshold_multiplier

                mask = sig_tensor > avg * adjusted_multiplier

                # FIXED: Apply calibration with 0-1 mapping to 0.1-2.0 range
                # 0.0 = minimal importance weighting (0.1x)
                # 0.5 = standard importance weighting (1.0x)
                # 1.0 = maximum importance weighting (2.0x)
                calibration_mapped = 0.1 + calibration_value * 1.9
                calibration_scaled = calibration_mapped * layer_weight
                sc = torch.where(mask, torch.tensor(calibration_scaled, dtype=sc.dtype, device=sc.device), sc)

                return sc

            except Exception as e:
                print(f"Warning: Thresholding failed for tensor shape {sig_tensor.shape}: {e}")
                calibration_mapped = 0.1 + calibration_value * 1.9
                return torch.full_like(sig_tensor, calibration_mapped * layer_weight)

        except Exception as e:
            print(f"Warning: Importance computation completely failed: {e}")
            # Ultimate fallback
            return torch.tensor(1.0, dtype=torch.float32, device=sig_tensor.device if hasattr(sig_tensor, 'device') else 'cpu')

    def merge_single_parameter_sdxl(self, deltas, base_param, mag_ranks, dir_ranks,
                                   param_name, metadata, widen_threshold=0.5, calibration_value=0.0):
        """SDXL-optimized parameter merging with layer-aware weighting - FIXED: Updated parameter name"""
        try:
            layer_type = metadata.get('layer_type', 'other')

            # FIXED: More robust delta magnitude calculation
            if len(deltas) == 0:
                return base_param

            # Calculate average magnitude safely
            total_norm = sum(torch.norm(delta).item() for delta in deltas if delta.numel() > 0)
            delta_mag = total_norm / max(len(deltas), 1)

            if not self.should_merge_parameter(param_name, delta_mag, metadata, widen_threshold):
                return base_param

            # Compute importance scores with comprehensive error handling
            try:
                mag_importance = self.compute_importance_sdxl(mag_ranks, layer_type, widen_threshold, calibration_value)
                dir_importance = self.compute_importance_sdxl(dir_ranks, layer_type, widen_threshold, calibration_value)
            except Exception as e:
                print(f"Warning: Failed to compute importance for {param_name}: {e}")
                # Fallback: use simple average instead of failing completely
                if hasattr(deltas, 'mean'):
                    return base_param + deltas.mean(0)
                else:
                    avg_delta = sum(deltas) / len(deltas)
                    return base_param + avg_delta

            # FIXED: Robust importance combination with proper shape validation
            try:
                # Ensure importance tensors have compatible shapes
                if mag_importance.numel() == 1 and dir_importance.numel() == 1:
                    # Both are scalars
                    combined_weights = 0.5 * (mag_importance + dir_importance)
                elif mag_importance.numel() == 1:
                    # Mag is scalar, dir is tensor
                    combined_weights = 0.5 * mag_importance.item() + 0.5 * dir_importance
                elif dir_importance.numel() == 1:
                    # Dir is scalar, mag is tensor
                    combined_weights = 0.5 * mag_importance + 0.5 * dir_importance.item()
                elif mag_importance.shape != dir_importance.shape:
                    print(f"Info: Using fallback for {param_name} due to shape mismatch: mag {mag_importance.shape} vs dir {dir_importance.shape}")
                    # FIXED: Better fallback - use scalar weights instead of tensor operations
                    mag_scalar = mag_importance.mean().item()
                    dir_scalar = dir_importance.mean().item()
                    combined_weights = 0.5 * (mag_scalar + dir_scalar)
                else:
                    # Layer-specific importance combination
                    if layer_type in ['cross_attention', 'self_attention']:
                        # For attention: direction is more important than magnitude
                        combined_weights = 0.3 * mag_importance + 0.7 * dir_importance
                    elif layer_type in ['normalization']:
                        # For normalization: magnitude is more important
                        combined_weights = 0.8 * mag_importance + 0.2 * dir_importance
                    else:
                        # Default balanced combination
                        combined_weights = 0.5 * mag_importance + 0.5 * dir_importance

                # Apply layer-specific importance weight
                layer_weight = self.sdxl_importance_weights.get(layer_type, 1.0)

                # FIXED: Ensure combined_weights is always scalar when multiplying with layer_weight
                if hasattr(combined_weights, 'numel') and combined_weights.numel() > 1:
                    combined_weights = combined_weights.mean() * layer_weight
                else:
                    if hasattr(combined_weights, 'item'):
                        combined_weights = combined_weights.item() * layer_weight
                    else:
                        combined_weights = combined_weights * layer_weight

            except Exception as e:
                print(f"Info: Using simple average for {param_name} due to weighting error: {e}")
                if hasattr(deltas, 'mean'):
                    return base_param + deltas.mean(0)
                else:
                    avg_delta = sum(deltas) / len(deltas)
                    return base_param + avg_delta

            # FIXED: Simplified tensor weighting - always use scalar weights to avoid shape issues
            try:
                # Ensure we always have a scalar weight
                if hasattr(combined_weights, 'item'):
                    weight_scalar = combined_weights.item()
                elif hasattr(combined_weights, '__len__') and len(combined_weights) > 1:
                    weight_scalar = float(torch.tensor(combined_weights).mean().item())
                else:
                    weight_scalar = float(combined_weights)

                # Apply scalar weight to deltas - much simpler and more reliable
                if hasattr(deltas, 'shape'):  # It's a tensor
                    weighted_deltas = deltas * weight_scalar
                else:  # It's a list
                    weighted_deltas = torch.stack([delta * weight_scalar for delta in deltas])

                # Sum weighted deltas and add to base
                merged = base_param + weighted_deltas.sum(0)

                # FIXED: Verify shape consistency more robustly
                if merged.shape != base_param.shape:
                    print(f"Info: Shape corrected for {param_name}: {merged.shape} -> {base_param.shape}")
                    # Try to reshape or fallback to simple average
                    if merged.numel() == base_param.numel():
                        merged = merged.view(base_param.shape)
                    else:
                        # Clean up failed merged tensor before fallback
                        del merged, weighted_deltas
                        if hasattr(deltas, 'mean'):
                            return base_param + deltas.mean(0)
                        else:
                            avg_delta = sum(deltas) / len(deltas)
                            return base_param + avg_delta

                # Clean up weighted_deltas immediately after use
                del weighted_deltas
                return merged

            except Exception as e:
                print(f"Info: Using simple fallback for {param_name}: {e}")
                # Simple cleanup without complex variable checking
                torch.cuda.empty_cache() if torch.cuda.is_available() else None

                # Final fallback: simple average
                if hasattr(deltas, 'mean'):
                    return base_param + deltas.mean(0)
                else:
                    avg_delta = sum(deltas) / len(deltas)
                    return base_param + avg_delta

        except Exception as e:
            print(f"Warning: Complete fallback for {param_name}: {e}")
            # Ultimate fallback: return unchanged base parameter
            return base_param

    def widen_merging_sdxl(
        self,
        target_model,
        base_model,
        models_to_merge,
        merge_strength: float = 1.0,
        renorm_mode: str = "magnitude",
        widen_threshold: float = 0.5,
        calibration_value: float = 0.0,
        batch_size: int = 50,
    ):
        """FULL ZERO-ACCUMULATION WIDEN algorithm for SDXL - No intermediate data storage - FIXED: Better memory management"""

        results_text = f"[{self.method}] Starting FULL ZERO-ACCUMULATION SDXL WIDEN merge\n"
        results_text += f"[{self.method}] Threshold: {widen_threshold}, Calibration: {calibration_value}\n"

        # Memory safety check
        safe, ram_percent, available_gb = check_memory_safety()
        if not safe:
            error_msg = f"[SAFETY] Cannot start - memory critical! RAM: {ram_percent*100:.1f}%, Available: {available_gb:.1f}GB"
            print(error_msg)
            raise MemoryExhaustionError(error_msg)

        print(f"[{self.method}] Initial memory check: {ram_percent*100:.1f}% used, {available_gb:.1f}GB available")

        # 1. Get base parameters list (names only, no tensor storage)
        print(f"[{self.method}] Getting parameter names...")
        base_param_names = list(base_model.named_parameters())
        param_names_only = [name for name, _ in base_param_names]

        # FIXED: Clean up the parameter list immediately
        del base_param_names
        gc.collect()

        # 2. Build task vectors with minimal storage
        print(f"[{self.method}] Building minimal task vectors...")
        task_vector_models = models_to_merge  # Just store model references

        # 3. Find common parameters without loading everything
        print(f"[{self.method}] Finding common parameters...")
        common_params = set(param_names_only)
        for model in models_to_merge:
            model_param_names = set(name for name, _ in model.named_parameters())
            common_params &= model_param_names
        common_params = list(common_params)

        # FIXED: Clean up parameter names immediately
        del param_names_only
        gc.collect()

        print(f"[{self.method}] Found {len(common_params)} common parameters")

        # 4. FULL ZERO-ACCUMULATION: Process each parameter individually
        target_state_dict = target_model.state_dict()
        layer_stats = {}
        merged_count = 0
        failed_count = 0
        skipped_count = 0

        print(f"[{self.method}] Starting FULL ZERO-ACCUMULATION processing...")

        for param_idx, name in enumerate(common_params):
            # Progress tracking - reduced frequency
            if param_idx % 200 == 0:  # Reduced from every 50 to every 200
                progress = (param_idx / len(common_params)) * 100
                print(f"[PROGRESS] {param_idx}/{len(common_params)} ({progress:.1f}%)")

                # Memory safety check
                safe, ram_percent, available_gb = check_memory_safety()
                if not safe:
                    print(f"[EMERGENCY] Memory critical at parameter {param_idx}! Stopping safely...")
                    partial_results = f"""
[PARTIAL] Emergency stop at parameter {param_idx}/{len(common_params)}:
  - Processed: {param_idx}/{len(common_params)} parameters ({param_idx/len(common_params)*100:.1f}%)"""
                    return results_text + partial_results

            try:
                # STEP 1: Load ONLY the current parameter from all models (zero-accumulation)
                base_param = None
                deltas = []

                # Get base parameter
                for param_name, param in base_model.named_parameters():
                    if param_name == name:
                        base_param = param.detach().cpu().float()  # FIXED: Removed redundant .clone()
                        break

                if base_param is None:
                    skipped_count += 1
                    continue

                # Get deltas from each model (one at a time)
                for model in task_vector_models:
                    other_param = None
                    for param_name, param in model.named_parameters():
                        if param_name == name:
                            other_param = param.detach().cpu().float()  # FIXED: Removed redundant .clone()
                            break

                    if other_param is not None and other_param.shape == base_param.shape:
                        delta = other_param - base_param
                        deltas.append(delta)
                        del other_param  # Immediate cleanup

                if len(deltas) == 0:
                    skipped_count += 1
                    del base_param
                    continue

                # STEP 2: Classify layer and get metadata (zero-accumulation)
                layer_type = self._classify_sdxl_layer(name)
                if layer_type not in layer_stats:
                    layer_stats[layer_type] = {'merged': 0, 'skipped': 0, 'failed': 0}

                # FIXED: More robust metadata calculation
                base_magnitude = torch.norm(base_param).item()
                delta_magnitudes = [torch.norm(d).item() for d in deltas if d.numel() > 0]
                avg_delta_magnitude = sum(delta_magnitudes) / max(len(delta_magnitudes), 1)

                metadata = {
                    'layer_type': layer_type,
                    'base_magnitude': base_magnitude,
                    'delta_magnitude': avg_delta_magnitude,
                    'change_ratio': avg_delta_magnitude / (base_magnitude + 1e-8)  # FIXED: Avoid division by zero
                }

                # DIAGNOSTIC: Log threshold analysis for first few parameters
                if param_idx < 10:
                    base_threshold = self.sdxl_thresholds.get(layer_type, 0.0001)
                    if widen_threshold <= 0.5:
                        threshold_multiplier = 0.001 + (widen_threshold * 2) ** 3 * 0.999
                    else:
                        threshold_multiplier = 1.0 + ((widen_threshold - 0.5) * 2) ** 4 * 999
                    final_threshold = base_threshold * threshold_multiplier

                    print(f"[DIAGNOSTIC] {name[:30]}: layer={layer_type}, "
                          f"delta_mag={avg_delta_magnitude:.6f}, base_thresh={base_threshold:.6f}, "
                          f"multiplier={threshold_multiplier:.3f}, final_thresh={final_threshold:.6f}, "
                          f"passes={avg_delta_magnitude >= final_threshold}")

                # Early threshold check for efficiency
                if not self.should_merge_parameter(name, avg_delta_magnitude, metadata, widen_threshold):
                    skipped_count += 1
                    layer_stats[layer_type]['skipped'] += 1
                    del base_param, deltas
                    continue

                # STEP 3: Compute magnitude/direction ON-THE-FLY (zero-accumulation)
                base_mag, base_dir = self.compute_magnitude_direction_sdxl({name: base_param}, "silent")

                mag_diffs = []
                dir_diffs = []

                for i, delta in enumerate(deltas):
                    # Compute magnitude/direction for this delta only
                    other_param = base_param + delta
                    other_mag, other_dir = self.compute_magnitude_direction_sdxl({name: other_param}, "silent")

                    if name in base_mag and name in other_mag:
                        mag_diff = (other_mag[name] - base_mag[name]).abs()
                        layer_weight = self.sdxl_importance_weights.get(layer_type, 1.0)
                        mag_diffs.append(mag_diff * layer_weight)

                    if name in base_dir and name in other_dir:
                        try:
                            b_dir_p = base_dir[name]
                            o_dir_p = other_dir[name]
                            
                            # Case 1: Per-feature magnitude case (Linear layers)
                            is_per_feature_magnitude_case = (
                                name in base_mag and
                                base_mag[name].ndim == 1 and
                                b_dir_p.ndim == 2 and o_dir_p.ndim == 2 and
                                b_dir_p.shape == o_dir_p.shape and
                                b_dir_p.shape[0] == base_mag[name].shape[0]
                            )

                            if is_per_feature_magnitude_case:
                                # Compute cosine similarity along feature embedding dimension
                                cos_sim = torch.cosine_similarity(o_dir_p, b_dir_p, dim=-1)
                                cos_sim = torch.clamp(cos_sim, -1.0, 1.0)
                                layer_weight_val = self.sdxl_importance_weights.get(layer_type, 1.0)
                                current_dir_diff_val = (1.0 - cos_sim) * layer_weight_val
                                dir_diffs.append(current_dir_diff_val)
                            else:
                                # Case 2: Scalar magnitude case (bias, etc.)
                                if b_dir_p.numel() == 1 and o_dir_p.numel() == 1:
                                    dir_diff = torch.abs(o_dir_p - b_dir_p)
                                    layer_weight_val = self.sdxl_importance_weights.get(layer_type, 1.0)
                                    dir_diffs.append(dir_diff * layer_weight_val)
                                else:
                                    # Case 3: General tensor case - flatten and compute cosine similarity
                                    try:
                                        base_flat = b_dir_p.view(-1)
                                        other_flat = o_dir_p.view(-1)
                                        cos_sim = torch.cosine_similarity(other_flat, base_flat, dim=0)
                                        cos_sim = torch.clamp(cos_sim, -1.0, 1.0)
                                        dir_diff = 1 - cos_sim
                                        layer_weight_val = self.sdxl_importance_weights.get(layer_type, 1.0)
                                        dir_diffs.append(dir_diff * layer_weight_val)
                                    except Exception as fallback_e:
                                        # Fallback to mean absolute difference
                                        dir_diff = torch.abs(o_dir_p - b_dir_p).mean()
                                        layer_weight_val = self.sdxl_importance_weights.get(layer_type, 1.0)
                                        dir_diffs.append(dir_diff * layer_weight_val)
                        except Exception as e:
                            # FIXED: Better error handling for direction computation
                            print(f"Warning: Direction computation failed for {name}: {e}")
                            dir_diff = torch.tensor(0.1, dtype=torch.float32)  # Small default value
                            layer_weight = self.sdxl_importance_weights.get(layer_type, 1.0)
                            dir_diffs.append(dir_diff * layer_weight)

                    del other_param  # Immediate cleanup

                # Clean up magnitude/direction data immediately
                del base_mag, base_dir

                # STEP 4: Apply WIDEN algorithm with immediate cleanup
                if len(mag_diffs) == 0 or len(dir_diffs) == 0:
                    # WIDEN failed - check if we should still merge with simple average
                    avg_delta_mag = metadata['delta_magnitude']
                    if not self.should_merge_parameter(name, avg_delta_mag, metadata, widen_threshold):
                        skipped_count += 1
                        layer_stats[layer_type]['skipped'] += 1
                        del base_param, deltas
                        continue

                    # Fallback to simple average (silent)
                    try:
                        if len(deltas) > 0:
                            avg_delta = sum(deltas) / len(deltas)
                            final_merged = base_param + avg_delta * merge_strength
                            del deltas, avg_delta  # Immediate cleanup
                        else:
                            final_merged = base_param
                            del deltas
                    except Exception as e:
                        failed_count += 1
                        layer_stats[layer_type]['failed'] += 1
                        del base_param, deltas
                        continue
                else:
                    try:
                        # FIXED: Better tensor stacking with validation
                        if not deltas:
                            final_merged = base_param
                        else:
                            deltas_tensor = torch.stack(deltas)

                            # FIXED: Enhanced tensor stacking validation for mag_diffs and dir_diffs
                            # Ensure all mag_diffs have the same shape before stacking
                            if mag_diffs and all(isinstance(d, torch.Tensor) for d in mag_diffs):
                                mag_shapes = [d.shape for d in mag_diffs]
                                if all(shape == mag_shapes[0] for shape in mag_shapes):
                                    mag_diffs_tensor = torch.stack(mag_diffs)
                                else:
                                    # Convert to scalars if shapes are inconsistent
                                    mag_scalars = [d.mean().item() if d.numel() > 1 else d.item() for d in mag_diffs]
                                    mag_diffs_tensor = torch.tensor(mag_scalars, dtype=torch.float32)
                            else:
                                # Fallback for empty or invalid mag_diffs
                                mag_diffs_tensor = torch.ones(len(deltas), dtype=torch.float32)

                            if dir_diffs and all(isinstance(d, torch.Tensor) for d in dir_diffs):
                                dir_shapes = [d.shape for d in dir_diffs]
                                if all(shape == dir_shapes[0] for shape in dir_shapes):
                                    dir_diffs_tensor = torch.stack(dir_diffs)
                                else:
                                    # Convert to scalars if shapes are inconsistent
                                    dir_scalars = [d.mean().item() if d.numel() > 1 else d.item() for d in dir_diffs]
                                    dir_diffs_tensor = torch.tensor(dir_scalars, dtype=torch.float32)
                            else:
                                # Fallback for empty or invalid dir_diffs
                                dir_diffs_tensor = torch.ones(len(deltas), dtype=torch.float32)

                            # Clean up lists immediately
                            del deltas, mag_diffs, dir_diffs

                            # Rank significance
                            mag_ranks = self.rank_significance_adaptive(mag_diffs_tensor, layer_type)
                            dir_ranks = self.rank_significance_adaptive(dir_diffs_tensor, layer_type)

                            # Clean up diff tensors immediately
                            del mag_diffs_tensor, dir_diffs_tensor

                            # Merge with WIDEN algorithm
                            merged_param = self.merge_single_parameter_sdxl(
                                deltas_tensor, base_param, mag_ranks, dir_ranks,
                                name, metadata, widen_threshold, calibration_value
                            )

                            # Clean up intermediate tensors immediately
                            del deltas_tensor, mag_ranks, dir_ranks

                            # Apply strength
                            final_merged = base_param + (merged_param - base_param) * merge_strength
                            del merged_param  # Immediate cleanup

                    except Exception as e:
                        # WIDEN failed - check threshold before fallback
                        avg_delta_mag = metadata['delta_magnitude']
                        if not self.should_merge_parameter(name, avg_delta_mag, metadata, widen_threshold):
                            skipped_count += 1
                            layer_stats[layer_type]['skipped'] += 1
                            del base_param
                            if 'deltas' in locals():
                                del deltas
                            continue

                        # Silent fallback for WIDEN failures
                        try:
                            if 'deltas' in locals() and deltas:
                                avg_delta = sum(deltas) / len(deltas)
                                final_merged = base_param + avg_delta * merge_strength
                                del deltas, avg_delta
                            else:
                                final_merged = base_param
                                if 'deltas' in locals():
                                    del deltas
                        except Exception as e2:
                            failed_count += 1
                            layer_stats[layer_type]['failed'] += 1
                            del base_param
                            if 'deltas' in locals():
                                del deltas
                            continue

                # STEP 5: Apply renormalization and write to target (zero-accumulation)
                if renorm_mode != "none":
                    try:
                        if renorm_mode == "calibrate":
                            # FIXED: More conservative calibrate parameters
                            # t=0.3 (lower = more selective), s=1.1 (lower = less aggressive scaling)
                            final_merged = calibrate_renormalize(
                                final_merged, base_param, renorm_mode, 0.3, 1.1
                            )
                        else:  # magnitude
                            final_merged = calibrate_renormalize(
                                final_merged, base_param, renorm_mode, 1.0, 1.0
                            )
                    except Exception as e:
                        # Silent renormalization failure handling
                        pass

                # Write directly to target model
                try:
                    target_device = target_state_dict[name].device
                    if final_merged.device != target_device:
                        final_merged = final_merged.to(target_device)

                    if final_merged.shape != target_state_dict[name].shape:
                        if final_merged.numel() == target_state_dict[name].numel():
                            final_merged = final_merged.view(target_state_dict[name].shape)
                        else:
                            failed_count += 1
                            layer_stats[layer_type]['failed'] += 1
                            del base_param, final_merged
                            continue

                    target_state_dict[name].copy_(final_merged)
                    merged_count += 1
                    layer_stats[layer_type]['merged'] += 1

                except Exception as e:
                    failed_count += 1
                    layer_stats[layer_type]['failed'] += 1

                # Clean up all remaining tensors for this parameter
                del base_param, final_merged

                # Light periodic cleanup - reduced frequency
                if param_idx % 150 == 0:  # Further reduced frequency
                    gentle_cleanup()

            except Exception as e:
                failed_count += 1
                if layer_type not in layer_stats:
                    layer_stats[layer_type] = {'merged': 0, 'skipped': 0, 'failed': 0}
                layer_stats[layer_type]['failed'] += 1

                # Simple cleanup on exception
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                gc.collect()
                continue

        total_params = len(common_params)

        # Generate detailed layer-wise results
        layer_report = "\n[LAYER-WISE RESULTS]:\n"
        for layer_type, stats in sorted(layer_stats.items()):
            total_layer = sum(stats.values())
            if total_layer > 0:
                layer_report += f"  {layer_type}: {stats['merged']}/{total_layer} merged "
                layer_report += f"({stats['merged']/total_layer*100:.1f}% success), "
                layer_report += f"{stats['skipped']} skipped, {stats['failed']} failed\n"

        # FIXED: Better summary stats
        total_processed = merged_count + skipped_count + failed_count
        fallback_count = merged_count - sum(1 for layer_stats_dict in layer_stats.values()
                                          for count in [layer_stats_dict.get('merged', 0)])

        results_text += f"""
[RESULTS] FULL ZERO-ACCUMULATION WIDEN merge complete:
  - Total parameters processed: {total_processed}
  - Successfully merged with WIDEN: {merged_count}/{total_params} parameters ({merged_count/total_params*100:.1f}%)
  - Skipped (below threshold): {skipped_count} ({skipped_count/total_params*100:.1f}%)
  - Failed: {failed_count}
  - Threshold effectiveness: {(total_params - skipped_count)/total_params*100:.1f}% of parameters met threshold
  - Renormalization: {'enabled' if renorm_mode != 'none' else 'disabled'} (mode: {renorm_mode})
  - Full zero-accumulation: âœ“ (absolute minimal memory footprint)
{layer_report}
[THRESHOLD ANALYSIS]:
  - widen_threshold: {widen_threshold} (0.0=permissive, 1.0=selective)
  - Parameters above threshold: {merged_count + failed_count}/{total_params}
  - Selectivity working: {'YES' if skipped_count > 0 else 'NO - All parameters passed threshold'}"""

        print(results_text)

        # FIXED: Extra aggressive cleanup at end of merge
        print("[CLEANUP] Post-merge aggressive cleanup...")
        del target_state_dict, layer_stats
        force_cleanup()
        force_cleanup()  # Double cleanup for stubborn references

        return results_text

    def _classify_sdxl_layer(self, param_name):
        """Classify SDXL layer types for specialized handling - ENHANCED"""
        name_lower = param_name.lower()

        # UNet structure classification - more comprehensive
        if 'time_embed' in name_lower:
            return 'time_embedding'
        elif 'label_emb' in name_lower:
            return 'class_embedding'
        elif any(x in name_lower for x in ['attn', 'attention']):
            if 'cross' in name_lower:
                return 'cross_attention'  # Text conditioning
            else:
                return 'self_attention'   # Spatial attention
        elif any(x in name_lower for x in ['conv', 'convolution']):
            if 'in_layers' in name_lower or 'input' in name_lower:
                return 'input_conv'
            elif 'out_layers' in name_lower or 'output' in name_lower:
                return 'output_conv'
            elif 'skip' in name_lower or 'residual' in name_lower:
                return 'skip_conv'
            else:
                return 'feature_conv'
        elif any(x in name_lower for x in ['norm', 'group_norm', 'layer_norm']):
            return 'normalization'
        elif 'bias' in name_lower:
            return 'bias'
        elif any(x in name_lower for x in ['down', 'upsample']):
            return 'resolution_change'
        # Enhanced classification for previously 'other' parameters
        elif any(x in name_lower for x in ['proj', 'projection']):
            return 'self_attention'  # Projections are usually part of attention
        elif any(x in name_lower for x in ['to_q', 'to_k', 'to_v', 'to_out']):
            return 'self_attention'  # Attention components
        elif any(x in name_lower for x in ['ff', 'feedforward', 'mlp']):
            return 'self_attention'  # Feed-forward networks in transformers
        elif 'weight' in name_lower and 'emb' in name_lower:
            return 'time_embedding'  # Embedding weights
        else:
            return 'other'


class DonutWidenMergeUNet:
    class_type = "MODEL"

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_base": ("MODEL",),
                "model_other": ("MODEL",),
                "merge_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 5.0, "step": 0.01}),
                "min_strength": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 5.0, "step": 0.1}),
                "max_strength": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 5.0, "step": 0.1}),
                "normalization_mode": (["magnitude", "calibrate", "none"], {"default": "magnitude"}),  # (renorm_mode)
                # Enhanced WIDEN parameters
                "importance_threshold": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 500.0, "step": 0.1}),  # (above_average_value_ratio)
                "importance_boost": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 3.0, "step": 0.1}),  # (score_calibration_value)
                # Dynamic compatibility settings  
                "rank_sensitivity": ("FLOAT", {"default": 2.0, "min": 0.0, "max": 10.0, "step": 0.1}),  # (compatibility_sensitivity)
                "skip_threshold": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.000001}),  # (compatibility_threshold)
            },
            "optional": {
                "lora_stack": ("LORA_STACK",),
                "model_3": ("MODEL",),
                "model_4": ("MODEL",),
                "model_5": ("MODEL",),
                "model_6": ("MODEL",),
                "model_7": ("MODEL",),
                "model_8": ("MODEL",),
                "model_9": ("MODEL",),
                "model_10": ("MODEL",),
                "model_11": ("MODEL",),
                "model_12": ("MODEL",),
            }
        }

    RETURN_TYPES = ("MODEL", "STRING", "STRING")
    RETURN_NAMES = ("model", "merge_results", "parameter_info")
    FUNCTION = "execute"
    CATEGORY = "donut/merge"

    def execute(self, model_base, model_other, merge_strength, min_strength, max_strength, normalization_mode,
                importance_threshold, importance_boost,
                rank_sensitivity, skip_threshold,
                lora_stack=None, model_3=None, model_4=None, model_5=None, model_6=None,
                model_7=None, model_8=None, model_9=None, model_10=None,
                model_11=None, model_12=None):

        # Conservative pre-merge setup with session cache management
        print("[MEMORY] Pre-merge setup...")
        
        # Clear session cache if it's getting large (prevents accumulation)
        if len(_MERGE_CACHE) >= _CACHE_MAX_SIZE:
            clear_session_cache()
        
        # Light pre-merge cleanup
        gentle_cleanup()

        # Check cache first
        all_models = [model_base, model_other, model_3, model_4, model_5, model_6,
                     model_7, model_8, model_9, model_10, model_11, model_12]
        cache_key = compute_merge_hash(all_models, merge_strength, min_strength, max_strength, importance_threshold, importance_boost, rank_sensitivity, skip_threshold, f"{normalization_mode}_enhanced_widen", lora_stack)
        # UNet cache analysis

        cached_result = check_cache_for_merge(cache_key)
        if cached_result is not None:
            # Cache hit
            return cached_result
        else:
            # Cache miss - computing fresh merge
            pass

        with memory_cleanup_context("DonutWidenMergeUNet"):
            import copy
            import gc

            # Process LoRA stack if provided
            lora_processor = None
            if lora_stack is not None:
                print("[DonutWidenMergeUNet] Processing LoRA stack for delta-based merging...")
                # For UNet LoRA processing, we need a CLIP object for proper key mapping
                # We'll pass None for now and warn about potential key mapping issues
                lora_processor = LoRAStackProcessor(model_base, base_clip=None)
                lora_processor.add_lora_from_stack(lora_stack)
                
                # Get summary of LoRA processing
                summary = lora_processor.get_summary()
                print(f"[LoRADelta] Processed {summary['lora_count']} LoRAs with {summary['total_delta_parameters']} delta parameters")
                print(f"[LoRADelta] LoRA names: {summary['lora_names']}")

            # FIXED: Filter out None models and filler models more safely
            models_to_merge = []
            for m in all_models[1:]:  # Skip model_base
                if m is not None and not getattr(m, "_is_filler", False):
                    models_to_merge.append(m)

            # Add LoRA-enhanced virtual models if available
            if lora_processor is not None:
                virtual_models = lora_processor.get_virtual_models()
                # Skip the base model (first item) since we already have it
                lora_virtual_models = virtual_models[1:]  # Only LoRA deltas
                models_to_merge.extend(lora_virtual_models)
                print(f"[LoRADelta] Added {len(lora_virtual_models)} LoRA-enhanced virtual models")

            print(f"[DonutWidenMergeUNet] WIDEN merging {len(models_to_merge)} models ({len([m for m in models_to_merge if hasattr(m, 'lora_name')])} from LoRA stack)")

            try:
                base_model_obj = model_base.model
                # Handle both regular models and LoRADelta objects
                other_model_objs = []
                for model in models_to_merge:
                    if hasattr(model, 'lora_name'):  # LoRADelta object
                        other_model_objs.append(model)
                    else:  # Regular model
                        other_model_objs.append(model.model)

                # FIXED: Memory-efficient model cloning to avoid GPU OOM
                # Instead of deepcopy which duplicates all tensors in VRAM, just copy the wrapper
                model_merged = model_base.clone()  # Clone to always use fresh ingredients
                # Keep reference to original model - we'll update parameters in-place later
                # This avoids the VRAM spike from deepcopy

                # Use enhanced WIDEN merger with dynamic compatibility
                print("[DonutWidenMergeUNet] Using WIDEN merge with dynamic compatibility-based strength...")
                
                merger = MergingMethod("DonutWidenMergeUNet")
                
                if rank_sensitivity > 0.0:
                    # Use WIDEN with dynamic strength based on compatibility
                    merged_params, widen_diagnostics = enhanced_widen_merging_with_dynamic_strength(
                        merger=merger,
                        merged_model=model_merged.model,
                        models_to_merge=other_model_objs,
                        exclude_param_names_regex=[],
                        importance_threshold=importance_threshold,
                        importance_boost=importance_boost,
                        merge_strength=merge_strength,
                        min_strength=min_strength,
                        max_strength=max_strength,
                        rank_sensitivity=rank_sensitivity,
                        skip_threshold=skip_threshold,
                        normalization_mode=normalization_mode
                    )
                    
                    # Apply merged parameters with shape validation
                    applied_count = 0
                    shape_mismatch_count = 0
                    for param_name, param_value in merged_params.items():
                        for name, param in model_merged.model.named_parameters():
                            if name == param_name:
                                # Detect and remove an accidental leading batch-of-1 dim
                                if param_value.ndim == param.ndim + 1 and param_value.size(0) == 1:
                                    param_value = param_value.squeeze(0)
                                
                                if param_value.shape == param.shape:
                                    param.data.copy_(param_value)
                                    applied_count += 1
                                else:
                                    print(f"[WARNING] Shape mismatch for {param_name}: expected {param.shape}, got {param_value.shape}")
                                    shape_mismatch_count += 1
                                break
                    
                    print(f"[ENHANCED WIDEN] Applied {applied_count} parameters, {shape_mismatch_count} shape mismatches")
                    
                    # Compute and display merge sanity metrics
                    try:
                        sanity_metrics = compute_merge_sanity_metrics(base_model, merged_model)
                        print_merge_diagnostics(sanity_metrics)
                    except Exception as e:
                        diagnostic_logger.warning(f"Failed to compute sanity metrics: {e}")
                    
                    # Create detailed merge results with enhanced WIDEN diagnostics
                    total_models = len([m for m in [model_other, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10, model_11, model_12] if m is not None]) + 1
                    
                    # Generate enhanced diagnostics from widen_diagnostics
                    compatibility_scores = widen_diagnostics['compatibility_scores']
                    varied_count = widen_diagnostics['varied_score_count']
                    uniform_count = widen_diagnostics['uniform_score_count']
                    skipped_threshold = widen_diagnostics['parameters_skipped_threshold']
                    
                    # Extract strength distribution metrics
                    strength_dist = widen_diagnostics['strength_distribution']
                    applied_strengths = widen_diagnostics['applied_strengths']
                    
                    # Sanitize strength distribution for display
                    clean_strength_dist = sanitize_strength_distribution(strength_dist)
                    
                    if compatibility_scores:
                        compat_values = [score['compatibility'] for score in compatibility_scores]
                        compat_min, compat_max = min(compat_values), max(compat_values)
                        compat_mean = sum(compat_values) / len(compat_values)
                        compat_variance = sum((x - compat_mean)**2 for x in compat_values) / len(compat_values)
                        compat_range = compat_max - compat_min
                        # Use relative variance threshold based on score range
                        relative_variance_threshold = max(1e-6, (compat_range * 0.01) ** 2)
                        score_health = "âœ“ VARIED" if compat_variance > relative_variance_threshold else "âœ— UNIFORM (BUG!)"
                    else:
                        compat_min = compat_max = compat_mean = 0.0
                        score_health = "NO SCORES"
                    
                    ranking_health = "âœ“ HEALTHY" if varied_count > uniform_count else "âœ— FAILING"
                    total_scored = varied_count + uniform_count
                    
                    # Generate skip threshold recommendation based on actual compatibility scores
                    compatibility_score_dict = {score['parameter']: score['compatibility'] for score in compatibility_scores}
                    recommended_threshold, threshold_analysis = _analyze_compatibility_patterns_and_recommend_threshold(
                        compatibility_score_dict, skip_threshold
                    )
                    
                    # Add threshold analysis to console output
                    performance_logger.info(f"Skip threshold analysis:\n{threshold_analysis}")
                    
                    results_text = f"""â•”â• WIDEN MERGE RESULTS (Dynamic Compatibility) â•â•—
â•‘ Models: {total_models} | Strength: {min_strength}-{max_strength} | Mode: {normalization_mode}
â•‘ Threshold: {importance_threshold} | Boost: {importance_boost} | Sensitivity: {rank_sensitivity}
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Parameters Merged: {len(merged_params)} | Applied: {applied_count}
â•‘ Shape Mismatches: {shape_mismatch_count} | Success: {(applied_count/(applied_count+shape_mismatch_count)*100):.1f}%
â•‘ Status: âœ“ Enhanced WIDEN with Dynamic Compatibility
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ðŸ” WIDEN ALGORITHM HEALTH DIAGNOSTICS:
â•‘ Compatibility Range: {compat_min:.4f} - {compat_max:.4f} (avg: {compat_mean:.4f})
â•‘ Strength Distribution: {clean_strength_dist['display_text']}
â•‘ Applied Strengths: {clean_strength_dist['count']} parameters with dynamic adjustment
â•‘ Score Distribution: {score_health}
â•‘ Parameter Ranking: {varied_count}/{total_scored} varied ({100*varied_count/total_scored if total_scored > 0 else 0:.1f}%) - {ranking_health}
â•‘ Skip Threshold: {skip_threshold} (percentile) â†’ {skipped_threshold} parameters skipped
â•‘ ðŸ’¡ RECOMMENDED: skip_threshold = {recommended_threshold:.6f} for optimal performance
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    # Create detailed parameter information
                    parameter_info = f"""â•”â• WIDEN PARAMETER DETAILS â•â•—
â•‘ Merge Strength: Controls blend intensity between models
â•‘ Importance Threshold: Multiplier for classifying important parameters
â•‘ Importance Boost: Score amplification for important parameters  
â•‘ Rank Sensitivity: Dynamic compatibility adjustment strength
â•‘ Skip Threshold: Excludes low-compatibility parameters from merge
â•‘ Normalization: {normalization_mode} - post-merge parameter scaling
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Dynamic Mode: Adapts strength based on compatibility
â•‘ Total Processed: {len(merged_params)} parameters analyzed
â•‘ Applied Successfully: {applied_count} parameters merged
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    
                else:
                    # Use enhanced WIDEN without dynamic strength (fallback mode)
                    merged_params, widen_diagnostics = enhanced_widen_merging_with_dynamic_strength(
                        merger=merger,
                        merged_model=model_merged.model,
                        models_to_merge=other_model_objs,
                        exclude_param_names_regex=[],
                        importance_threshold=importance_threshold,
                        importance_boost=importance_boost,
                        merge_strength=merge_strength,
                        min_strength=min_strength,
                        max_strength=max_strength,
                        rank_sensitivity=0.0,  # Disable dynamic strength
                        skip_threshold=skip_threshold,
                        normalization_mode=normalization_mode
                    )
                    
                    # Apply merged parameters with shape validation
                    applied_count = 0
                    shape_mismatch_count = 0
                    for param_name, param_value in merged_params.items():
                        for name, param in model_merged.model.named_parameters():
                            if name == param_name:
                                # Detect and remove an accidental leading batch-of-1 dim
                                if param_value.ndim == param.ndim + 1 and param_value.size(0) == 1:
                                    param_value = param_value.squeeze(0)
                                
                                if param_value.shape == param.shape:
                                    param.data.copy_(param_value)
                                    applied_count += 1
                                else:
                                    print(f"[WARNING] Shape mismatch for {param_name}: expected {param.shape}, got {param_value.shape}")
                                    shape_mismatch_count += 1
                                break
                    
                    print(f"[ENHANCED WIDEN] Applied {applied_count} parameters, {shape_mismatch_count} shape mismatches")
                    
                    # Compute and display merge sanity metrics
                    try:
                        sanity_metrics = compute_merge_sanity_metrics(base_model, merged_model)
                        print_merge_diagnostics(sanity_metrics)
                    except Exception as e:
                        diagnostic_logger.warning(f"Failed to compute sanity metrics: {e}")
                    
                    # Create detailed merge results with enhanced WIDEN diagnostics
                    total_models = len([m for m in [model_other, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10, model_11, model_12] if m is not None]) + 1
                    
                    # Generate enhanced diagnostics from widen_diagnostics
                    compatibility_scores = widen_diagnostics['compatibility_scores']
                    varied_count = widen_diagnostics['varied_score_count']
                    uniform_count = widen_diagnostics['uniform_score_count']
                    skipped_threshold = widen_diagnostics['parameters_skipped_threshold']
                    
                    # Extract strength distribution metrics
                    strength_dist = widen_diagnostics['strength_distribution']
                    applied_strengths = widen_diagnostics['applied_strengths']
                    
                    # Sanitize strength distribution for display
                    clean_strength_dist = sanitize_strength_distribution(strength_dist)
                    
                    if compatibility_scores:
                        compat_values = [score['compatibility'] for score in compatibility_scores]
                        compat_min, compat_max = min(compat_values), max(compat_values)
                        compat_mean = sum(compat_values) / len(compat_values)
                        compat_variance = sum((x - compat_mean)**2 for x in compat_values) / len(compat_values)
                        compat_range = compat_max - compat_min
                        # Use relative variance threshold based on score range
                        relative_variance_threshold = max(1e-6, (compat_range * 0.01) ** 2)
                        score_health = "âœ“ VARIED" if compat_variance > relative_variance_threshold else "âœ— UNIFORM (BUG!)"
                    else:
                        compat_min = compat_max = compat_mean = 0.0
                        score_health = "NO SCORES"
                    
                    ranking_health = "âœ“ HEALTHY" if varied_count > uniform_count else "âœ— FAILING"
                    total_scored = varied_count + uniform_count
                    
                    # Generate skip threshold recommendation for static strength mode too
                    compatibility_score_dict = {score['parameter']: score['compatibility'] for score in compatibility_scores}
                    recommended_threshold, threshold_analysis = _analyze_compatibility_patterns_and_recommend_threshold(
                        compatibility_score_dict, skip_threshold
                    )
                    
                    # Add threshold analysis to console output
                    performance_logger.info(f"Skip threshold analysis (static mode):\n{threshold_analysis}")
                    
                    results_text = f"""â•”â• WIDEN MERGE RESULTS (Static Strength) â•â•—
â•‘ Models: {total_models} | Strength: {min_strength}-{max_strength} | Mode: {normalization_mode}
â•‘ Threshold: {importance_threshold} | Boost: {importance_boost} | Sensitivity: {rank_sensitivity} (off)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Parameters Merged: {len(merged_params)} | Applied: {applied_count}
â•‘ Shape Mismatches: {shape_mismatch_count} | Success: {(applied_count/(applied_count+shape_mismatch_count)*100):.1f}%
â•‘ Status: âœ“ Enhanced WIDEN with Static Strength
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ðŸ” WIDEN ALGORITHM HEALTH DIAGNOSTICS:
â•‘ Compatibility Range: {compat_min:.4f} - {compat_max:.4f} (avg: {compat_mean:.4f})
â•‘ Strength Distribution: {clean_strength_dist['display_text']}
â•‘ Applied Strengths: {clean_strength_dist['count']} parameters with dynamic adjustment
â•‘ Score Distribution: {score_health}
â•‘ Parameter Ranking: {varied_count}/{total_scored} varied ({100*varied_count/total_scored if total_scored > 0 else 0:.1f}%) - {ranking_health}
â•‘ Skip Threshold: {skip_threshold} (percentile) â†’ {skipped_threshold} parameters skipped
â•‘ ðŸ’¡ RECOMMENDED: skip_threshold = {recommended_threshold:.6f} for optimal performance
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    # Create detailed parameter information  
                    parameter_info = f"""â•”â• WIDEN PARAMETER DETAILS â•â•—
â•‘ Merge Strength: Controls blend intensity between models
â•‘ Importance Threshold: Multiplier for classifying important parameters
â•‘ Importance Boost: Score amplification for important parameters
â•‘ Rank Sensitivity: {rank_sensitivity} (disabled) - no dynamic adjustment
â•‘ Skip Threshold: Excludes low-compatibility parameters from merge
â•‘ Normalization: {normalization_mode} - post-merge parameter scaling
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Static Mode: Uses fixed strength for all parameters
â•‘ Total Processed: {len(merged_params)} parameters analyzed
â•‘ Applied Successfully: {applied_count} parameters merged
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""

                # FIXED: Aggressive cleanup before returning
                del base_model_obj, other_model_objs, models_to_merge
                if rank_sensitivity <= 0.0:
                    del merger
                force_cleanup()

                result = (model_merged, results_text, parameter_info)

                # Store in cache
                store_merge_result(cache_key, result)

                # Unload input models to free memory - they'll be reloaded if parameters change
                print("[MEMORY] Unloading input models after successful merge")
                del model_base
                if 'other_model_objs' in locals():
                    del other_model_objs
                force_cleanup()

                return result

            except MemoryExhaustionError as e:
                print(f"[SAFETY] Memory exhaustion prevented crash: {e}")
                # FIXED: Cleanup on error
                force_cleanup()
                error_results = f"""â•”â• WIDEN MERGE RESULTS (MEMORY ERROR) â•â•—
â•‘ ERROR: Memory exhaustion prevented crash
â•‘ DETAILS: {str(e)[:40]}...
â•‘ STATUS: âœ— Failed - Memory limit exceeded
â•‘ FIX: Reduce batch size or model count
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                error_param_info = """â•”â• ERROR PARAMETER INFO â•â•—
â•‘ Merge was terminated due to memory limits
â•‘ No parameter analysis available
â•‘ Try reducing model count or batch size
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                result = (model_base, error_results, error_param_info)
                store_merge_result(cache_key, result)
                return result

            except Exception as e:
                print(f"[DonutWidenMergeUNet] Error: {e}")
                # FIXED: Cleanup on error
                force_cleanup()
                if "memory" in str(e).lower():
                    error_results = f"""â•”â• WIDEN MERGE RESULTS (MEMORY ERROR) â•â•—
â•‘ ERROR: Memory error prevented crash
â•‘ DETAILS: {str(e)[:40]}...
â•‘ STATUS: âœ— Failed - Memory limit exceeded
â•‘ FIX: Reduce batch size or model count
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    error_param_info = """â•”â• ERROR PARAMETER INFO â•â•—
â•‘ Merge was terminated due to memory error
â•‘ No parameter analysis available  
â•‘ Try reducing model count or batch size
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    result = (model_base, error_results, error_param_info)
                    store_merge_result(cache_key, result)
                    return result
                else:
                    raise


# VERSION CHECK - This should appear in logs if new code is loading
print("="*50)
print("LOADING DONUTWIDENMERGECLIP VERSION 7.0 - FULL ZERO-ACCUMULATION - BUGFIXED")
print("="*50)

class DonutWidenMergeCLIP:
    class_type = "CLIP"

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "clip_base": ("CLIP",),
                "clip_other": ("CLIP",),
                "merge_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 5.0, "step": 0.01}),
                "min_strength": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 5.0, "step": 0.1}),
                "max_strength": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 5.0, "step": 0.1}),
                "normalization_mode": (["magnitude", "calibrate", "none"], {"default": "magnitude"}),  # (renorm_mode)
                # Enhanced WIDEN parameters
                "importance_threshold": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 500.0, "step": 0.1}),  # (above_average_value_ratio)
                "importance_boost": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 3.0, "step": 0.1}),  # (score_calibration_value)
                # Dynamic compatibility settings  
                "rank_sensitivity": ("FLOAT", {"default": 2.0, "min": 0.0, "max": 10.0, "step": 0.1}),  # (compatibility_sensitivity)
                "skip_threshold": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.000001}),  # (compatibility_threshold)
            },
            "optional": {
                "lora_stack": ("LORA_STACK",),
                "clip_3": ("CLIP",),
                "clip_4": ("CLIP",),
                "clip_5": ("CLIP",),
                "clip_6": ("CLIP",),
                "clip_7": ("CLIP",),
                "clip_8": ("CLIP",),
                "clip_9": ("CLIP",),
                "clip_10": ("CLIP",),
                "clip_11": ("CLIP",),
                "clip_12": ("CLIP",),
            }
        }

    RETURN_TYPES = ("CLIP", "STRING", "STRING")
    RETURN_NAMES = ("clip", "merge_results", "parameter_info")
    FUNCTION = "execute"
    CATEGORY = "donut/merge"

    def execute(self, clip_base, clip_other, merge_strength, min_strength, max_strength, normalization_mode,
                importance_threshold, importance_boost,
                rank_sensitivity, skip_threshold,
                lora_stack=None, clip_3=None, clip_4=None, clip_5=None, clip_6=None,
                clip_7=None, clip_8=None, clip_9=None, clip_10=None,
                clip_11=None, clip_12=None):

        # Conservative pre-merge setup with session cache management
        print("[MEMORY] Pre-merge setup...")
        
        # Clear session cache if it's getting large (prevents accumulation)
        if len(_MERGE_CACHE) >= _CACHE_MAX_SIZE:
            clear_session_cache()
        
        # Light pre-merge cleanup
        gentle_cleanup()

        # Check cache first
        all_clips = [clip_base, clip_other, clip_3, clip_4, clip_5, clip_6,
                    clip_7, clip_8, clip_9, clip_10, clip_11, clip_12]
        cache_key = compute_merge_hash(all_clips, merge_strength, min_strength, max_strength, importance_threshold, importance_boost, rank_sensitivity, skip_threshold, f"{normalization_mode}_enhanced_widen", lora_stack)
        # CLIP cache analysis

        cached_result = check_cache_for_merge(cache_key)
        if cached_result is not None:
            # Cache hit
            return cached_result
        else:
            # Cache miss - computing fresh merge
            pass

        with memory_cleanup_context("DonutWidenMergeCLIP"):
            import copy
            import gc

            # Get base encoder first for LoRA processing
            base_enc = getattr(clip_base, "model", getattr(clip_base, "clip",
                      getattr(clip_base, "cond_stage_model", None)))
            if not base_enc:
                raise AttributeError("Could not locate base CLIP encoder")

            # Process LoRA stack if provided
            lora_processor = None
            if lora_stack is not None:
                print("[DonutWidenMergeCLIP] Processing LoRA stack for delta-based merging...")
                lora_processor = LoRAStackProcessor(clip_base)  # Pass the wrapper, not base_enc
                lora_processor.add_lora_from_stack(lora_stack)
                
                # Get summary of LoRA processing
                summary = lora_processor.get_summary()
                print(f"[LoRADelta] Processed {summary['lora_count']} LoRAs with {summary['total_delta_parameters']} delta parameters")
                print(f"[LoRADelta] LoRA names: {summary['lora_names']}")

            # FIXED: Filter out None clips and filler clips more safely
            clips_to_merge = []
            for c in all_clips[1:]:  # Skip clip_base
                if c is not None and not getattr(c, "_is_filler", False):
                    clips_to_merge.append(c)

            # Add LoRA-enhanced virtual models if available
            if lora_processor is not None:
                virtual_models = lora_processor.get_virtual_models()
                # Skip the base model (first item) since we already have it
                lora_virtual_models = virtual_models[1:]  # Only LoRA deltas
                clips_to_merge.extend(lora_virtual_models)
                print(f"[LoRADelta] Added {len(lora_virtual_models)} LoRA-enhanced virtual CLIP models")

            print(f"[DonutWidenMergeCLIP] WIDEN merging {len(clips_to_merge)} CLIP models ({len([m for m in clips_to_merge if hasattr(m, 'lora_name')])} from LoRA stack)")

            try:
                # Handle both regular clips and LoRADelta objects
                other_encs = []
                for clip in clips_to_merge:
                    if hasattr(clip, 'lora_name'):  # LoRADelta object
                        other_encs.append(clip)
                    else:  # Regular clip
                        enc = getattr(clip, "model", getattr(clip, "clip",
                             getattr(clip, "cond_stage_model", None)))
                        if enc:
                            other_encs.append(enc)

                # FIXED: Memory-efficient clip cloning to avoid GPU OOM
                # Instead of deepcopy which duplicates all tensors in VRAM, just copy the wrapper
                clip_merged = clip_base.clone()  # Clone to always use fresh ingredients
                
                # Get the encoder from the cloned CLIP for merging
                enc_merged = getattr(clip_merged, "model", getattr(clip_merged, "clip",
                           getattr(clip_merged, "cond_stage_model", None)))

                # Use enhanced WIDEN merger with dynamic compatibility
                print("[DonutWidenMergeCLIP] Using WIDEN merge with dynamic compatibility-based strength...")
                
                merger = MergingMethod("DonutWidenMergeCLIP")
                
                if rank_sensitivity > 0.0:
                    # Use WIDEN with dynamic strength based on compatibility
                    merged_params, widen_diagnostics = enhanced_widen_merging_with_dynamic_strength(
                        merger=merger,
                        merged_model=enc_merged,
                        models_to_merge=other_encs,
                        exclude_param_names_regex=[],
                        importance_threshold=importance_threshold,
                        importance_boost=importance_boost,
                        merge_strength=merge_strength,
                        min_strength=min_strength,
                        max_strength=max_strength,
                        rank_sensitivity=rank_sensitivity,
                        skip_threshold=skip_threshold,
                        normalization_mode=normalization_mode
                    )
                    
                    # Apply merged parameters with shape validation
                    applied_count = 0
                    shape_mismatch_count = 0
                    for param_name, param_value in merged_params.items():
                        for name, param in enc_merged.named_parameters():
                            if name == param_name:
                                # Detect and remove an accidental leading batch-of-1 dim
                                if param_value.ndim == param.ndim + 1 and param_value.size(0) == 1:
                                    param_value = param_value.squeeze(0)
                                
                                if param_value.shape == param.shape:
                                    param.data.copy_(param_value)
                                    applied_count += 1
                                else:
                                    print(f"[WARNING] Shape mismatch for {param_name}: expected {param.shape}, got {param_value.shape}")
                                    shape_mismatch_count += 1
                                break
                    
                    print(f"[ENHANCED WIDEN] Applied {applied_count} parameters, {shape_mismatch_count} shape mismatches")
                    
                    # Compute and display merge sanity metrics
                    try:
                        sanity_metrics = compute_merge_sanity_metrics(base_model, merged_model)
                        print_merge_diagnostics(sanity_metrics)
                    except Exception as e:
                        diagnostic_logger.warning(f"Failed to compute sanity metrics: {e}")
                    
                    # Create detailed merge results with enhanced WIDEN diagnostics
                    total_clips = len([c for c in [clip_other, clip_3, clip_4, clip_5, clip_6, clip_7, clip_8, clip_9, clip_10, clip_11, clip_12] if c is not None]) + 1
                    
                    # Generate enhanced diagnostics from widen_diagnostics
                    compatibility_scores = widen_diagnostics['compatibility_scores']
                    varied_count = widen_diagnostics['varied_score_count']
                    uniform_count = widen_diagnostics['uniform_score_count']
                    skipped_threshold = widen_diagnostics['parameters_skipped_threshold']
                    
                    # Extract strength distribution metrics
                    strength_dist = widen_diagnostics['strength_distribution']
                    applied_strengths = widen_diagnostics['applied_strengths']
                    
                    # Sanitize strength distribution for display
                    clean_strength_dist = sanitize_strength_distribution(strength_dist)
                    
                    if compatibility_scores:
                        compat_values = [score['compatibility'] for score in compatibility_scores]
                        compat_min, compat_max = min(compat_values), max(compat_values)
                        compat_mean = sum(compat_values) / len(compat_values)
                        compat_variance = sum((x - compat_mean)**2 for x in compat_values) / len(compat_values)
                        compat_range = compat_max - compat_min
                        # Use relative variance threshold based on score range
                        relative_variance_threshold = max(1e-6, (compat_range * 0.01) ** 2)
                        score_health = "âœ“ VARIED" if compat_variance > relative_variance_threshold else "âœ— UNIFORM (BUG!)"
                    else:
                        compat_min = compat_max = compat_mean = 0.0
                        score_health = "NO SCORES"
                    
                    ranking_health = "âœ“ HEALTHY" if varied_count > uniform_count else "âœ— FAILING"
                    total_scored = varied_count + uniform_count
                    
                    # Generate skip threshold recommendation for CLIP dynamic mode
                    compatibility_score_dict = {score['parameter']: score['compatibility'] for score in compatibility_scores}
                    recommended_threshold, threshold_analysis = _analyze_compatibility_patterns_and_recommend_threshold(
                        compatibility_score_dict, skip_threshold
                    )
                    
                    # Add threshold analysis to console output
                    performance_logger.info(f"CLIP skip threshold analysis:\n{threshold_analysis}")
                    
                    results_text = f"""â•”â• WIDEN CLIP MERGE RESULTS (Dynamic Compatibility) â•â•—
â•‘ CLIP Models: {total_clips} | Strength: {min_strength}-{max_strength} | Mode: {normalization_mode}
â•‘ Threshold: {importance_threshold} | Boost: {importance_boost} | Sensitivity: {rank_sensitivity}
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Parameters Merged: {len(merged_params)} | Applied: {applied_count}
â•‘ Shape Mismatches: {shape_mismatch_count} | Success: {(applied_count/(applied_count+shape_mismatch_count)*100):.1f}%
â•‘ Status: âœ“ Enhanced WIDEN with Dynamic Compatibility
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ðŸ” WIDEN ALGORITHM HEALTH DIAGNOSTICS:
â•‘ Compatibility Range: {compat_min:.4f} - {compat_max:.4f} (avg: {compat_mean:.4f})
â•‘ Strength Distribution: {clean_strength_dist['display_text']}
â•‘ Applied Strengths: {clean_strength_dist['count']} parameters with dynamic adjustment
â•‘ Score Distribution: {score_health}
â•‘ Parameter Ranking: {varied_count}/{total_scored} varied ({100*varied_count/total_scored if total_scored > 0 else 0:.1f}%) - {ranking_health}
â•‘ Skip Threshold: {skip_threshold} (percentile) â†’ {skipped_threshold} parameters skipped
â•‘ ðŸ’¡ RECOMMENDED: skip_threshold = {recommended_threshold:.6f} for optimal performance
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    # Create detailed parameter information
                    parameter_info = f"""â•”â• WIDEN CLIP PARAMETER DETAILS â•â•—
â•‘ Merge Strength: Controls blend intensity between CLIP models
â•‘ Importance Threshold: Multiplier for classifying important parameters
â•‘ Importance Boost: Score amplification for important parameters
â•‘ Rank Sensitivity: Dynamic compatibility adjustment strength  
â•‘ Skip Threshold: Excludes low-compatibility parameters from merge
â•‘ Normalization: {normalization_mode} - post-merge parameter scaling
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Dynamic Mode: Adapts strength based on compatibility
â•‘ Total Processed: {len(merged_params)} parameters analyzed
â•‘ Applied Successfully: {applied_count} parameters merged
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    
                else:
                    # Use enhanced WIDEN without dynamic strength (fallback mode)
                    merged_params, widen_diagnostics = enhanced_widen_merging_with_dynamic_strength(
                        merger=merger,
                        merged_model=enc_merged,
                        models_to_merge=other_encs,
                        exclude_param_names_regex=[],
                        importance_threshold=importance_threshold,
                        importance_boost=importance_boost,
                        merge_strength=merge_strength,
                        min_strength=min_strength,
                        max_strength=max_strength,
                        rank_sensitivity=0.0,  # Disable dynamic strength
                        skip_threshold=skip_threshold,
                        normalization_mode=normalization_mode
                    )
                    
                    # Apply merged parameters with shape validation
                    applied_count = 0
                    shape_mismatch_count = 0
                    for param_name, param_value in merged_params.items():
                        for name, param in enc_merged.named_parameters():
                            if name == param_name:
                                # Detect and remove an accidental leading batch-of-1 dim
                                if param_value.ndim == param.ndim + 1 and param_value.size(0) == 1:
                                    param_value = param_value.squeeze(0)
                                
                                if param_value.shape == param.shape:
                                    param.data.copy_(param_value)
                                    applied_count += 1
                                else:
                                    print(f"[WARNING] Shape mismatch for {param_name}: expected {param.shape}, got {param_value.shape}")
                                    shape_mismatch_count += 1
                                break
                    
                    print(f"[ENHANCED WIDEN] Applied {applied_count} parameters, {shape_mismatch_count} shape mismatches")
                    
                    # Compute and display merge sanity metrics
                    try:
                        sanity_metrics = compute_merge_sanity_metrics(base_model, merged_model)
                        print_merge_diagnostics(sanity_metrics)
                    except Exception as e:
                        diagnostic_logger.warning(f"Failed to compute sanity metrics: {e}")
                    
                    # Create detailed merge results with enhanced WIDEN diagnostics
                    total_clips = len([c for c in [clip_other, clip_3, clip_4, clip_5, clip_6, clip_7, clip_8, clip_9, clip_10, clip_11, clip_12] if c is not None]) + 1
                    
                    # Generate enhanced diagnostics from widen_diagnostics
                    compatibility_scores = widen_diagnostics['compatibility_scores']
                    varied_count = widen_diagnostics['varied_score_count']
                    uniform_count = widen_diagnostics['uniform_score_count']
                    skipped_threshold = widen_diagnostics['parameters_skipped_threshold']
                    
                    # Extract strength distribution metrics
                    strength_dist = widen_diagnostics['strength_distribution']
                    applied_strengths = widen_diagnostics['applied_strengths']
                    
                    # Sanitize strength distribution for display
                    clean_strength_dist = sanitize_strength_distribution(strength_dist)
                    
                    if compatibility_scores:
                        compat_values = [score['compatibility'] for score in compatibility_scores]
                        compat_min, compat_max = min(compat_values), max(compat_values)
                        compat_mean = sum(compat_values) / len(compat_values)
                        compat_variance = sum((x - compat_mean)**2 for x in compat_values) / len(compat_values)
                        compat_range = compat_max - compat_min
                        # Use relative variance threshold based on score range
                        relative_variance_threshold = max(1e-6, (compat_range * 0.01) ** 2)
                        score_health = "âœ“ VARIED" if compat_variance > relative_variance_threshold else "âœ— UNIFORM (BUG!)"
                    else:
                        compat_min = compat_max = compat_mean = 0.0
                        score_health = "NO SCORES"
                    
                    ranking_health = "âœ“ HEALTHY" if varied_count > uniform_count else "âœ— FAILING"
                    total_scored = varied_count + uniform_count
                    
                    results_text = f"""â•”â• WIDEN CLIP MERGE RESULTS (Static Strength) â•â•—
â•‘ CLIP Models: {total_clips} | Strength: {min_strength}-{max_strength} | Mode: {normalization_mode}
â•‘ Threshold: {importance_threshold} | Boost: {importance_boost} | Sensitivity: {rank_sensitivity} (off)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Parameters Merged: {len(merged_params)} | Applied: {applied_count}
â•‘ Shape Mismatches: {shape_mismatch_count} | Success: {(applied_count/(applied_count+shape_mismatch_count)*100):.1f}%
â•‘ Status: âœ“ Enhanced WIDEN with Static Strength
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ðŸ” WIDEN ALGORITHM HEALTH DIAGNOSTICS:
â•‘ Compatibility Range: {compat_min:.4f} - {compat_max:.4f} (avg: {compat_mean:.4f})
â•‘ Strength Distribution: {clean_strength_dist['display_text']}
â•‘ Applied Strengths: {clean_strength_dist['count']} parameters with dynamic adjustment
â•‘ Score Distribution: {score_health}
â•‘ Parameter Ranking: {varied_count}/{total_scored} varied ({100*varied_count/total_scored if total_scored > 0 else 0:.1f}%) - {ranking_health}
â•‘ Skip Threshold: {skip_threshold} (percentile) â†’ {skipped_threshold} parameters skipped
â•‘ ðŸ’¡ RECOMMENDED: skip_threshold = {recommended_threshold:.6f} for optimal performance
â•‘ Status: âœ“ Enhanced WIDEN with Static Strength
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    # Create detailed parameter information
                    parameter_info = f"""â•”â• WIDEN CLIP PARAMETER DETAILS â•â•—
â•‘ Merge Strength: Controls blend intensity between CLIP models
â•‘ Importance Threshold: Multiplier for classifying important parameters
â•‘ Importance Boost: Score amplification for important parameters
â•‘ Rank Sensitivity: {rank_sensitivity} (disabled) - no dynamic adjustment
â•‘ Skip Threshold: Excludes low-compatibility parameters from merge  
â•‘ Normalization: {normalization_mode} - post-merge parameter scaling
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Static Mode: Uses fixed strength for all parameters
â•‘ Total Processed: {len(merged_params)} parameters analyzed
â•‘ Applied Successfully: {applied_count} parameters merged
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""

                # FIXED: Aggressive cleanup before returning
                del base_enc, other_encs, clips_to_merge, enc_merged
                if rank_sensitivity <= 0.0:
                    del merger
                force_cleanup()

                result = (clip_merged, results_text, parameter_info)

                # Store in cache
                store_merge_result(cache_key, result)

                # Unload input CLIP models to free memory - they'll be reloaded if parameters change
                print("[MEMORY] Unloading input CLIP models after successful merge")
                del clip_base
                if 'all_clips' in locals():
                    del all_clips
                force_cleanup()

                return result

            except MemoryExhaustionError as e:
                print(f"[SAFETY] Memory exhaustion prevented crash: {e}")
                # FIXED: Cleanup on error
                force_cleanup()
                error_results = f"""â•”â• WIDEN CLIP MERGE RESULTS (MEMORY ERROR) â•â•—
â•‘ ERROR: Memory exhaustion prevented crash
â•‘ DETAILS: {str(e)[:40]}...
â•‘ STATUS: âœ— Failed - Memory limit exceeded
â•‘ FIX: Reduce batch size or CLIP model count
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                error_param_info = """â•”â• ERROR PARAMETER INFO â•â•—
â•‘ CLIP merge was terminated due to memory limits
â•‘ No parameter analysis available
â•‘ Try reducing CLIP model count or batch size
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                result = (clip_base, error_results, error_param_info)
                store_merge_result(cache_key, result)
                return result

            except Exception as e:
                print(f"[DonutWidenMergeCLIP] Error: {e}")
                # FIXED: Cleanup on error
                force_cleanup()
                if "memory" in str(e).lower():
                    error_results = f"""â•”â• WIDEN CLIP MERGE RESULTS (MEMORY ERROR) â•â•—
â•‘ ERROR: Memory error prevented crash
â•‘ DETAILS: {str(e)[:40]}...
â•‘ STATUS: âœ— Failed - Memory limit exceeded
â•‘ FIX: Reduce batch size or CLIP model count
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    error_param_info = """â•”â• ERROR PARAMETER INFO â•â•—
â•‘ CLIP merge was terminated due to memory error
â•‘ No parameter analysis available
â•‘ Try reducing CLIP model count or batch size  
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"""
                    result = (clip_base, error_results, error_param_info)
                    store_merge_result(cache_key, result)
                    return result
                else:
                    raise


class DonutFillerModel:
    class_type = "MODEL"
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {}, "optional": {}}
    RETURN_TYPES = ("MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "execute"
    CATEGORY = "utils"

    def execute(self):
        class _Stub:
            def state_dict(self): return {}
            def named_parameters(self): return iter([])
        m = _Stub()
        setattr(m, "_is_filler", True)
        return (m,)


class DonutFillerClip:
    class_type = "CLIP"
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {}, "optional": {}}
    RETURN_TYPES = ("CLIP",)
    RETURN_NAMES = ("clip",)
    FUNCTION = "execute"
    CATEGORY = "utils"

    def execute(self):
        class _StubClip:
            def state_dict(self): return {}
            def named_parameters(self): return iter([])
        c = _StubClip()
        setattr(c, "_is_filler", True)
        return (c,)


NODE_CLASS_MAPPINGS = {
    "DonutWidenMergeUNet": DonutWidenMergeUNet,
    "DonutWidenMergeCLIP": DonutWidenMergeCLIP,
    "DonutFillerClip": DonutFillerClip,
    "DonutFillerModel": DonutFillerModel,
}

# FIXED: Add manual cleanup function for ComfyUI
def manual_cleanup():
    """Manual cleanup function that users can call"""
    print("="*50)
    print("MANUAL MEMORY CLEANUP INITIATED")
    print("="*50)
    clear_merge_cache()
    force_cleanup()
    print("="*50)
    print("MANUAL CLEANUP COMPLETE")
    print("="*50)

# Export the manual cleanup function
NODE_CLASS_MAPPINGS["DonutManualCleanup"] = type("DonutManualCleanup", (), {
    "class_type": "FUNCTION",
    "INPUT_TYPES": classmethod(lambda cls: {"required": {}}),
    "RETURN_TYPES": ("STRING",),
    "RETURN_NAMES": ("status",),
    "FUNCTION": "execute",
    "CATEGORY": "donut/utils",
    "execute": lambda self: (f"Memory cleanup completed at {time.time()}",)
})

def clear_merge_cache():
    """Clear the model merge cache"""
    global _MERGE_CACHE
    _MERGE_CACHE.clear()
    print("[Cache] Cleared all cached merge results")

import atexit
def cleanup_on_exit():
    """Cleanup on exit"""
    try:
        clear_merge_cache()  # FIXED: Actually call the cache clearing function
        force_cleanup()      # FIXED: Call force cleanup on exit
    except Exception:
        pass

atexit.register(cleanup_on_exit)
